{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stylographic_features import get_stylographic_feat\n",
    "from clustering import KMeansAuthors\n",
    "\n",
    "from style_point_cloud import style_point_cloud\n",
    "\n",
    "from distances import mod_hausdorff_dist\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/Reuters.json\")\n",
    "\n",
    "# No need to clean the text here, as it is already in good shape. We only need to lowercase it, since\n",
    "# we assume it.\n",
    "df.text = df.text.apply(lambda x: x.lower())\n",
    "\n",
    "# Generate point cloud for each text. Since this is shorter, we must\n",
    "# take less tokens\n",
    "df[\"point_cloud\"] = df.text.apply(lambda text: style_point_cloud(text, window_size=300, window_overlap=100, max_tokens=1_000))\n",
    "\n",
    "# Remove documents with no points\n",
    "df = df[df.point_cloud.apply(len) > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an author label for each point\n",
    "auth_labels = [[author]*n_chunks for author, n_chunks in \n",
    "                zip(df.author, df.point_cloud.apply(len))]\n",
    "\n",
    "# Flatten\n",
    "auth_labels = [x for y in auth_labels for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = PCA(n_components=3).fit_transform(np.vstack(df.point_cloud))\n",
    "\n",
    "cl = KMeansAuthors(n_authors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.fit(X, auth_labels)\n",
    "cl.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document accuracy\n",
    "(df.author == cl.predict_document(X, df.point_cloud.apply(len).to_numpy())).sum() / df.author.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cl.predict(X, author_labels=True)\n",
    "auth_idx = dict(zip(set(predictions), range(len(set(predictions)))))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "for label in set(predictions): \n",
    "    mask_pred = [True if pr == label else False for pr in predictions]\n",
    "    mask_true = [True if pr == label else False for pr in auth_labels]\n",
    "    ax1.scatter(X[:, 0][mask_pred], X[:, 2][mask_pred], label=label)\n",
    "    ax2.scatter(X[:, 0][mask_true], X[:, 2][mask_true], label=label)\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax1.set_title(\"KMeans Predicted\")\n",
    "    ax2.set_title(\"True labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "936a5b082d031c3f84051317a1cecdb9fa62954e636831d12ecae1cf9c53cca8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
