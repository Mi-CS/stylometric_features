{"index":{"0":14,"1":150,"2":191,"3":214,"4":252,"5":325,"6":336,"7":340,"8":402,"9":443,"10":572,"11":573,"12":779,"13":1775},"id":{"0":"http:\/\/arxiv.org\/abs\/2112.15124v1","1":"http:\/\/arxiv.org\/abs\/2112.11494v1","2":"http:\/\/arxiv.org\/abs\/2112.10021v1","3":"http:\/\/arxiv.org\/abs\/2112.09526v1","4":"http:\/\/arxiv.org\/abs\/2112.08789v1","5":"http:\/\/arxiv.org\/abs\/2112.08087v1","6":"http:\/\/arxiv.org\/abs\/2112.07882v1","7":"http:\/\/arxiv.org\/abs\/2112.07870v1","8":"http:\/\/arxiv.org\/abs\/2112.06507v1","9":"http:\/\/arxiv.org\/abs\/2112.05807v1","10":"http:\/\/arxiv.org\/abs\/2112.02714v1","11":"http:\/\/arxiv.org\/abs\/2112.02706v1","12":"http:\/\/arxiv.org\/abs\/1910.14084v2","13":"http:\/\/arxiv.org\/abs\/2110.06474v1"},"updated":{"0":1640882788000,"1":1640114841000,"2":1639868301000,"3":1639751023000,"4":1639653478000,"5":1639572484000,"6":1639543993000,"7":1639542194000,"8":1639388321000,"9":1639166021000,"10":1638748553000,"11":1638745993000,"12":1637863806000,"13":1634096284000},"published":{"0":"2021-12-30T16:46:28Z","1":"2021-12-21T19:27:21Z","2":"2021-12-18T22:58:21Z","3":"2021-12-17T14:23:43Z","4":"2021-12-16T11:17:58Z","5":"2021-12-15T12:48:04Z","6":"2021-12-15T04:53:13Z","7":"2021-12-15T04:23:14Z","8":"2021-12-13T09:38:41Z","9":"2021-12-10T19:53:41Z","10":"2021-12-05T23:55:53Z","11":"2021-12-05T23:13:13Z","12":"2019-10-30T18:57:28Z","13":"2021-10-13T03:38:04Z"},"title":{"0":"Utilizing Wordnets for Cognate Detection among Indian Languages","1":"Sentence Embeddings and Highspeed Similarity Search for Fast Computer Assisted Annotation of Legal Documents","2":"Continual Learning with Knowledge Transfer for Sentiment Classification","3":"Challenge Dataset of Cognates and False Friend Pairs from Indian Languages","4":"Harnessing Crosslingual Features to Improve Cognate Detection for Lowresource Languages","5":"Cognitionaware Cognate Detection","6":"Lex Rosetta. Transfer of Predictive Models Across Languages Jurisdictions and Legal Domains","7":"CrossDomain Generalization and Knowledge Transfer in Transformers Trained on Legal Data","8":"Automated Evidence Collection for Fake News Detection","9":"ComputerAssisted Creation of Boolean Search Rules for Text Classification in the Legal Domain","10":"CLASSIC. Continual and Contrastive Learning of Aspect Sentiment Classification Tasks","11":"Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning","12":"Building an Application Independent Natural Language Interface","13":"ActiveEA. Active Learning for Neural Entity Alignment"},"summary":{"0":"automatic cognate detection (acd) is a challenging task which has been utilized to help nlp applications like machine translation, information retrieval and computational phylogenetics. unidentified cognate pairs can pose a challenge to these applications and result in a degradation of performance. in this paper, we detect cognate word pairs among ten indian languages with hindi and use deep learning methodologies to predict whether a word pair is cognate or not. we identify indowordnet as a potential resource to detect cognate word pairs based on orthographic similarity-based methods and train neural network models using the data obtained from it. we identify parallel corpora as another potential resource and perform the same experiments for them. we also validate the contribution of wordnets through further experimentation and report improved performance of up to 26%. we discuss the nuances of cognate detection among closely related indian languages and release the lists of detected cognates as a dataset. we also observe the behaviour of, to an extent, unrelated indian language pairs and release the lists of detected cognates among them as well. ","1":"human-performed annotation of sentences in legal documents is an important prerequisite to many machine learning based systems supporting legal tasks. typically, the annotation is done sequentially, sentence by sentence, which is often time consuming and, hence, expensive. in this paper, we introduce a proof-of-concept system for annotating sentences \"laterally.\" the approach is based on the observation that sentences that are similar in meaning often have the same label in terms of a particular type system. we use this observation in allowing annotators to quickly view and annotate sentences that are semantically similar to a given sentence, across an entire corpus of documents. here, we present the interface of the system and empirically evaluate the approach. the experiments show that lateral annotation has the potential to make the annotation process quicker and more consistent. ","2":"this paper studies continual learning (cl) for sentiment classification (sc). in this setting, the cl system learns a sequence of sc tasks incrementally in a neural network, where each task builds a classifier to classify the sentiment of reviews of a particular product category or domain. two natural questions are: can the system transfer the knowledge learned in the past from the previous tasks to the new task to help it learn a better model for the new task? and, can old models for previous tasks be improved in the process as well? this paper proposes a novel technique called kan to achieve these objectives. kan can markedly improve the sc accuracy of both the new task and the old tasks via forward and backward knowledge transfer. the effectiveness of kan is demonstrated through extensive experiments. ","3":"cognates are present in multiple variants of the same text across different languages (e.g., \"hund\" in german and \"hound\" in english language mean \"dog\"). they pose a challenge to various natural language processing (nlp) applications such as machine translation, cross-lingual sense disambiguation, computational phylogenetics, and information retrieval. a possible solution to address this challenge is to identify cognates across language pairs. in this paper, we describe the creation of two cognate datasets for twelve indian languages, namely sanskrit, hindi, assamese, oriya, kannada, gujarati, tamil, telugu, punjabi, bengali, marathi, and malayalam. we digitize the cognate data from an indian language cognate dictionary and utilize linked indian language wordnets to generate cognate sets. additionally, we use the wordnet data to create a false friends' dataset for eleven language pairs. we also evaluate the efficacy of our dataset using previously available baseline cognate detection approaches. we also perform a manual evaluation with the help of lexicographers and release the curated gold-standard dataset with this paper. ","4":"cognates are variants of the same lexical form across different languages; for example 'fonema' in spanish and 'phoneme' in english are cognates, both of which mean 'a unit of sound'. the task of automatic detection of cognates among any two languages can help downstream nlp tasks such as cross-lingual information retrieval, computational phylogenetics, and machine translation. in this paper, we demonstrate the use of cross-lingual word embeddings for detecting cognates among fourteen indian languages. our approach introduces the use of context from a knowledge graph to generate improved feature representations for cognate detection. we, then, evaluate the impact of our cognate detection mechanism on neural machine translation (nmt), as a downstream task. we evaluate our methods to detect cognates on a challenging dataset of twelve indian languages, namely, sanskrit, hindi, assamese, oriya, kannada, gujarati, tamil, telugu, punjabi, bengali, marathi, and malayalam. additionally, we create evaluation datasets for two more indian languages, konkani and nepali. we observe an improvement of up to 18% points, in terms of f-score, for cognate detection. furthermore, we observe that cognates extracted using our method help improve nmt quality by up to 2.76 bleu. we also release our code, newly constructed datasets and cross-lingual models publicly. ","5":"automatic detection of cognates helps downstream nlp tasks of machine translation, cross-lingual information retrieval, computational phylogenetics and cross-lingual named entity recognition. previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. in this paper, we propose a novel method for enriching the feature sets, with cognitive features extracted from human readers' gaze behaviour. we collect gaze behaviour data for a small sample of cognates and show that extracted cognitive features help the task of cognate detection. however, gaze data collection and annotation is a costly task. we use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. we report improvements of 10% with the collected gaze features, and 12% using the predicted gaze features, over the previously proposed approaches. furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models. ","6":"in this paper, we examine the use of multi-lingual sentence embeddings to transfer predictive models for functional segmentation of adjudicatory decisions across jurisdictions, legal systems (common and civil law), languages, and domains (i.e. contexts). mechanisms for utilizing linguistic resources outside of their original context have significant potential benefits in ai & law because differences between legal systems, languages, or traditions often block wider adoption of research outcomes. we analyze the use of language-agnostic sentence representations in sequence labeling models using gated recurrent units (grus) that are transferable across languages. to investigate transfer between different contexts we developed an annotation scheme for functional segmentation of adjudicatory decisions. we found that models generalize beyond the contexts on which they were trained (e.g., a model trained on administrative decisions from the us can be applied to criminal law decisions from italy). further, we found that training the models on multiple contexts increases robustness and improves overall performance when evaluating on previously unseen contexts. finally, we found that pooling the training data from all the contexts enhances the models' in-context performance. ","7":"we analyze the ability of pre-trained language models to transfer knowledge among datasets annotated with different type systems and to generalize beyond the domain and dataset they were trained on. we create a meta task, over multiple datasets focused on the prediction of rhetorical roles. prediction of the rhetorical role a sentence plays in a case decision is an important and often studied task in ai & law. typically, it requires the annotation of a large number of sentences to train a model, which can be time-consuming and expensive. further, the application of the models is restrained to the same dataset it was trained on. we fine-tune language models and evaluate their performance across datasets, to investigate the models' ability to generalize across domains. our results suggest that the approach could be helpful in overcoming the cold-start problem in active or interactvie learning, and shows the ability of the models to generalize across datasets and domains. ","8":"fake news, misinformation, and unverifiable facts on social media platforms propagate disharmony and affect society, especially when dealing with an epidemic like covid-19. the task of fake news detection aims to tackle the effects of such misinformation by classifying news items as fake or real. in this paper, we propose a novel approach that improves over the current automatic fake news detection approaches by automatically gathering evidence for each claim. our approach extracts supporting evidence from the web articles and then selects appropriate text to be treated as evidence sets. we use a pre-trained summarizer on these evidence sets and then use the extracted summary as supporting evidence to aid the classification task. our experiments, using both machine learning and deep learning-based methods, help perform an extensive evaluation of our approach. the results show that our approach outperforms the state-of-the-art methods in fake news detection to achieve an f1-score of 99.25 over the dataset provided for the constraint-2021 shared task. we also release the augmented dataset, our code and models for any further research. ","9":"in this paper, we present a method of building strong, explainable classifiers in the form of boolean search rules. we developed an interactive environment called case (computer assisted semantic exploration) which exploits word co-occurrence to guide human annotators in selection of relevant search terms. the system seamlessly facilitates iterative evaluation and improvement of the classification rules. the process enables the human annotators to leverage the benefits of statistical information while incorporating their expert intuition into the creation of such rules. we evaluate classifiers created with our case system on 4 datasets, and compare the results to machine learning methods, including skope rules, random forest, support vector machine, and fasttext classifiers. the results drive the discussion on trade-offs between superior compactness, simplicity, and intuitiveness of the boolean search rules versus the better performance of state-of-the-art machine learning models for text classification. ","10":"this paper studies continual learning (cl) of a sequence of aspect sentiment classification(asc) tasks in a particular cl setting called domain incremental learning (dil). each task is from a different domain or product. the dil setting is particularly suited to asc because in testing the system needs not know the task\/domain to which the test data belongs. to our knowledge, this setting has not been studied before for asc. this paper proposes a novel model called classic. the key novelty is a contrastive continual learning method that enables both knowledge transfer across tasks and knowledge distillation from old tasks to the new task, which eliminates the need for task ids in testing. experimental results show the high effectiveness of classic. ","11":"continual learning (cl) learns a sequence of tasks incrementally with the goal of achieving two main objectives: overcoming catastrophic forgetting (cf) and encouraging knowledge transfer (kt) across tasks. however, most existing techniques focus only on overcoming cf and have no mechanism to encourage kt, and thus do not do well in kt. although several papers have tried to deal with both cf and kt, our experiments show that they suffer from serious cf when the tasks do not have much shared knowledge. another observation is that most current cl methods do not use pre-trained models, but it has been shown that such models can significantly improve the end task performance. for example, in natural language processing, fine-tuning a bert-like pre-trained language model is one of the most effective approaches. however, for cl, this approach suffers from serious cf. an interesting question is how to make the best use of pre-trained models for cl. this paper proposes a novel model called ctr to solve these problems. our experimental results demonstrate the effectiveness of ctr ","12":"traditional approaches to building natural language (nl) interfaces typically use a semantic parser to parse the user command and convert it to a logical form, which is then translated to an executable action in an application. however, it is still challenging for a semantic parser to correctly parse natural language. for a different domain, the parser may need to be retrained or tuned, and a new translator also needs to be written to convert the logical forms to executable actions. in this work, we propose a novel and application independent approach to building nl interfaces that does not need a semantic parser or a translator. it is based on natural language to natural language matching and learning, where the representation of each action and each user command are both in natural language. to perform a user intended action, the system only needs to match the user command with the correct action representation, and then execute the corresponding action. the system also interactively learns new (paraphrased) commands for actions to expand the action representations over time. our experimental results show the effectiveness of the proposed approach. ","13":"entity alignment (ea) aims to match equivalent entities across different knowledge graphs (kgs) and is an essential step of kg fusion. current mainstream methods -- neural ea models -- rely on training with seed alignment, i.e., a set of pre-aligned entity pairs which are very costly to annotate. in this paper, we devise a novel active learning (al) framework for neural ea, aiming to create highly informative seed alignment to obtain more effective ea models with less annotation cost. our framework tackles two main challenges encountered when applying al to ea: (1) how to exploit dependencies between entities within the al strategy. most al strategies assume that the data instances to sample are independent and identically distributed. however, entities in kgs are related. to address this challenge, we propose a structure-aware uncertainty sampling strategy that can measure the uncertainty of each entity as well as its impact on its neighbour entities in the kg. (2) how to recognise entities that appear in one kg but not in the other kg (i.e., bachelors). identifying bachelors would likely save annotation budget. to address this challenge, we devise a bachelor recognizer paying attention to alleviate the effect of sampling bias. empirical results show that our proposed al strategy can significantly improve sampling quality with good generality across different datasets, ea models and amount of bachelors. "},"authors":{"0":["Diptesh Kanojia","Kevin Patel","Pushpak Bhattacharyya","Malhar Kulkarni","Gholamreza Haffari"],"1":["Hannes Westermann","Jaromir Savelka","Vern R. Walker","Kevin D. Ashley","Karim Benyekhlef"],"2":["Zixuan Ke","Bing Liu","Hao Wang","Lei Shu"],"3":["Diptesh Kanojia","Pushpak Bhattacharyya","Malhar Kulkarni","Gholamreza Haffari"],"4":["Diptesh Kanojia","Raj Dabre","Shubham Dewangan","Pushpak Bhattacharyya","Gholamreza Haffari","Malhar Kulkarni"],"5":["Diptesh Kanojia","Prashant Sharma","Sayali Ghodekar","Pushpak Bhattacharyya","Gholamreza Haffari","Malhar Kulkarni"],"6":["Jaromir Savelka","Hannes Westermann","Karim Benyekhlef","Charlotte S. Alexander","Jayla C. Grant","David Restrepo Amariles","Rajaa El Hamdani","S\u00e9bastien Mee\u00f9s","Micha\u0142 Araszkiewicz","Kevin D. Ashley","Alexandra Ashley","Karl Branting","Mattia Falduti","Matthias Grabmair","Jakub Hara\u0161ta","Tereza Novotn\u00e1","Elizabeth Tippett","Shiwanni Johnson"],"7":["Jaromir Savelka","Hannes Westermann","Karim Benyekhlef"],"8":["Mrinal Rawat","Diptesh Kanojia"],"9":["Hannes Westermann","Jaromir Savelka","Vern R. Walker","Kevin D. Ashley","Karim Benyekhlef"],"10":["Zixuan Ke","Bing Liu","Hu Xu","Lei Shu"],"11":["Zixuan Ke","Bing Liu","Nianzu Ma","Hu Xu","Lei Shu"],"12":["Sahisnu Mazumder","Bing Liu","Shuai Wang","Sepideh Esmaeilpour"],"13":["Bing Liu","Harrisen Scells","Guido Zuccon","Wen Hua","Genghong Zhao"]},"pdf_link":{"0":"http:\/\/export.arxiv.org\/pdf\/2112.15124v1","1":"http:\/\/export.arxiv.org\/pdf\/2112.11494v1","2":"http:\/\/export.arxiv.org\/pdf\/2112.10021v1","3":"http:\/\/export.arxiv.org\/pdf\/2112.09526v1","4":"http:\/\/export.arxiv.org\/pdf\/2112.08789v1","5":"http:\/\/export.arxiv.org\/pdf\/2112.08087v1","6":"http:\/\/export.arxiv.org\/pdf\/2112.07882v1","7":"http:\/\/export.arxiv.org\/pdf\/2112.07870v1","8":"http:\/\/export.arxiv.org\/pdf\/2112.06507v1","9":"http:\/\/export.arxiv.org\/pdf\/2112.05807v1","10":"http:\/\/export.arxiv.org\/pdf\/2112.02714v1","11":"http:\/\/export.arxiv.org\/pdf\/2112.02706v1","12":"http:\/\/export.arxiv.org\/pdf\/1910.14084v2","13":"http:\/\/export.arxiv.org\/pdf\/2110.06474v1"},"category":{"0":["cs.CL"],"1":["cs.CL","cs.AI","cs.LG"],"2":["cs.CL","cs.AI","cs.LG","cs.NE"],"3":["cs.CL"],"4":["cs.CL"],"5":["cs.CL","cs.AI"],"6":["cs.CL"],"7":["cs.CL"],"8":["cs.CL"],"9":["cs.LG","cs.AI","cs.CL"],"10":["cs.CL","cs.AI","cs.LG","cs.NE"],"11":["cs.CL","cs.AI","cs.LG","cs.NE"],"12":["cs.CL","cs.HC","cs.IR"],"13":["cs.CL","cs.AI"]},"text_body":{"0":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilizing Wordnets for Cognate Detection among Indian Languages\n\nDiptesh Kanojia\u2020,\u2663,?, Kevin Patel\u2020, Pushpak Bhattacharyya\u2020,\nMalhar Kulkarni\u2020, Gholamreza Haffari?\n\n\u2020Indian Institute of Technology Bombay, India\n\u2663IITB-Monash Research Academy, India\n\n?Monash University, Australia\n\u2020{diptesh, kevin, pb, malhar}@iitb.ac.in\n\n?gholamreza.haffari@monash.edu\n\nAbstract\n\nAutomatic Cognate Detection (ACD) is a\nchallenging task which has been utilized\nto help NLP applications like Machine\nTranslation, Information Retrieval and\nComputational Phylogenetics. Unidenti-\nfied cognate pairs can pose a challenge to\nthese applications and result in a degra-\ndation of performance. In this paper, we\ndetect cognate word pairs among ten In-\ndian languages with Hindi and use deep\nlearning methodologies to predict whether\na word pair is cognate or not. We iden-\ntify IndoWordnet as a potential resource to\ndetect cognate word pairs based on ortho-\ngraphic similarity-based methods and train\nneural network models using the data ob-\ntained from it. We identify parallel cor-\npora as another potential resource and per-\nform the same experiments for them.\n\nWe also validate the contribution of Word-\nnets through further experimentation and\nreport improved performance of up to\n26%. We discuss the nuances of cog-\nnate detection among closely related In-\ndian languages and release the lists of de-\ntected cognates as a dataset. We also ob-\nserve the behaviour of, to an extent, unre-\nlated Indian language pairs and release the\nlists of detected cognates among them as\nwell.\n\n1 Introduction\n\nCognates are words that have a common etymo-\nlogical origin (Crystal, 2008). They account for\na considerable amount of unique words in many\nlexical domains, notably technical texts. The or-\nthographic similarity of cognates can be exploited\nin different tasks involving recognition of trans-\nlational equivalence between words, such as ma-\n\nchine translation and bilingual terminology com-\npilation. For e.g., the German - English cog-\nnates, Blume - bloom can be identified as cog-\nnates with orthographic similarity methods. De-\ntection of cognates helps various NLP applications\nlike IR (Pranav, 2018). Rama et al. (2018) study\nvarious cognate detection techniques and provide\nsubstantial proof that automatic cognate detection\ncan help infer phylogenetic trees. In many NLP\ntasks, the orthographic similarity of cognates can\ncompensate for the insufficiency of other kinds\nof evidence about the translational equivalency of\nwords (Mulloni and Pekar, 2006). The detection\nof cognates in compiling bilingual dictionaries\nhas proven to be helpful in Machine Translation\n(MT), and Information Retrieval (IR) tasks (Meng\net al., 2001). Orthographic similarity-based meth-\nods have relied on the lexical similarity of word\npairs and have been used extensively to detect cog-\nnates (Ciobanu and Dinu, 2014; Mulloni, 2007;\nInkpen et al., 2005). These methods, generally,\ncalculate the similarity score between two words\nand use the result to build training data for fur-\nther classification. Cognate detection can also be\nperformed using phonetic features and researchers\nhave previously used consonant class matching\n(CCM) (Turchin et al., 2010), sound class-based\nalignment (SCA) (List, 2010) etc. to detect cog-\nnates in multilingual wordlists. The identification\nof cognates, here, is based on the comparison of\nwords sound correspondences. Semantic similar-\nity methods have also been deployed to detect cog-\nnates among word pairs (Kondrak, 2001). The\nmeasure of semantic similarity uses the context\naround both word pairs and helps in the identifi-\ncation of a cognate word pair by looking of simi-\nlarity among the collected contexts.\n\nFor our work, we can primarily divide words\ninto four main categories viz. True Cog-\nnates, False Cognates, False Friends and Non-\nCognates. In Figure 1, we present this clas-\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n15\n12\n\n4v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 3\n\n0 \nD\n\nec\n 2\n\n02\n1\n\n\n\nsification with examples from various languages\nalong with their meanings for better understand-\ning. While some false friends are also false cog-\nnates, most of them are genuine cognates. Our pri-\nmary goal is to be able to identify True Cognates.\nSanskrit (Sa) is known to be the mother of most\nof the Indian languages. Hindi (Hi), Bengali (Bn),\nPunjabi (Pa), Marathi (Mr), Gujarati (Gu), Malay-\nalam (Ml), Tamil (Ta) and Telugu (Te) are known\nto borrow many words from it. Thus, one may ob-\nserve that words which belong to the same concept\nin these languages, if orthographically similar, are\nTrue Cognates. Currently, we include loan words\nin the dataset used for our work and include them\nas cognates. Since, eventually we aim to apply our\nwork to Machine Translation and other NLP appli-\ncations, we believe that this would help establish\na better correlation among source-target language\npairs. Also, we do not detect false friends and\nhence restrict the scope of True cognate detection\nusing this hypothesis to Figure 2.\n\nFigure 1: The Cognate Identification Matrix\n\nWe utilize the synset information from linked\nWordnets to identify words within the same\nconcept and deploy orthographic similarity-\nbased methods to compute similarity scores be-\ntween them. This helps us identify words with a\nhigh similarity score. In case of most of the In-\ndian languages, a sizeable contribution of words\/-\nconcepts is loaned from the Sanskrit language. In\nlinked IndoWordnet, each concept is aligned to the\nother based on an \u2018id\u2019 which can be reliably used\nas a measure to say that the etymological origin\nis the same, for both the concepts. Hence, words\nwith the same orthographic similarity can be said\nto be \u2018True Cognates\u2019. Using this methodology,\nwe detect highly similar words and use them as\n\nFigure 2: Scope of our work; Detection of True\nCognates and False Friends\n\ntraining data to build models which can predict\nwhether a word pair is cognate or not. The rest\nof the paper is organized as follows. In Section\n2 we describe the related work that has been car-\nried out on cognate detection together with some\nof its practical applications, while in Section 3\nwe present our approach and deal in greater detail\nwith our learning algorithms. Once the proposed\nmethodology has been outlined, we step through\nan evaluation method we devised and report on the\nresults obtained as specified in Section 4. Section\n5 concludes our paper with a brief summary and\ntackling further challenges in the near future.\n\n1.1 Contributions\nWe make the following contributions in this paper:\n1. We perform cognate detection for eleven Indian\nLanguages.\n2. We exploit Indian languages behaviour to ob-\ntain a list of true cognates (WNdata from WordNet\nand PCData from Parallel Corpora).\n3. We train neural networks to establish a baseline\nfor cognate detection.\n4. We validate the importance of Wordnets as a\nresource to perform cognate detection.\n5. We release our dataset (WNdata + PCdata) of\ncognate pairs publicly for the language pairs Hi -\nMr, Hi - Pa, Hi - Gu, Hi - Bn, Hi - Sa, Hi - Ml, Hi\n- Ta, Hi - Te, Hi - Ne, and Hi - Ur.\n\n2 Related Work\n\nOne of the most common techniques to find cog-\nnates is based on the manual design of rules de-\nscribing how orthography of a borrowed word\nshould change, once it has been introduced into\n\n\n\nthe other language. Koehn and Knight (2000)\nexpand a list of English-German cognate words\nby applying well-established transformation rules.\nThey also noted that the accuracy of their algo-\nrithm increased proportionally with the length of\nthe word since the accidental coexistence of two\nwords with the same spelling with different mean-\nings (we identify them as \u2018false friends\u2019) decreases\nthe accuracy.\n\nMost previous studies on automatic cognate\nidentification do not investigate Indian languages.\nMost of the Indian languages borrow cognates or\n\u201cloan words\u201d from Sanskrit. Indian languages like\nHindi, Bengali, Sinhala, Oriya and Dravidian lan-\nguages like Malayalam, Tamil, Telugu, and Kan-\nnada borrow many words from Sanskrit. Although\nrecently, Kanojia et al. (2019) perform cognate\ndetection for a few Indian languages, but report\nresults with manual verification of their output.\nIdentification of cognates for improving IR has al-\nready been explored for Indian languages (Makin\net al., 2007). String similarity-based methods are\noften used as baseline methods for cognate detec-\ntion and the most commonly used among them is\nEdit distance based similarity measure. It is used\nas the baseline in the early cognate detection pa-\npers (Melamed, 1999). Essentially, it computes\nthe number of operations required to transform\nfrom source to target cognate.\n\nResearch in automatic cognate detection us-\ning phonetic aspects involves computation of sim-\nilarity by decomposing phonetically transcribed\nwords (Kondrak, 2000), acoustic models (Mielke\net al., 2012), phonetic encodings (Rama et al.,\n2015), aligned segments of transcribed phonemes\n(List, 2012). We study Rama (2016)\u2019s research,\nwhich employs a Siamese convolutional neural\nnetwork to learn the phonetic features jointly with\nlanguage relatedness for cognate identification,\nwhich was achieved through phoneme encodings.\nAlthough it performs well on the accuracy, it\nshows poor results with MRR. Ja\u0308ger et al. (2017)\nuse SVM for phonetic alignment and perform cog-\nnate detection for various language families. Vari-\nous works on Orthographic cognate detection usu-\nally take alignment of substrings within classifiers\nlike SVM (Ciobanu and Dinu, 2014; Ciobanu and\nDinu, 2015) or HMM (Bhargava and Kondrak,\n2009). We also consider the method of Ciobanu\nand Dinu (2014), which employs dynamic pro-\ngramming based methods for sequence alignment.\n\nAmong cognate sets common overlap set mea-\nsures like set intersection, Jaccard (Ja\u0308rvelin et al.,\n2007), XDice (Brew et al., 1996) or TF-IDF (Wu\net al., 2008) could be used to measure similarities\nand validate the members of the set.\n\n3 Datasets and Methodology\n\nWe investigate language pairs for major Indian\nlanguages namely Marathi (Mr), Gujarati (Gu),\nBengali (Bn), Punjabi (Pa), Sanskrit (Sa), Malay-\nalam (Ml), Tamil (Ta), Telugu (Te), Nepali (Ne)\nand Urdu (Ur) with Hindi (Hi). We create two\ndatasets as described below for <source lang>\n-<target lang> where the source language is\nalways Hindi. We describe each step in the sub-\nsections below.\n\n3.1 Datasets\nDataset 1: WordNet based dataset\nWe create this dataset (WNData) by extracting\nsynset data from the IndoWordnet database. We\nmaintain all words, in the concept space, in a\ncomma-separated format. We, then, create word\nlists by combining all possible permutations of\nword pairs within each synset. For e.g., If synset\nID X on the source side (Hindi) contains words\nS1W1 and S1W2, and parallelly on the target side\n(other Indian languages), synset ID X contains\nT1W1 and T1W2, we create a word list such as:\nS1W1, T1W1\nS1W2, T1W1\nS1W1, T1W2\nS1W2, T1W2\n\nTo avoid redundancy, we remove duplicate\nword pairs from this list.\n\nDataset 2: Parallel Corpora based dataset\nWe use the ILCI parallel corpora for Indian lan-\nguages (Jha, 2010) and create word pairs list by\ncomparing all words in the source side sentence\nwith all words on the target side sentence. Our\nhypothesis, here, is that words with high ortho-\ngraphic similarity which occur in the same con-\ntext window (a sentence) would be cognates with\na high probability. Due to the unavailability of\nILCI parallel corpora for Sa and Ne, we download\nthese corpora from Wikipedia and align it with the\nHindi articles from Hindi Wikipedia. We calcu-\nlate exact word matches to align articles to each\nother thus creating comparable corpora and dis-\ncard unaligned lines from both sides. We, then,\n\n\n\nFigure 3: Block Diagram for our experimental setup\n\ncreate similar word pairs list between Hindi and\nall the other languages pairs. We removed dupli-\ncated word pairs from this list as well and call this\ndata PCData.\n\n3.2 Script Standardization and Text\nNormalization\n\nThe languages mentioned above share a major por-\ntion of the most spoken languages in India. Al-\nthough most of them borrow words from Sanskrit,\nthey belong to different language families. Mr,\nGu, Bn, Pa, Ne and Ur belong to the Indo-Aryan\nfamily of languages; and Ml, Ta, Te belong to\nthe family of Dravidian languages. They also use\ndifferent scripts to represent themselves textually.\nFor standardization, we convert all the other writ-\nten scripts to Devanagari. We perform Unicode\ntransliteration using Indic NLP Library1 to con-\nvert scripts for Bn, Gu, Pa, Ta, Te, Ml, and Ur to\nDevanagari, for both our datasets. Hi, Mr, Sa, and\nNe are already based on the Devanagari script, and\nhence we only perform text normalization for both\nour datasets, for these languages. The whole pro-\ncess is outlined in Figure 3.\n\n3.3 Similarity Scores Calculation\n\nWe calculate similarity scores for each word on the\nsource side i.e., Hi by matching it with each word\non the target side i.e., Sa, Bn, Gu, Pa, Mr, Ml, Ne,\nTa, Te, and Ur.\n\nSince we match the words from the same con-\n1https:\/\/anoopkunchukuttan.github.io\/\n\nindic_nlp_library\/\n\ncept space or the same context window, we elimi-\nnate the possibility of this word pair carrying dif-\nferent meanings, and hence a high orthographic\nsimilarity score gives us a strong indication of\nthese words falling under the category of True\nCognates. For training neural network models, we\nthen divide the positive and negative labels based\non a threshold and follow empirical methods in\nsetting this threshold to 0.5 for both datasets2. Us-\ning 0.5 as threshold, we obtained the best train-\ning performance and hence chose to use this as the\nthreshold for similarity calculation. The various\nsimilarity measures used are described in the next\nsubsection.\n\n3.4 Similarity Measures\n\nNormalized Edit Distance Method (NED)\nThe Normalized Edit Distance approach computes\nthe edit distance (Nerbonne and Heeringa, 1997)\nfor all word pairs in a synset\/concept and then pro-\nvides the output of probable cognate sets with dis-\ntance and similarity scores. We assign labels for\nthese sets based on the similarity score obtained\nfrom the NED method, where the similarity score\nis (1 - NED score). It is usually defined as a pa-\nrameterizable metric calculated with a specific set\nof allowed edit operations, and each operation is\nassigned a cost (possibly infinite). The score is\nnormalized such that 0 equates to no similarity and\n1 is an exact match. NED is equal to the mini-\nmum number of operations required to transform\n\n2We ran experiments with 0.25, 0.60, and 0.75 as well,\nand choose 0.5 based on training performance\n\nhttps:\/\/anoopkunchukuttan.github.io\/indic_nlp_library\/\nhttps:\/\/anoopkunchukuttan.github.io\/indic_nlp_library\/\n\n\n\u2018word a\u2019 to \u2018word b\u2019. A more general definition as-\nsociates non-negative weight functions (insertions,\ndeletions, and substitutions) with the operations.\n\nCosine Similarity (Cos)\nThe cosine similarity measure (Salton and Buck-\nley, 1988) is another similarity metric that depends\non envisioning preferences as points in space. It\nmeasures the cosine of the angle between two vec-\ntors projected in a multi-dimensional space. In this\ncontext, the two vectors are the arrays of charac-\nter counts of two words. The cosine similarity is\nparticularly used in positive space, where the out-\ncome is neatly bounded in [0,1]. For example, in\ninformation retrieval and text mining, each term\nis notionally assigned a different dimension and a\ndocument is characterised by a vector where the\nvalue in each dimension corresponds to the num-\nber of times the term appears in the document. Co-\nsine similarity then gives a useful measure of how\nsimilar two documents are likely to be in terms of\ntheir subject matter. This is analogous to the co-\nsine, which is 1 (maximum value) when the seg-\nments subtend a zero angle and 0 (uncorrelated)\nwhen the segments are perpendicular. In this con-\ntext, the two vectors are the arrays of character\ncounts of two words.\n\nJaro-Winkler Similarity (JWS)\nJaro-Winkler distance (Winkler, 1990) is a string\nmetric measuring similar to the normalized edit\ndistance deriving itself from Jaro Distance (Jaro,\n1989). It uses a prefix scale P which gives more\nfavourable ratings to strings that match from the\nbeginning, for a set prefix length L. We ensure\na normalized score in this case as well. Here,\nthe edit distance between two sequences is cal-\nculated using a prefix scale P which gives more\nfavourable ratings to strings that match from the\nbeginning, for a set prefix length L. The lower the\nJaro\u2013Winkler distance for two strings is, the more\nsimilar the strings are. The score is normalized\nsuch that 1 equates to no similarity and 0 is an ex-\nact match.\n\n3.5 Models\n\n3.5.1 Feed Forward Neural Network (FFN)\nIn this network, we deal with a word as a whole.\nWords of the source and target languages reside\nin separate embedding space. The source word\npasses through the source embedding layer. The\ntarget word passes through the target embedding\n\nFFN RNN\nD1 D2 D1 D2\n\nHi-Mr 69.76 85.76 74.76 89.78\nHi-Bn 65.18 81.04 69.18 86.44\nHi-Pa 73.04 78.50 76.04 83.64\nHi-Gu 61.74 79.16 69.84 89.44\nHi-Sa 61.72 85.87 68.92 91.66\nHi-Ml 56.96 74.77 66.96 79.59\nHi-Ta 55.62 61.70 65.62 68.92\nHi-Te 52.78 65.26 62.78 74.83\nHi-Ne 70.20 83.85 80.20 89.63\nHi-Ur 69.99 73.84 76.99 80.12\n\nTable 1: Stratified 5-fold Evaluation using Deep\nNeural Models on both PCData (D1) and WNData\n(D2)\n\nlayer. The outputs of both embedding lookups\nare concatenated. The resulting representation is\npassed to a fully connected layer with ReLU acti-\nvations, followed by a softmax layer.\n\n3.5.2 Recurrent Neural Network (RNN)\nIn this network (see Figure 4), we treat a word as\na sequence of characters. Characters of the source\nand the target language reside in separate embed-\nding spaces. The characters of the source word are\npassed through source embedding layer. The char-\nacters of the target word are passed through the tar-\nget embedding layer. The outputs of both embed-\nding lookups are, then, concatenated. The result-\ning embedded representation is passed through a\nrecurrent layer. The final hidden state of the recur-\nrent layer is then passed through a fully connected\nlayer with ReLU activation. The resulting output\nis finally passed through a softmax layer.\n\nFigure 4: Architecture of a Recurrent Neural Net-\nwork\n\n\n\nCorp+WN20 Corp+WN40 Corp+WN60 Corp+WN80 Corp+WN100\nFFN RNN FFN RNN FFN RNN FFN RNN FFN RNN\n\nHi-Mr 70.12 74.12 73.56 78.37 76.09 81.56 81.34 85.24 86.90 91.87\nHi-Bn 71.06 73.17 73.29 74.98 77.33 76.28 83.99 81.45 82.18 89.58\nHi-Pa 74.16 75.94 76.02 77.39 76.18 79.04 78.04 81.22 80.66 85.64\nHi-Gu 65.26 70.76 71.21 74.83 75.09 79.95 80.14 84.32 81.85 89.81\nHi-Sa 65.93 74.23 69.25 77.51 74.84 79.92 81.03 86.62 88.13 93.86\nHi-Ml 57.75 59.38 56.31 65.67 58.02 71.19 61.01 75.59 69.11 82.54\nHi-Ta 54.63 60.12 56.69 63.38 57.46 66.17 59.36 67.17 60.41 70.62\nHi-Te 53.21 58.18 56.19 63.90 64.15 67.70 65.19 70.65 66.10 74.92\nHi-Ne 70.78 71.23 74.30 78.11 72.19 83.20 79.70 85.01 84.69 90.95\nHi-Ur 69.94 71.25 70.01 72.35 72.03 76.59 71.07 78.27 73.99 80.99\n\nTable 2: Results after combining chunks of WNData with PCData\n\n4 Results\n\nWe average the similarity scores obtained using\nthe three methodologies (NED, Cos, and JWS) de-\nscribed above, for each word pair, and then use\nthese as training labels for cognate detection mod-\nels. We obtain results using the networks de-\nscribed above and report them in Table 1. We\ncalculate average scores for both models and both\ndatasets and show the chart in Figure 5. We ob-\nserve that RNN outperforms FFN for both the\ndatasets across all language pairs (see Figure 5).\nWe also find that Hi-Sa (see Figure 5) has the\nbest cognate detection accuracy among all lan-\nguage pairs (for both RNN and FFN), which is in\nline with the fact that they are closely related lan-\nguages when compared to other Indian language\npairs. We observe that average scores for WNData\nare always higher than average scores for PCData\nfor all language pairs (Figure 5). Also, in line\nwith our observations above, the overall average\nof RNN scores for both datasets are even higher\nthan average FFN scores (Figure 5).\n\nWe perform another set of experiments by\ncombining non-redundant word pairs from both\ndatasets. We add WNData in chunks of 20 per\ncent to PCData for each language pair and create\nseparate word lists with average similarity scores.\nWe use FFN to train and perform a stratified 5-\nfold evaluation for each language pair after adding\neach chunk and show the results in Table 2. Af-\nter evaluating our results for FFN, we perform\nthe same training and evaluation with RNN. We\nobserve that adding complete WNData to PC-\ndata improves our performance drastically and\ngiven us the best results for almost all cases.\n\nWNPairs CorpPairs Matches\nHi-Bn 324537 505721 17402\nHi-Pa 260123 465140 16325\nHi-Mr 322013 555719 17698\nHi-Gu 423030 542311 17005\nHi-Sa 669911 248421 10109\nHi-Ml 353104 315234 12392\nHi-Ta 225705 248207 7112\nHi-Te 369872 431869 7599\nHi-Ne 191701 420176 11264\nHi-Ur 99803 420176 6509\n\nTable 3: Total Word Pairs for both datasets and\nMatches among them\n\nOnly in case of Hi-Bn, when using the FFN for\ntraining, PCData combined with 80% WNData\nperforms better than 100% Data; possibly due to\nadded sparsity of the additional data. Our hypoth-\nesis that adding WNData to PCdata improves the\nperformance holds for all the other cases, includ-\ning when trained using RNN.\n\n5 Discussion and Analysis\n\nA parallel corpus is a costly resource to obtain\nin terms of both time and effort. For resource-\nscarce languages, parallel corpora cannot easily\nbe crawled. We wanted to validate how crucial\nWordnets are as a resource and can they act as a\nsubstantial dataset in the absence of parallel cor-\npora. In addition to validating the performance of\nchunks of WNData combined with PCData, we\nalso calculated the exact matches of word pairs\nfrom both the datasets and show the results in\nTable 3. We observed that Hi-Mr had the most\n\n\n\nFigure 5: Average Results using Neural Network models on both datasets\n\nSource Word Target Word Meaning Cos NED JWS\ntadanukool tadanusaar accordingly 0.500 0.571 0.482\n\nyogadaan karna yogadaan karane to contribute 0.631 0.636 0.593\nduraatma dushtaatama evil soul 0.629 0.700 0.648\n\nTable 4: Manual analysis of the similarity scores\n\nmatched pairs amongst all the languages. PC-\nData is extracted from parallel corpora and is not\nstemmed for root words, whereas WNData is ex-\ntracted from IndoWordnet and only contains root\nwords. Despite many words with morphological\ninflections, we were able to obtain exactly match-\ning words, amongst the datasets. WNData consti-\ntutes a fair chunk of root words used in PCData as\nwell, and this validates the fact that models trained\non WNData can be used to detect cognate word\npairs from any standard parallel corpora as well.\n\nIt is a well-established fact that Indian lan-\nguages are spoken just like they are written and\nunlike their western counterparts are not spoken\nand spelled differently. Hence, we choose to per-\nform cognate detection using orthographic simi-\nlarity methods. This very nature of Indian lan-\n\nguages allows us to eliminate the need for using\naspects of Phonetic similarity to detect true cog-\nnates. Most of the Indian languages borrow words\nfrom Sanskrit in either of the two forms - tatsama\nor tadbhava. When a word is borrowed in tatsama\nform, it retains its spelling, but in case of tadb-\nhava form, the spelling undergoes a minor change\nto complete change. Before averaging the similar-\nity scores, we tried to observe which of the three\n(NED, JWS, or Cos) scores would perform better\nfor true cognates known to us in tadbhava form\nwith minor spelling changes. We analysed individ-\nual word pairs from the data and presented a small\nsample of our analysis in Table 4. We observe that\nNED consistently outperforms Cos and JWS for\ncognate word pairs and confirmed that NED based\nsimilarity is the most suited metric for cognate\n\n\n\ndetection (Rama et al., 2015). We also observe\nthat our methodology can handle word pairs with-\nout any changes and with minor spelling changes\namong cognates, the total of which, constitutes a\nlarge portion of the cognates among Indian Lan-\nguage pairs.\n\n6 Conclusion and Future Work\n\nIn this paper, we investigate cognate detection for\nIndian Language pairs (Hi-Bn, Hi-Gu, Hi-Pa, Hi-\nMr, Hi-Sa, Hi-Ml, Hi-Ta, Hi-Te, Hi-Ne, and Hi-\nUr). A pair of words is said to be Cognates if they\nare etymologically related; and True Cognates, if\nthey carry the same meaning as well. We know\nthat parallel concepts, bearing the same sense in\nlinked WordNets, are etymologically related. We,\nthen, use the measures of orthographic similar-\nity to find probable Cognates among parallel con-\ncepts. We perform the same task for a parallel\ncorpus and then train neural network models on\nthis data to perform automatic cognate detection.\nWe compute a list of True Cognates and release\nthis data along with the data processed previously.\nWe observe that Recurrent Neural Networks are\nbest suited for this task. We observe that Hindi -\nSanskrit language pair, being the closest, has the\nhighest percentage of cognates among them. We\nobserve that RNN, which treats the words as a se-\nquence of characters, outperforms FFN for all the\nlanguage pairs and both the datasets. We validate\nthat Wordnets can play a crucial role in detecting\ncognates by combining the datasets for improved\nperformance. We observe a minor, but crucial,\nincrease in the performance of our models when\nchunks of Wordnet data are added to the data gen-\nerated from the parallel corpora thus confirming\nthat Wordnets are a crucial resource for Cognate\nDetection task. We also calculate the matches be-\ntween word pairs from the Wordnet data and the\nword pairs from the parallel corpora to show that\nWordnet data can form a significant part of paral-\nlel corpora and thus can be used in the absence of\nparallel corpora.\n\nIn the near future, we would like to use cross-\nlingual word embeddings, include more Indian\nlanguages, and investigate how semantic similar-\nity could also help in cognate detection. We\nwill also investigate the use of Phonetic Similar-\nity based methods for Cognate detection. We shall\nalso study how our cognate detection techniques\ncan help infer phylogenetic trees for Indian lan-\n\nguages. We would also like to combine the sim-\nilarity score by providing them weights based on\nan empirical evaluation of their outputs and extend\nour experiments to all the Indian languages.\n\nAcknowledgement\n\nWe would like to thank the reviwers for their time\nand insightful comments which helped us improve\nthe draft. We would also like to thank CFILT lab\nfor its resources which helped us perform our ex-\nperiments and its members for reading the draft\nand helping us improve it.\n\nReferences\n[Bhargava and Kondrak2009] Aditya Bhargava and\n\nGrzegorz Kondrak. 2009. Multiple word align-\nment with profile hidden markov models. In\nProceedings of Human Language Technologies:\nThe 2009 Annual Conference of the North American\nChapter of the Association for Computational\nLinguistics, Companion Volume: Student Research\nWorkshop and Doctoral Consortium, pages 43\u201348.\nAssociation for Computational Linguistics.\n\n[Brew et al.1996] Chris Brew, David McKelvie, et al.\n1996. Word-pair extraction for lexicography. In\nProceedings of the 2nd International Conference on\nNew Methods in Language Processing, pages 45\u201355.\n\n[Ciobanu and Dinu2014] Alina Maria Ciobanu and\nLiviu P Dinu. 2014. Automatic detection of cog-\nnates using orthographic alignment. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), volume 2, pages 99\u2013105.\n\n[Ciobanu and Dinu2015] Alina Maria Ciobanu and\nLiviu P Dinu. 2015. Automatic discrimination be-\ntween cognates and borrowings. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), volume 2, pages 431\u2013437.\n\n[Crystal2008] DA Crystal. 2008. Dictionary of linguis-\ntics and phonetics 6th edition crystal. DA Crystal\u2013\nOxford: Blackwell Publishing.\n\n[Inkpen et al.2005] Diana Inkpen, Oana Frunza, and\nGrzegorz Kondrak. 2005. Automatic identification\nof cognates and false friends in french and english.\nIn Proceedings of the International Conference Re-\ncent Advances in Natural Language Processing, vol-\nume 9, pages 251\u2013257.\n\n[Ja\u0308ger et al.2017] Gerhard Ja\u0308ger, Johann-Mattis List,\nand Pavel Sofroniev. 2017. Using support vec-\ntor machines and state-of-the-art algorithms for pho-\nnetic alignment to identify cognates in multi-lingual\nwordlists. In Proceedings of the 15th Conference of\n\n\n\nthe European Chapter of the Association for Com-\nputational Linguistics: Volume 1, Long Papers, vol-\nume 1, pages 1205\u20131216.\n\n[Jaro1989] Matthew A Jaro. 1989. Advances in record-\nlinkage methodology as applied to matching the\n1985 census of tampa, florida. Journal of the Amer-\nican Statistical Association, 84(406):414\u2013420.\n\n[Ja\u0308rvelin et al.2007] Anni Ja\u0308rvelin, Antti Ja\u0308rvelin, and\nKalervo Ja\u0308rvelin. 2007. s-grams: Defining general-\nized n-grams for information retrieval. Information\nProcessing & Management, 43(4):1005\u20131019.\n\n[Jha2010] Girish Nath Jha. 2010. The tdil program\nand the indian langauge corpora intitiative (ilci). In\nLREC.\n\n[Kanojia et al.2019] Diptesh Kanojia, Malhar Kulkarni,\nPushpak Bhattacharyya, and Gholemreza Haffari.\n2019. Cognate identification to improve phyloge-\nnetic trees for indian languages. In Proceedings of\nthe ACM India Joint International Conference on\nData Science and Management of Data, pages 297\u2013\n300. ACM.\n\n[Koehn and Knight2000] Philipp Koehn and Kevin\nKnight. 2000. Estimating word translation prob-\nabilities from unrelated monolingual corpora using\nthe em algorithm. In AAAI\/IAAI, pages 711\u2013715.\n\n[Kondrak2000] Grzegorz Kondrak. 2000. A new algo-\nrithm for the alignment of phonetic sequences. In\nProceedings of the 1st North American chapter of\nthe Association for Computational Linguistics con-\nference, pages 288\u2013295. Association for Computa-\ntional Linguistics.\n\n[Kondrak2001] Grzegorz Kondrak. 2001. Identify-\ning cognates by phonetic and semantic similarity.\nIn Proceedings of the second meeting of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics on Language technologies, pages\n1\u20138. Association for Computational Linguistics.\n\n[List2010] Johann-Mattis List. 2010. Sca: phonetic\nalignment based on sound classes. In New Direc-\ntions in Logic, Language and Computation, pages\n32\u201351. Springer.\n\n[List2012] Johann-Mattis List. 2012. Lexstat:\nAutomatic detection of cognates in multilingual\nwordlists. In Proceedings of the EACL 2012 Joint\nWorkshop of LINGVIS & UNCLH, pages 117\u2013125.\nAssociation for Computational Linguistics.\n\n[Makin et al.2007] Ranbeer Makin, Nikita Pandey,\nPrasad Pingali, and Vasudeva Varma. 2007. Ap-\nproximate string matching techniques for effective\nclir among indian languages. In International Work-\nshop on Fuzzy Logic and Applications, pages 430\u2013\n437. Springer.\n\n[Melamed1999] I Dan Melamed. 1999. Bitext maps\nand alignment via pattern recognition. Computa-\ntional Linguistics, 25(1):107\u2013130.\n\n[Meng et al.2001] Helen M Meng, Wai-Kit Lo, Berlin\nChen, and Karen Tang. 2001. Generating pho-\nnetic cognates to handle named entities in english-\nchinese cross-language spoken document retrieval.\nIn IEEE Workshop on Automatic Speech Recog-\nnition and Understanding, 2001. ASRU\u201901., pages\n311\u2013314. IEEE.\n\n[Mielke et al.2012] Michelle M Mielke, Rosebud O\nRoberts, Rodolfo Savica, Ruth Cha, Dina I Drubach,\nTeresa Christianson, Vernon S Pankratz, Yonas E\nGeda, Mary M Machulda, Robert J Ivnik, et al.\n2012. Assessing the temporal relationship between\ncognition and gait: slow gait predicts cognitive de-\ncline in the mayo clinic study of aging. Journals\nof Gerontology Series A: Biomedical Sciences and\nMedical Sciences, 68(8):929\u2013937.\n\n[Mulloni and Pekar2006] Andrea Mulloni and Viktor\nPekar. 2006. Automatic detection of orthograph-\nics cues for cognate recognition. In LREC, pages\n2387\u20132390.\n\n[Mulloni2007] Andrea Mulloni. 2007. Automatic pre-\ndiction of cognate orthography using support vector\nmachines. In Proceedings of the 45th Annual Meet-\ning of the ACL: Student Research Workshop, pages\n25\u201330. Association for Computational Linguistics.\n\n[Nerbonne and Heeringa1997] John Nerbonne and\nWilbert Heeringa. 1997. Measuring dialect distance\nphonetically. In Computational Phonology: Third\nMeeting of the ACL Special Interest Group in\nComputational Phonology.\n\n[Pranav2018] A Pranav. 2018. Alignment analysis of\nsequential segmentation of lexicons to improve au-\ntomatic cognate detection. In Proceedings of ACL\n2018, Student Research Workshop, pages 134\u2013140.\n\n[Rama et al.2015] Taraka Rama, Lars Borin,\nGK Mikros, and J Macutek. 2015. Compara-\ntive evaluation of string similarity measures for\nautomatic language classification.\n\n[Rama et al.2018] Taraka Rama, Johann-Mattis List,\nJohannes Wahle, and Gerhard Ja\u0308ger. 2018. Are au-\ntomatic methods for cognate detection good enough\nfor phylogenetic reconstruction in historical linguis-\ntics? arXiv preprint arXiv:1804.05416.\n\n[Rama2016] Taraka Rama. 2016. Siamese convolu-\ntional networks for cognate identification. In Pro-\nceedings of COLING 2016, the 26th International\nConference on Computational Linguistics: Techni-\ncal Papers, pages 1018\u20131027.\n\n[Salton and Buckley1988] Gerard Salton and Christo-\npher Buckley. 1988. Term-weighting approaches\nin automatic text retrieval. Information processing\n& management, 24(5):513\u2013523.\n\n[Turchin et al.2010] Peter Turchin, Ilia Peiros, and\nMurray Gell-Mann. 2010. Analyzing genetic con-\nnections between languages by matching consonant\nclasses. (5):117\u2013126.\n\n\n\n[Winkler1990] William E Winkler. 1990. String com-\nparator metrics and enhanced decision rules in the\nfellegi-sunter model of record linkage.\n\n[Wu et al.2008] Ho Chung Wu, Robert Wing Pong Luk,\nKam Fai Wong, and Kui Lam Kwok. 2008. Inter-\npreting tf-idf term weights as making relevance de-\ncisions. ACM Transactions on Information Systems\n(TOIS), 26(3):13.\n\n\n\t1 Introduction\n\t1.1 Contributions\n\n\t2 Related Work\n\t3 Datasets and Methodology\n\t3.1 Datasets\n\t3.2 Script Standardization and Text Normalization\n\t3.3 Similarity Scores Calculation\n\t3.4 Similarity Measures\n\t3.5 Models\n\t3.5.1 Feed Forward Neural Network (FFN)\n\t3.5.2 Recurrent Neural Network (RNN)\n\n\n\t4 Results\n\t5 Discussion and Analysis\n\t6 Conclusion and Future Work\n\n","1":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 2020\n\nSentence Embeddings and High-speed\nSimilarity Search for Fast Computer\n\nAssisted Annotation of Legal Documents\n\nHannes WESTERMANN a,1, Jarom\u0131\u0301r S\u030cAVELKA b, Vern R. WALKER c,\nKevin D. ASHLEY d and Karim BENYEKHLEF a\n\na Cyberjustice Laboratory, Faculte\u0301 de droit, Universite\u0301 de Montre\u0301al\nb School of Computer Science, Carnegie Mellon University\n\nc LLT Lab, Maurice A. Deane School of Law, Hofstra University\nd School of Computing and Information, University of Pittsburgh\n\nAbstract. Human-performed annotation of sentences in legal documents is an im-\nportant prerequisite to many machine learning based systems supporting legal tasks.\nTypically, the annotation is done sequentially, sentence by sentence, which is of-\nten time consuming and, hence, expensive. In this paper, we introduce a proof-of-\nconcept system for annotating sentences \u201claterally.\u201d The approach is based on the\nobservation that sentences that are similar in meaning often have the same label\nin terms of a particular type system. We use this observation in allowing annota-\ntors to quickly view and annotate sentences that are semantically similar to a given\nsentence, across an entire corpus of documents. Here, we present the interface of\nthe system and empirically evaluate the approach. The experiments show that lat-\neral annotation has the potential to make the annotation process quicker and more\nconsistent.\n\nKeywords. Annotation, Language Models, Sentence Embeddings, Approximate\nNearest Neighbour, Interactive Machine Learning\n\n1. Introduction\n\nA lot of AI & Law research is enabled by annotation of legal texts. The annotation\ncan be performed on several levels of textual units, such as the entire document, the\nparagraph, or an individual sentence. In this work, we focus on annotations performed\non the sentence level. AI & Law research has employed a variety of annotation schemes\non the sentence level, such as the annotation of:\n\n\u2022 the rhetorical roles sentences play in a legal case (such as factual circumstances, a\nlegal rule or an application of a legal rule to factual circumstances);\n\n\u2022 the presence or absence of a certain factual circumstance the sentence describes\n(such as whether a security measure was present in a trade-secret case);\n\n\u2022 the type and attributes of contractual clauses (such as the kind of liability ad-\ndressed in a certain clause); and\n\n1Corresponding Author: Hannes Westermann, E-mail: hannes.westermann@umontreal.ca\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n11\n49\n\n4v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 2\n\n1 \nD\n\nec\n 2\n\n02\n1\n\n\n\nDecember 2020\n\n\u2022 the relevance of a sentence retrieved from a legal case to interpret a statutory term.\n\nAn annotated corpus of documents has many useful applications. For instance, a classi-\nfication algorithm may be trained to infer labels for new sentences in a larger corpus of\ndocuments. This may lead, for example, to insights about the distribution of clauses in\na large data set of contracts, to improved predictions about the outcome of a legal case\nfrom factors, or to ranking documents by relevance to a particular search query.\n\nTypically, annotation of documents is performed by one or several annotators using\na tool that allows them to review one document at a time, and to sequentially assign\na label for each sentence as it occurs in that document. This approach has significant\ndrawbacks. First, it is inefficient because annotating large corpora in this way takes a long\ntime and is expensive. The label has to be determined from scratch for each sentence,\ncausing significant cognitive overhead. Second, the annotations might be inconsistent\nacross similar sentences. Since annotators often work through thousands of sentences,\nthey may not remember how a certain sentence type was annotated the last time they\nsaw it. If multiple annotators are involved, this problem may be exacerbated, as similar\nsentences are reviewed by different annotators.\n\nIn this paper, we investigate an alternative approach which we call \u201clateral annota-\ntion.\u201d Similarly to the traditional approach, annotators use a tool to view documents and\nlabel sentences. However, given any sentence there is an option to see sentences across\nthe entire corpus (or from the rest of the document) that are semantically similar to the\nfocused sentence. This feature uses sentence encoders based on deep learning models\nand libraries to quickly deliver semantically similar sentences using approximate nearest\nneighbour searches. The annotator then has the option of reviewing these similar sen-\ntences and assigning labels to one or more of them. Although the computer system as-\nsists the user by showing similar sentences, the choice of how to label a sentence ulti-\nmately rests with the annotator. It is therefore a hybrid approach, using machine learning\nto support human annotators with their task.\n\nLegal language is often formalized and uses recurring linguistic structures. This\nmeans that identical, or very similar, sentences often appear in many documents. For\nexample, contract clauses specifying a certain type of liability might often use the same\nwords, syntax and sentence structure. Lateral annotation makes use of this attribute of\nlegal language by allowing the annotator to label all similar sentences at one time. This\ncan increase the speed of annotation. Since all similar sentences can be labelled at once,\nthe consistency of the annotations is also likely to increase. Consequently this approach\ncan significantly ease the important task of labelling large data sets in the legal domain.\n\n2. Related Work\n\nBranting et al. [4,5] proposed a semi-supervised approach to annotation of case deci-\nsions. The approach is based on several observations about the consistency of language\nacross separate cases and within different sections of the same case. The researchers an-\nnotated a small set of decisions and calculated the mean of the semantic vectors [2,17]\nof all the spans annotated by a given tag (the \u201ctag centroid\u201d). The annotations were then\nprojected to semantically similar sentences in the entire corpus to enable explainable\nprediction. In our work, we describe a hybrid method where we show the semantically\nsimilar sentences to an annotator for rapid and reliable annotation.\n\n\n\nDecember 2020\n\nA steady line of work in AI & Law focuses on making the annotation effort more\neffective. Westermann et al. [26] proposed and assessed a method for building strong,\nexplainable classifiers in the form of Boolean search rules. Employing an intuitive inter-\nface, the user develops Boolean rules for matching instead of annotating the individual\nsentences. Here, we replace the Boolean matching rules with sentence semantic similar-\nity. Instead of developing the rules, the user confirms that the semantically similar sen-\ntences should be labeled as instances of the same types. S\u030cavelka and Ashley [21] evalu-\nated the effectiveness of an approach where a user labels the documents by confirming\n(or correcting) the prediction of a ML algorithm (interactive approach). The application\nof active learning has been explored in the context of classification of statutory provi-\nsions [25] and eDiscovery [8,9]. Hogan et al. [13] proposed and evaluated a human-aided\ncomputer cognition framework for eDiscovery. Tools to retrieve and rank text fragment\nby similarity for coding have further been implemented in qualitative data analysis tools,\nsuch as QDA Miner2 and Nvivio.3\n\n3. Proposed Framework\n\nWe investigate a system that enables an annotator to perform lateral annotations on a\ncorpus of documents. We use sentence embeddings to capture the meaning of sentences,\nand then use approximate nearest neighbour search to find sentences that are semanti-\ncally similar to a source sentence. This enables us to provide the annotators with viable\nsentence candidates for annotation in sub-second time.\n\n3.1. Sentence Embeddings\n\nIn order to search for similar sentences based on an original sentence, we need a way to\nstore sentences in a vector format that makes comparison easy. There are several ways\nof representing sentences in this way.\n\nA bag of words representation (e.g., TF-IDF) is a simple but effective way to encode\nthe meaning of a sentence. It has, however, at least two notable disadvantages: an enor-\nmously large feature space and the inability to account for the relatedness of meaning\nin different words. This means that sentences with the same meaning may be deemed to\nbe completely different if they use largely non-overlapping vocabulary (e.g., synonyms).\nThis is problematic for applications where sentence similarity is a key component (as in\nthis work). In our experiments, we include the representation as a strong baseline due to\nits simplicity and wide usage.\n\nMore recently, pre-trained word embeddings and language models have gained pop-\nularity in creating word embeddings. These representations are motivated by the so-\ncalled distributional hypothesis: words that are used and occur in the same contexts tend\nto have similar meanings. [12] The idea that \u201ca word is characterized by the company\nit keeps\u201d was popularized by Firth. [11] The gist of the method is that words with sim-\nilar meanings are projected onto similar vectors, by analyzing massive corpora of text\nto learn the distributions. There are several ways of combining these word vectors to\nproduce sentence vectors, of which we have chosen three:\n\n2provalisresearch.com\/products\/qualitative-data-analysis-software\n3www.qsrinternational.com\/nvivo-qualitative-data-analysis-software\/home\n\nprovalisresearch.com\/products\/qualitative-data-analysis-software\n\n\nDecember 2020\n\n1. Cer et al. [6] use the transformer architecture [22] and Deep Averaging Network\n[15] trained on the SNLI dataset. We work with the implementation released by\nthe authors as the Google Universal Sentence Encoder (GUSE).4\n\n2. Reimers et al. [19] build on top of BERT [10] and RoBERTa [18], which have\nbeen shown to be remarkably effective on a number of NLP tasks. Specifically,\nthey use siamese and triplet network structures to derive semantically meaningful\nsentence embeddings. The authors released the models as Sentence Transformers\n(ST). We use this implementation in our work.5\n\n3. Conneau et al. [7] demonstrate the effectiveness of models trained on a natural\nlanguage inference task (Stanford NLI dataset [3]). They propose a BiLSTM\nnetwork with max pooling trained with fastText word embeddings [2,17] as the\nbest universal sentence encoding method. We adapt the implementation released\nby the authors which is commonly referred to as InferSent.6\n\n3.2. Efficient Similarity Search over High-dimensional Vectors\n\nDocument search traditionally relies on a combination of relational databases built on\nstructured data (metadata search) and inverted indexes (full-text search). These cannot\ndeal efficiently with the vectors that represent documents\u2019 meaning. The brute-force ap-\nproaches to indexing (i.e., store the results for all documents) or querying (i.e., compare\nthe query to each data point) do not scale well beyond fairly small data sets.\n\nIn order to achieve semantic similarity search with the desired properties, one has to\ntackle the problem of preprocessing a set of n data points P = {p1, p2 . . . , pn} in some\nmetric space X (e.g., the d-dimensional Euclidean space Rd) so as to efficiently answer\nqueries by finding the point in P closest to a query point q \u2208 X. Solutions to this well-\nstudied problem in other domains can be readily applied in our context. [14] For example,\nimages and videos have become a massive source of data for indexing and search. And\nsince it is often not practical to manually annotate the data to enable the use of relational\ndatabases, a search in some sort of a vector space remains the only option.\n\nJohnson et al. [16] proposed a system that allows efficient indexing and search over\ncollections containing around 1 billion documents. This shows that the current state-\nof-the-art is capable of supporting virtually any practical scenario in a legal annotation\ndomain. For example, S\u030cavelka et al. [20] segmented the whole corpus of US case-law\ninto 0.5 billion sentences. While the technique in [16] would allow for efficient semantic\nsimilarity search over such a collection, it is most likely several magnitudes larger than\nany realistic legal annotation task. We utilize the Annoy similarity search library released\nby Spotify7 for its ease of use and minimal system requirements. Annoy is an efficient\nimplementation of the Approximate Nearest Neighbors algorithm proposed in [14].\n\n3.3. Lateral Annotation\n\nSemantic sentence embeddings and efficient vector similarity search are combined to\nenable lateral annotation. We have developed a prototype interface called CAESAR,\n\n4https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\n5github.com\/UKPLab\/sentence-transformers\n6github.com\/facebookresearch\/InferSent\n7github.com\/spotify\/annoy\n\nhttps:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\ngithub.com\/UKPLab\/sentence-transformers\ngithub.com\/facebookresearch\/InferSent\ngithub.com\/spotify\/annoy\n\n\nDecember 2020\n\nFigure 1. A screenshot of the prototype Computer-Assisted Efficient Semantic Annotation & Ranking (CAE-\nSAR) Interface\n\nComputer-Assisted Enhanced Semantic Annotation & Ranking, to demonstrate this ca-\npability. The sentence embedding frameworks are used to create semantic embeddings\nof all sentences in a corpus. These are then used to create an index for fast similarity\nsearch.\n\nThe capability is provided to the user through an annotation interface (see Figure 1).\nThe interface allows the annotator to define a schema of labels in a hierarchical structure\n(i.e. a type system), and to tag individual sentences with these labels. For each sentence, it\nis possible to retrieve semantically similar sentences, using the methods described above.\nThese are shown in the sidebar to the right, sorted by similarity in descending order.\nThis panel allows the annotator to perform lateral annotations by quickly annotating the\nsentences shown, or to see the context of the shown sentences before annotation.\n\nWe envision the following procedure for labeling sentences using CAESAR. An\nannotator starts with the first document, and labels the first sentence. Then, he asks to\nbe shown similar sentences in the sidebar. He then labels sentences in the sidebar until\nsentences are no longer similar enough to quickly allow the annotator to determine that\nthey are of the same class. The annotator then returns to the full text of the case and labels\nthe next sentence. As the annotator moves to the next documents, many of the sentences\nmay already be labelled, and can be skipped.\n\nThis method of improving annotation efficiency is completely unsupervised. It can\nbe implemented before having started any kind of annotation, by relying on the so-\nphisticated neural models trained on huge datasets of general texts (e.g., news corpora,\nWikipedia). Despite this, the method seems to perform very well on legal annotation\ntasks, as demonstrated in Section 4.\n\n\n\nDecember 2020\n\n4. Evaluation\n\n4.1. Datasets\n\nIn order to evaluate the lateral annotation method, we use three existing data sets:\n\n1. Walker et al. [23] analyzed 50 fact-finding decisions issued by the U.S. Board\nof Veterans\u2019 Appeals (BVA) from 2013 through 2017, all arbitrarily selected\ncases dealing with claims by veterans for service-related post-traumatic stress\ndisorder (PTSD). For each of the 50 BVA decisions in the PTSD dataset, the re-\nsearchers extracted all sentences addressing the factual issues related to the claim\nfor PTSD, or for a closely-related psychiatric disorder. These were tagged with\nthe rhetorical roles the sentences play in the decision [24]. We conducted our\nexperiments on this set of 6,153 sentences.8\n\n2. S\u030cavelka et al. [20] studied methods for retrieving useful sentences from court\nopinions that elaborate on the meaning of a vague statutory term. To support their\nexperiments they queried the database of sentences from case law that mentioned\nthree terms from different provisions of the U.S. Code. They manually classified\nthe sentences in terms of four categories with respect to their usefulness for the\ninterpretation of the corresponding statutory term. In [20] the goal was to rank\nthe sentences with respect to their usefulness; here, we classify them into the four\nvalue categories (StatInt).9\n\n3. Bhattacharya et al. [1] analyzed 50 opinions of the Supreme Court of India. The\ncases were sampled from 5 different domains in proportion to their frequencies\n(criminal, land and property, constitutional, labor and industrial, and intellec-\ntual property). From each of the 50 decisions the sentence boundaries were de-\ntected using an off-the-shelf general tool.10 Then the 9,380 sentences were man-\nually classified into one of the seven categories according to the rhetorical roles\nthey play in a decision. Our experiments were conducted on this set of sentences\n(IndSC).11\n\n4.2. Experiments\n\nIn order to evaluate the effectiveness of lateral annotation and compare different embed-\nding methods, we use our system to retrieve the closest sentences to a query sentence,\nand investigate how many of them have the same label as the source sentence. Assuming\nthat lateral annotation is more efficient than sequential annotation, the more retrieved\nsentences that have the same label, the more efficiently the annotator will be able to\nannotate the data set.\n\nWe report several metrics. First, we investigate the length of chains of annotations\nby traversing the retrieved sentences, from the most similar to the least, until we arrive\nat a label that does not match the label of the source sentence. We calculate the longest\nencountered chain (Max) and the average chain length (Avg) for each data set and em-\n\n8Dataset available at github.com\/LLTLab\/VetClaims-JSON\n9Dataset available at github.com\/jsavelka\/statutory_interpretation\n10spacy.io\n11Dataset available at github.com\/Law-AI\/semantic-segmentation\n\ngithub.com\/LLTLab\/VetClaims-JSON\ngithub.com\/jsavelka\/statutory_interpretation\nspacy.io\ngithub.com\/Law-AI\/semantic-segmentation\n\n\nDecember 2020\n\nbedding method. Second, we determine how many of the top 20 closest sentences have\nthe same label as the source sentence (P@20) - a measure of precision.\n\nThird, we visualize the high-dimensional GUSE embeddings of all sentences in a\ndataset, reduced to 2 dimensions using a Principal Component Analysis. The colors in the\nresulting visualization correspond to the gold standard labels for the individual sentences.\nThe most important feature of an embedding space for our purpose is that each sentence\nshould be surrounded by multiple sentences with the same label, e.g. the same color.\n\n4.3. Results\n\nTable 1 presents the Max, Avg, and P@20 statistics for each data set and for each embed-\nding method. Overall, the sentence embeddings seem to capture enough linguistic infor-\nmation to achieve improvement in all three metrics, without any training on the domain-\nspecific data sets. The neural models perform much better than the random baseline, and\nperform better or equal to the TF-IDF baseline.\n\nLooking at the individual data sets, it seems like the Board of Veterans\u2019 Appeals\ndata set benefits significantly from lateral annotation. On average, 70% of the closest 20\nsentences to each sentence have the same label, meaning an annotator can likely annotate\nthese sentences laterally. This could offer a significant speed-up in the annotation of such\na data set.\n\nLooking at the individual labels in Table 2, the \u201cCitation\u201d label seems by far the\neasiest to annotate laterally. 94% of the top 20 closest sentences to a citation sentence are\nalso citation sentences. This is likely due to the special tokens (such as parentheses, year\nnumbers and special words such as \u201cSee\u201d) in these sentences. \u201cRule\u201d also performs very\nwell, which might be due to the same rule being cited in multiple cases. The embeddings\ncapture these distinctions well, which can be seen in Figure 2, where sentences of the\nsame type seem clearly concentrated in certain areas.\n\nIn the Statutory Interpretation data set the technique appears most suitable for the\nsentences labeled as \u201cNo value.\u201d This makes sense since these are mostly sentences that\nfully or partially quote or paraphrase a statutory provision. Hence, these sentences are\noften very similar to each other. The middle graph in Figure 2 confirms this observation.\nThree compact red clusters clearly correspond to the \u201cNo value\u201d sentences associated\nwith the three terms of interest. The sentences with the other three labels are somewhat\nmore challenging. Yet, even for the more challenging categories, a significant amount of\nsentences could still be annotated laterally, as seen in Table 2.\n\nThe Indian Supreme Court data set is where lateral annotation gives the least ad-\nvantage, with our models retrieving under 40% of matching sentences in the top 20 posi-\ntions. On average, each sentence seems to be next to only 2.1-2.4 sentences of the same\nclass in the embedding space. This can also be seen in the PCA visualization in figure 2.\nUnlike the other data sets, the sentence embeddings do not seem to result in clearly sepa-\nrate classes. The comparative difficulty of separating this data set might be explained by\nthe fact that the sentences are selected from five different domains, and assigned seven\nlabels\u2014more than the other two data sets. The \u201cRatio\u201d and \u201cFacts\u201d sections seem slightly\neasier to annotate in a lateral fashion, which might be due to a consistent structure or\ncontent of these sentences. It is surprising that the \u201cRatio\u201d class has a high precision,\nwhile the \u201cRuling of lower court\u201d has low precision, but this matches the findings of the\nauthors in [1] for classification difficulty.\n\n\n\nDecember 2020\n\nStatistics\nBVA StatInt IndSC\n\nMax Avg P@20 Max Avg P@20 Max Avg P@20\n\nRandom 9 1.35 .24 18 2 .45 10 1.36 .24\nTF-IDF 195 13.73 .59 197 24.40 .70 27 2.16 .36\nGUSE 696 55.92 .70 257 25.80 .73 45 2.43 .37\nST 710 48.62 .68 277 30.16 .69 30 2.22 .35\nInferSent 863 83.92 .70 204 22.5 .66 45 2.41 .38\n\nTable 1. Statistics for different sentence embedding methods, including evaluation of chains of lateral annota-\ntion (Max, Avg) and how many of the 20 closest sentences on average have the same label (P@20).\n\nBVA SID ISC\nLabel P@20 Label P@20 Label P@20\n\nSentence 0.60 No Value 0.89 Facts 0.40\nFinding 0.49 Potential 0.58 Ruling (lower) 0.08\nEvidence 0.76 Certain 0.15 Argument 0.18\nRule 0.73 High 0.33 Ratio 0.46\nCitation 0.94 Statute 0.25\nReasoning 0.27 Precedent 0.32\n\nRuling (present) 0.32\nTable 2. The ratio of matching labels among the top 20 most similar sentences, per label\n\nFigure 2. Visualizations of the sentences across the data sets, embedded using the GUSE and reduced to two\ndimensions using a Principal Component Analysis. The colors correspond to different labels.\n\n5. Discussion\n\nWe have introduced and evaluated a lateral annotation framework. We experimented with\nfour types of sentence embeddings and compared them against the random baseline. All\nof the embeddings showed significant improvements in selecting sentences that are of\nthe same class compared to the random baseline. The neural models show strong perfor-\nmance across the three data sets. In general, they perform similarly, although the Google\nUniversal Sentence Encoder and InferSent seems to have a slight edge. In the BVA data\nset, the neural models clearly outperform the TF-IDF baseline, while the performance is\nmore balanced in the StatInt and IndSC data sets. Even in the most challenging data set,\nalmost 40% of the 20 closest sentences had the same label as a source sentences. For the\n\n\n\nDecember 2020\n\nother data sets, this number was 70%. This indicates a significant potential benefit for\nusing lateral annotation.\n\nDifferent areas might benefit from the use of lateral annotations. The assumption\nbehind the method is that sentences that have similar semantic embeddings are likely to\nbelong to the same class that an annotator is aiming to label. This should work better for\nlabeling schemas and data sets where the semantic properties of a sentence are linked to\nits label, and where the homogeneity of sentences with the same label is high. This can be\nseen in the per-class analysis of precision, showing that citation sentences and recitation\nof previous rules and cases are more suitable for lateral annotation. Sentences with less\ninherent structure and similarity, such as reasoning sentences, seem to perform slightly\nworse. The Indian Supreme Court data set, which draws from five different domains and\nuses seven classes, performs worse in a lateral annotation context, which could indicate\nthat more diverse data sets are more difficult to annotate laterally.\n\nFurther, the method benefits from data sets where the set of sentences with a partic-\nular label is made up of several clusters of semantically similar sentences that the anno-\ntator can efficiently scope. For each sentence that is part of such a cluster, the annotator\ncan efficiently label a large number of sentences. Outlier sentences, which do not belong\nto any larger cluster of sentences with the same label, are less likely to benefit from the\nmethod, as they do not assist the annotator in finding other sentences of the same label.\n\nWe hypothesize that the legal domain is well-suited for the lateral annotation\nmethod. Legal practitioners often use stereotypical language to describe certain facts, in-\ncluding a shared vocabulary and sentence structure. This shared language is more likely\nto be suited for annotation supported by semantic similarity search, and could signifi-\ncantly speed-up annotating large data sets with per-sentence labels.\n\n6. Future Work\n\nThere are multiple ways of building upon this research. First, it is important to inves-\ntigate how lateral annotation performs in additional real-world scenarios, and compare\nit to traditional methods of annotation. Second, finding ways to expand our framework\nby extending vectors with relevant properties or by combining vectors could increase\nthe system\u2019s performance. Third, augmenting the method to integrate active learning ap-\nproaches (where a machine learning model suggests which sample to label next to the\nannotator) could help to discover more relevant sentences. Furthermore, the approach of\nannotating sentences laterally could be used at an earlier stage, to support the exploration\nof data sets and the creation of type systems.\n\n7. Conclusions\n\nIn this paper, we have explored a method for the efficient annotation of sentences, by\nleveraging sophisticated sentence embedding models and approximate nearest neighbour\nsearches. Using these technologies, we designed a method and an interface that allow\nannotators to label similar sentences in one go across documents, rather than having to\nepisodically label similar sentences as they come up in new documents. We investigated\nsome properties of different possible embeddings and demonstrated the benefits of using\nthe method on three legal data sets.\n\n\n\nDecember 2020\n\nReferences\n\n[1] Bhattacharya, P., S. Paul, K. Ghosh, S. Ghosh, and A. Wyner. \u201cIdentification of Rhetorical Roles of\nSentences in Indian Legal Judgments.\u201d arXiv preprint arXiv:1911.05405 (2019).\n\n[2] Bojanowski, P., E. Grave, A. Joulin, and T. Mikolov. \u201cEnriching word vectors with subword informa-\ntion.\u201d Transactions of the Association for Computational Linguistics 5 (2017): 135-146.\n\n[3] Bowman, S., G. Angeli, C. Potts, and C. Manning. \u201cA large annotated corpus for learning natural lan-\nguage inference.\u201d arXiv preprint arXiv:1508.05326 (2015).\n\n[4] Branting, L. K., C. Pfeifer, B. Brown, L. Ferro, J. Aberdeen, B. Weiss, M. Pfaff, and B. Liao. \u201cScalable\nand explainable legal prediction.\u201d Artificial Intelligence and Law (2020): 1-26.\n\n[5] Branting, L.K., B. Weiss, B. Brown, C. Pfeifer, A. Chakraborty, L. Ferro, M. Pfaff, and A. Yeh. \u201cSemi-\nsupervised methods for explainable legal prediction.\u201d In Proc. ICAIL 2019, pp. 22-31. 2019.\n\n[6] Cer, D., Y. Yang, S. Kong, N. Hua, N. Limtiaco, R. St John, N. Constant et al. \u201cUniversal sentence\nencoder.\u201d arXiv preprint arXiv:1803.11175 (2018).\n\n[7] Conneau, A., D. Kiela, H. Schwenk, L. Barrault, and A. Bordes. \u201cSupervised learning of universal sen-\ntence representations from natural language inference data.\u201d arXiv preprint arXiv:1705.02364 (2017).\n\n[8] Cormack, G., and M. Grossman. \u201cScalability of continuous active learning for reliable high-recall text\nclassification.\u201d In Proc. 25th ACM Int\u2019l Conf. on Info. & Knowledge Management, pp. 1039-1048. 2016.\n\n[9] Cormack, G., and M. Grossman. \u201cAutonomy and reliability of continuous active learning for technology-\nassisted review.\u201d arXiv preprint arXiv:1504.06868 (2015).\n\n[10] Devlin, J., M. Chang, K. Lee, and K. Toutanova. \u201cBert: Pre-training of deep bidirectional transformers\nfor language understanding.\u201d arXiv preprint arXiv:1810.04805 (2018).\n\n[11] Firth, J. \u201cA synopsis of linguistic theory, 1930-1955.\u201d Studies in linguistic analysis (1957).\n[12] Harris, Z. \u201cDistributional structure.\u201d Word 10, no. 2-3 (1954): 146-162.\n[13] Hogan, C., R. Bauer, and D. Brassil. \u201cHuman-aided computer cognition for e-discovery.\u201d In Proc. 12th\n\nInt\u2019l Conf. on Artificial Intelligence and Law, pp. 194-201. 2009.\n[14] Indyk, P., and R. Motwani. \u201cApproximate nearest neighbors: towards removing the curse of dimension-\n\nality.\u201d In Proc. 30th Annual ACM Symposium on Theory of Computing, pp. 604-613. 1998.\n[15] Iyyer, M., V. Manjunatha, J. Boyd-Graber, and H. Daume\u0301 III. \u201cDeep unordered composition rivals syn-\n\ntactic methods for text classification.\u201d In Proc. 53rd ann. meet. ACL (Vol 1), pp. 1681-1691. 2015.\n[16] Johnson, J., M. Douze, and H.e\u0301 Je\u0301gou. \u201cBillion-scale similarity search with GPUs.\u201d IEEE Transactions\n\non Big Data (2019).\n[17] Joulin, A., E. Grave, P. Bojanowski, and T. Mikolov. \u201cBag of tricks for efficient text classification.\u201d arXiv\n\npreprint arXiv:1607.01759 (2016).\n[18] Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov.\n\n\u201cRoberta: A robustly optimized bert pretraining approach.\u201d arXiv preprint arXiv:1907.11692 (2019).\n[19] Reimers, N., and I. Gurevych. \u201cSentence-bert: Sentence embeddings using siamese bert-networks.\u201d\n\narXiv preprint arXiv:1908.10084 (2019).\n[20] S\u030cavelka, J., H. Xu, and K. Ashley. \u201cImproving Sentence Retrieval from Case Law for Statutory Inter-\n\npretation.\u201d Proc. 17th Int\u2019l Conf. on Artificial Intelligence and Law, pp. 113-122. 2019.\n[21] S\u030cavelka, J., G. Trivedi, and K. Ashley. \u201cApplying an interactive machine learning approach to statutory\n\nanalysis.\u201d In Proc. 28th Ann. Conf. on Legal Knowledge & Info. Systems (JURIX\u201915). IOS Press. 2015.\n[22] Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, \u0141. Kaiser, and I. Polosukhin.\n\n\u201cAttention is all you need.\u201d In Advances in neural information processing systems, pp. 5998-6008. 2017.\n[23] Walker, V., K. Pillaipakkamnatt, A. Davidson, M. Linares, and D. Pesce. \u201cAutomatic Classification of\n\nRhetorical Roles for Sentences: Comparing Rule-Based Scripts with Machine Learning.\u201d In ASAIL@\nICAIL. 2019.\n\n[24] Walker, V., J. Han, X. Ni, and K. Yoseda. \u201cSemantic types for computational legal reasoning: proposi-\ntional connectives and sentence roles in the veterans\u2019 claims dataset.\u201d Proc. ICAIL \u201917. ACM, 2017.\n\n[25] Waltl, B., J. Muhr, I. Glaser, G. Bonczek, E. Scepankova, and F. Matthes. \u201cClassifying Legal Norms\nwith Active Machine Learning.\u201d In JURIX, pp. 11-20. 2017.\n\n[26] Westermann, H., J. S\u030cavelka, V. Walker, K. Ashley, and K. Benyekhlef. \u201cComputer-Assisted Creation of\nBoolean Search Rules for Text Classification in the Legal Domain.\u201d In JURIX, pp. 123-132. 2019.\n\nhttp:\/\/arxiv.org\/abs\/1911.05405\n\n","2":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinual Learning with Knowledge Transfer for\nSentiment Classification\n\nZixuan Ke1, Bing Liu1, Hao Wang2, and Lei Shu1\n\n1 University of Illinois at Chicago, USA\n{zke4,liub}@uic.edu, shulindt@gmail.com\n\n2 Southwest Jiaotong University, Chengdu, China\ncshaowang@gmail.com\n\nAbstract. This paper studies continual learning (CL) for sentiment\nclassification (SC). In this setting, the CL system learns a sequence of\nSC tasks incrementally in a neural network, where each task builds a\nclassifier to classify the sentiment of reviews of a particular product cat-\negory or domain. Two natural questions are: Can the system transfer the\nknowledge learned in the past from the previous tasks to the new task\nto help it learn a better model for the new task? And, can old models\nfor previous tasks be improved in the process as well? This paper pro-\nposes a novel technique called KAN to achieve these objectives. KAN\ncan markedly improve the SC accuracy of both the new task and the old\ntasks via forward and backward knowledge transfer. The effectiveness of\nKAN is demonstrated through extensive experiments3.\n\n1 Introduction\n\nContinual learning (CL) aims to learn a sequence of tasks incrementally [4,18].\nOnce a task is learned, its training data is typically forgotten. The focus of\nthe existing CL research has been on solving the catastrophic forgetting (CF)\nproblem [4,18]. CF means that when a neural network learns a sequence of\ntasks, the learning of each new task is likely to change the network weights or\nparameters learned for previous tasks, which degrades the model accuracy for\nthe previous tasks [17]. There are two main CL settings in the existing research:\n\nClass continual learning (CCL): In CCL, each task consists of one or\nmore classes to be learned. Only one model is built for all classes seen so far. In\ntesting, a test instance from any class may be presented to the model for it to\nclassify without giving it any task information used in training.\n\nTask continual learning (TCL). In TCL, each task is a separate classifica-\ntion problem (e.g., one classifying different breeds of dogs and another classifying\ndifferent types of birds). TCL builds a set of classification models (one per task)\nin one neural network. In testing, the system knows which task each test instance\nbelongs to and uses only the model for the task to classify the test instance.\n\nIn this paper, we work in the TCL setting to continually learn a sequence of\nsentiment analysis (SA) tasks. Typically, a SA company has to work for many\n\n3 Code and data are available at: https:\/\/github.com\/ZixuanKe\/LifelongSentClass\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n10\n02\n\n1v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n8 \nD\n\nec\n 2\n\n02\n1\n\nhttps:\/\/github.com\/ZixuanKe\/LifelongSentClass\n\n\n2 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\nclients and each client wants to study public opinions about one or more cate-\ngories of its products\/services and those of its competitors. The sentiment anal-\nysis of each category of products\/services is a task. For confidentiality, a client\noften does not allow the SA company to share its data with or use its data for\nany other client. Continual learning is a natural fit. In this case, we also want\nto improve the SA accuracy over time without breaching confidentiality. This\npresents two key challenges: (1) how to transfer the knowledge learned from the\nprevious tasks to the new task to help it learn better without using the previous\ntasks\u2019 data, and (2) how to improve old models for the previous tasks in the\nprocess without CF? In [15], the authors showed that CL can help improve the\naccuracy of document-level sentiment classification (SC), which is a sub-problem\nof SA [13]. In this paper, we propose a significantly better model, called KAN\n(Knowledge Accessibility Network). Note that each task here is a two-class SC\nproblem, i.e., classifying whether a review for a product is positive or negative.\n\nA fair amount of work has been done on CL. However, existing techniques\nhave mainly focused on dealing with catastrophic forgetting (CF) [4,18]. In learn-\ning a new task, they typically try to make the weights update toward less harmful\ndirections to previous tasks, or to prevent the important weights for previous\ntasks from being significantly changed. We will detail these and other related\nwork in the next section. Dealing with only CF is far from sufficient for SC. In\nmost existing studies of CL, the tasks are quite different and have little shared\nknowledge. It thus makes sense to focus on dealing with CF. However, for SC,\nthe tasks are similar because words and phrases used to express sentiments for\ndifferent products\/tasks are similar. As we will see in Section 4.4, CF is not\na major problem in CL for SC due to the shared knowledge across tasks. Our\nmain goal is thus to leverage the shared knowledge among tasks to perform\nsignificantly better than learning individual tasks separately in isolation.\n\nTo achieve the goal of leveraging the shared knowledge among tasks to im-\nprove the SC accuracy, KAN uses two sub-networks, the main continual learning\n(MCL) network and the accessibility (AC) network. The core of MCL is a knowl-\nedge base (KB), which stores the knowledge learned from all trained tasks. In\nlearning each new task, the AC network decides which part of the past knowl-\nedge is useful to the new task and can be shared. This enables forward knowledge\ntransfer. Also importantly, the shared knowledge is enhanced during the new\ntask training using its data, which results in backward knowledge transfer. Thus,\nKAN not only improves the model accuracy of the new task but also improves\nthe accuracy of the previous tasks without any additional operations. Extensive\nexperiments show that KAN markedly outperforms state-of-the-art baselines.\n\n2 Related Work\n\nContinual learning (CL) has been researched fairly extensively in machine learn-\ning (see the surveys in [4,18]). Existing approaches have primarily focused on\ndealing with catastrophic forgetting (CF). Lifelong learning is also closely re-\nlated [27,22,3,4], which mainly aims to improve the new task learning through\n\n\n\nContinual Learning with Knowledge Transfer for Sentiment Classification 3\n\nforward knowledge transfer. We discuss them in turn and also their applications\nin sentiment classification (SC).\n\nContinual Learning. Several approaches have been proposed to deal with CF:\n\nRegularization-based methods, such as those in [9,12,23], add a regularization\nin the loss function to consolidate previous knowledge when learning a new task.\n\nParameter isolation-based methods, such as those in [24,16,6], make different\nsubsets of the model parameters dedicated to different tasks. They identify the\nparts of the network that are important to the previous tasks and mask them\nout during the training of the new task.\n\nGradient projection-based methods, such as that in [32], ensure the gradient\nupdates occur only in the orthogonal direction to the input of the previous tasks.\nThen, the weight updating for the new task have little effect on the weights for\nthe previous tasks.\n\nExemplar and replay-based methods, such as those in [20,14,2], retain an ex-\nemplar set that best approximates the previous tasks to help train the new task.\nThe methods in [25,8,21,7] instead took the approach of building data genera-\ntors for the previous tasks so that in learning the new task, they can use some\ngenerated data for previous tasks to help avoid forgetting.\n\nAs these methods are mainly for avoiding CF, after learning a sequence of\ntasks, their final models are typically worse than learning each task separately.\nThe proposed KAN not only deals with CF, but also perform forward and back-\nward transfer to improve the accuracy of both the past and the new tasks.\n\nTo our knowledge, SRK [15] is the only CL method for sentiment classification\n(SC). It consists of two networks, a feature learning network and a knowledge\nretention network, which are combined to perform CL. However, SRK only does\nforward transfer as it protects the past knowledge and thus cannot do backward\ntransfer as KAN does. More importantly, due to this protection, its forward\ntransfer also suffers because it cannot adapt the previous knowledge but only\nuse it without change. Its results are thus poorer than KAN. The SRK paper also\nshowed that CF is not a major issue for continual SC learning as the SC tasks\nare highly similar, which also explains why adaption of the previous knowledge\nin forward transfer in KAN does not cause CF.\n\nKAN is closely related to the TCL system HAT [24] as HAT also trains a\nbinary mask using hard attention. However, HAT\u2019s hard attention is for identi-\nfying what past knowledge in the network should be protected for each previous\ntask so that the new task learning will not modify this previous knowledge. This\nis effective for avoiding CF, not appropriate for our SC tasks due to the shared\nknowledge across tasks in SC. KAN trains an accessibility mask for the cur-\nrent\/new task to decide what previous knowledge can be accessed by or shared\nwith the current task to enable both forward and backward knowledge transfer.\nThere is no concept of knowledge transfer in HAT. In terms of architecture,\nKAN has two sub-networks: the accessibility (AC) network and the main con-\ntinual learning (MCL) network, while HAT has only one - it does not have the\nAC network. KAN\u2019s AC network trains the AC mask to determine what knowl-\nedge can be shared. The MCL network stores the knowledge and applies the\n\n\n\n4 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\ntrained AC mask to the knowledge base. This setting enables KAN not only to\nadapt the shared knowledge across tasks to produce more accurate models, but\nalso to avoid CF. HAT has only one network, and its mask is to block only the\nknowledge that is important to previous task models.\n\nLifelong Learning (LL) for SC. The authors of [5,28] proposed a Naive Bayes\n(NB) approach to help improve the new task learning. A heuristic NB method\nwas also proposed in [28]. [30] presented a LL approach based on voting of\nindividual task classifiers. All these works do not use neural networks, and are\nnot concerned with the CF problem. The work in [26,29] uses LL for aspect-based\nsentiment analysis, which is an entirely different problem than document-level\nsentiment classification (SC) studied in this paper.\n\n3 Proposed Model KAN\n\nTo improve the classification accuracy and also to avoid forgetting, we need to\nidentify some past knowledge that is shareable and update-able in learning the\nnew task so that no forgetting of the past knowledge will occur and both the\nnew task and the past tasks can improve.\n\nWe solve this problem by taking inspiration from what we humans seem to\ndo. For example, we may \u201cforget\u201d our phone number 10 years ago, but if the same\nnumber or a similar number shows up again, our brain may quickly retrieve the\nold phone number and make both the new and old numbers memorized more\nstrongly. Biological research [10] has shown that our brain keeps track of the\nknowledge accessibility. If some parts of our previous knowledge are useful to\nthe new task (i.e., shared knowledge between the new task and some previous\ntasks), our brain sets those parts accessible to enable forward knowledge transfer.\nThis also enables backward knowledge transfer as they are now accessible and we\nhave the opportunity to strengthen them based on the new task data. For those\nnot useful parts of the previous knowledge, they are set to inaccessible, which\nprotects them from being changed. Inspired by this idea, we design a memory\nand accessibility mechanism.\n\nWe need to solve two key problems: (1) how to detect the accessibility of the\nknowledge in the memory (which we call knowledge base (KB)), i.e., identifying\nthe part of the previous knowledge that is useful to the new task; (2) how to\nleverage the identified useful\/shared knowledge to help the new task learning\nwhile also protecting the other part. To address these challenges, we propose the\nKnowledge and Accessibility Network (KAN) shown in Figure 1.\n\n3.1 Overview of KAN\n\nKAN has two components (see Figure 1), the main continual learning (MCL)\ncomponent in the purple box and the accessibility (AC) component in the yellow\nbox. MCL performs the main continual learning and testing (AC is not used in\ntesting except the mask at generated from the task id t). We see the sentiment\nclassification heads (pos\/neg) for each task at the top. Below them are the dense\n\n\n\nContinual Learning with Knowledge Transfer for Sentiment Classification 5\n\nFig. 1: The KAN architecture (best viewed in color). The purple box contains\nthe main continual learning (MCL) component, and the yellow box contains the\naccessibility (AC) component. The green arrows represent forward paths shared\nby both components. The yellow and purple arrows represent the forward paths\nused only in AC and MCL respectively.\n\nlayers and further down is the knowledge base (KB) (the memory) in the green\ndash-lined box. KB, which is modeled using an RNN (we use GRU in our system)\nand is called KB-RNN, contains both the task-specific and shared knowledge\nacross tasks. AC decides which part of the knowledge (or units) in the KB is\naccessible by the current task t by setting a binary task-based mask at. Each\ntask is indicated by its task embedding produced by AC-EMB from the task id\n(t). AC-EMB is a randomly initialized embedding layer. The inputs to KAN are\nthe task id t and document d. They are used in training both components via\nthe mask at and {hKBi } (hidden states in KB-RNN) links.\n\nAC Training. In the AC training phase, only Task Embedding (AC-EMB), AC-\nRNN and others in the yellow box are trainable. Consider the KB has already\nretained the knowledge learned from tasks 0...t-1. When the new task t arrives,\nwe first train the AC component to produce a binary task-based mask at (a\nmatrix with the same size as the KB {hKBi }) to indicate which units in KB-\nRNN are accessible for the new\/current task t. Since the mask is trained based\non the new task data with the previous knowledge in KB-RNN fixed, those KB-\n\n\n\n6 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\nRNN units that are not masked (meaning that they are useful to the new task)\nare the accessible units with their entries in at as 1 (unmasked). The other units\nare inaccessible with their entries in at as 0 (masked).\n\nMCL Training. After AC training, we start MCL training. In this phase, only\nKB-RNN and others in the purple box are trainable. The trained binary task-\nbased mask at is element-wise multiplied by the output vectors of KB-RNN.\nThis operation protects those inaccessible units since no gradient flows across\nthem while allowing those accessible units to be updated since the mask does not\nstop gradients for them. This clearly enables forward knowledge transfer because\nthose accessible units selected by the mask represent the knowledge from the\nprevious tasks that can be leveraged by the current\/new task. It also enables\nbackward knowledge transfer and avoidance of CF because (1) if the accessible\nunits are not important to the previous tasks, any modification to them does\nnot degrade the previous tasks\u2019 performance, and (2) if the accessible units are\nuseful for some previous tasks, updating them enable them to improve as we\nnow have more data to enhance the shared knowledge.\n\nAlgorithm 1: Continual Learning in KAN\n\nInput: Dataset D = (D0, ..., DT )\nOutput: Parameters of KB-RNN WKB-RNN , of AC-RNN WAC-RNN ,\n\nand of AC-EMB WAC-EMB\n\n1 for t = 0, .., T do\n2 if t = 0 then\n3 WKB-RNN0 = MCLtraining(D0)\n\n4 WAC-RNN0 ,W\nAC-EMB\n0 = ACtraining(D0,W\n\nKB-RNN\n0 )\n\n5 else\n6 WAC-RNNt ,W\n\nAC-EMB\nt = ACtraining(Dt,W\n\nKB-RNN\nt-1 )\n\n7 WKB-RNNt = MCLtraining(Dt,W\nAC-EMB\nt )\n\nContinual Learning in KAN: The algorithm for continual learning in KAN\nis given in Algorithm 1. For each new task, AC training is done first and then\nMCL training. An exception is at the first task (lines 3-4 in Algorithm 1). At\nthe very beginning, KB has no knowledge, and thus nothing can be used by AC.\nTherefore, we train MCL (and KB) before AC to obtain some knowledge first.\nHowever, after the first task, AC is always trained before MCL (and KB) for\neach new task (lines 6-7).\n\n3.2 Details of Accessibility Training\n\nAC training aims to detect the accessibility of the knowledge retained in the KB\ngiven the new task data. As shown in Figure 1 and Algorithm 2 4, it takes as in-\nputs a training example d and the trained knowledge base KB-RNN WKB-RNNt\u22121 ,\n\n4 For simplicity, we only show the process for one training example, but our actual\nsystem trains in batches.\n\n\n\nContinual Learning with Knowledge Transfer for Sentiment Classification 7\n\nwhich has been trained because AC is always trained after KB (KB-RNN) was\ntrained on the previous task. To generate a binary matrix at so that it can work\nas a mask in both the MCL (and KB) and AC training phases, we borrow the\nidea of hard attention in [24,31,1] where the values in the attention matrix are\nbinary instead of a probability distribution as in soft attention.\n\nHard Attention Training. Since we can access the task id in both training\nand testing, a natural choice is to leverage the task embedding to compute the\nhard attention. Specifically, for a task id t, we first compute a task embedding\net by feeding the task id into the task embedding layer (AC-EMB) where a 1-\nhot id vector is multiplied by a randomly initialized parameter matrix. Using the\nresulting task embedding et, we apply a gate function \u03c3(x) \u2208 [0, 1] and a positive\nscaling parameter s to compute the binary attention at as shown in lines 1 and 2\nin Algorithm 2. Intuitively, it uses a unit step function as the activation function\n\u03c3(x). However, this is not ideal because it is non-differentiable. We want to train\nthe embedding et with back propagation. Motivated by [24], we use a sigmoid\nfunction with a positive scaling parameter s to construct a pseudo-step function\nallowing the gradient to flow. This scaling parameter is introduced to control\nthe polarization of our pseudo-step function and the output at. Our strategy is\nto anneal s during training, inducing a gradient flow and set s = smax during\ntesting. This is because using a hyperparameter smax \ufffd 1, we can make our\nsigmoid function approximate to a unit step function. Meanwhile, when s\u2192\u221e\nwe get at \u2192 {0, 1} and when s \u2192 0, we get at \u2192 0.5. We start the training\nepoch with all units being equally active by using the latter and progressively\npolarize them within the epoch. Specifically, we anneal s as follows:\n\n(1)s =\n1\n\nsmax\n+ (smax \u2212\n\n1\n\nsmax\n)\nb\u2212 1\nB \u2212 1\n\nwhere b = 1, ...B is the batch index and B is the total number of batches in an\nepoch.\n\nTo better understand the hard attention training, recall that the resulting\nmask at needs to be binary so that it can be used to block\/unblock some units\u2019\ntraining in both phases. The task id is used to control the mask to condition\nthe KB. To achieve this, we need to make sure the embedding of the task id is\ntrainable for which we adopt sigmoid as the pseudo gate function. The training\nprocedure is annealing: at first, s \u2192 0 (s = 1\n\nsmax\n) and the mask is still a soft\n\nattention. After certain batches, s becomes large and \u03c3(s \u2297 et) becomes very\nsimilar to a gate function. After training, those units with their entries in at as\n1 are accessible to task t while the others are inaccessible. Another advantage of\ntraining a hard attention is that we can easily retrieve the binary task-based at\nafter training: we simply adopt smax to be s and apply \u03c3(smax \u2297 et).\n\nApply Hard Attention to the Network. The fixed KB-RNN takes a training\nexample d as input and produces a sequence of output vectors {hKBi } which have\nincorporated the previous knowledge (line 3 in Algorithm 2). i denotes the ith\nstep of the RNN or the representation of the ith word of the input, ranging from\n\n\n\n8 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\n0 to the step size step. {hKBi } then performs element-wise multiplication with\nthe task-based mask at to get the accessibility representation of the previous\nknowledge {hKB;Accessi } (line 4). Recall that et is a task id embedding vector\nand et \u2208 Rdim, where dim refers to the dimension size, and therefore at \u2208\nRdim (at = \u03c3(s \u2297 et)). In other words, we expand the vector at (repeat the\nvector step times) to match the size of {hKBi }, and then perform element-wise\nmultiplication. This accessibility representation encodes the useful knowledge\nfrom the previous tasks. We first feed the representation into AC-RNN to learn\nsome additional new task knowledge (line 5). The last step of the resulting\nsequence of vectors {hACi }, which is h\n\nAC\nstep, then goes through a dense layer to\n\nreduce the vector\u2019s dimension and finally compute the loss based on cross entropy\n(line 6).\n\nAlgorithm 2: AC Training\n\nInput: A training example d, scaling parameter s, task id t, trained\nKB-RNN WKB-RNNt-1\n\nOutput: Parameters of AC-RNN WAC-RNNt and of AC-EMB\nWAC-EMBt\n\n1 et =AC-EMB(t) \/\/ AC-EMB is trainable.\n2 at = \u03c3(s\u2297 et) \/\/ We anneal s as shown in Eq.1\n3 {hKBi } =KB-RNN(d) \/\/ KB-RNN is already trained and is\n\nfixed.\n\n4 {hKB;Accessi } = {h\nKB\ni } \u2297 at\n\n5 {hACi } =AC-RNN({h\nKB;Access\ni }) \/\/ AC-RNN is trainable.\n\n6 LAC = CrossEntropy(dlabel, Dense(hACstep)) \/\/ Compute the AC loss.\n\n3.3 Details of Main Continual Learning Training\n\nMCL training learns the current task knowledge and protects the knowledge\nlearned in previous tasks in the KB (KB-RNN). As shown in Algorithm 3 and\nFigure 1, it takes an input training example d in the corresponding dataset\nDt and encodes d via KB-RNN (line 1), which results in a sequence of vectors\n{hKBi }. Following our training scheme, i.e., AC-RNN and AC-EMB are always\ntrained before KB for a new task t (except for the first one), we already have the\ntrained AC-EMB WAC-EMBt when discussing KB. We therefore can compute\nthe mask at from W\n\nAC-EMB\nt (lines 5-6). Note that we expand at to be a binary\n\nmatrix and at \u2208 Rstep\u00d7dim where step refers to step size of KB-RNN and dim\nrefers to the dimension size of the task embedding vector. For the first task (i.e.,\nt = 0), we simply assume a0 as a matrix of ones (line 3).\n\nBlock the Inaccessible and Unblock the Accessible Units. Naturally,\nwe want to \u201cremove\u201d those inaccessible units so that only accessible ones can\ncontribute to the training of KB-RNN. An efficient method is to simply element-\nwise multiply the outputs of KB {hKBi } by at (line 7). Since at is a binary mask,\n\n\n\nContinual Learning with Knowledge Transfer for Sentiment Classification 9\n\nonly those KB with mask 1 can be updated by backward propagation. This is\nequivalent to modifying the gradient g with the mask at:\n\n(2)g\u2032 = at \u2297 g\n\nThe resulting vectors can be seen as the representation of accessible knowl-\nedge {hKB;Accessi }. Finally, we take the last step of the accessible knowledge\nvectors {hKB;Accessi }, which is h\n\nKB;Access\nstep , to a fully connected layer to perform\n\nclassification (line 8). Note that we employ multi-head configuration (upper com-\nponents in the KB training phase in Figure 1) which means each task is allocated\nan exclusive dense layer. These dense layers are mapped to the dimension of the\nnumber of classes c (c=2) so that we can use different dense layer to perform\nclassification according to different task id.\n\nAlgorithm 3: MCL Training\n\nInput: A training example d, task id t, and trained task embedding\nWAC-EMBt\n\nOutput: Parameters of KB-RNN WKB-RNNt\n1 {hKBi } =KB-RNN(d) \/\/ KB-RNN is trainable.\n2 if t = 0 then\n3 Set all values of a0 to 1\n\n4 else\n5 et =AC-EMB(t) \/\/ AC-EMB is trainable.\n6 at = \u03c3(smax \u2297 et) \/\/ Use smax to retrieve the trained mask.\n7 {hKB;Accesi } = {h\n\nKB\ni } \u2297 at\n\n8 LKB = CrossEntropy(dlabel, Dense(h\nKB;Acces\nstep )) \/\/ Compute the MCL\n\nloss.\n\n3.4 Comparing Accessibility and Importance\n\nMany existing models discussed in Section 2 detect the importance of units. That\nis, they identify units or parameters that are important to previous tasks so that\nin learning the new task, the learner can protect them to avoid forgetting the\nprevious tasks\u2019 models. However, it is also stifling the chance for adapting and\nupdating the previous knowledge to help learn the new task in forward trans-\nfer and for improving previous tasks to achieve backward transfer. In contrast,\nour concept of accessibility is very different, which is for the current task. KAN\ndetects the accessibility of units and safely update the weights of the accessi-\nble units because of the shared knowledge. This enables adaptation in forward\ntransfer. If some units are accessible for the current task, KAN can update them\nbased on the current task training data. If those units are also accessible by some\nprevious tasks, it suggests that there is some shared knowledge between the cur-\nrent and the previous tasks. This results in backward transfer. One can also see\nthis as strengthening the shared knowledge since we now have more data for the\nshared knowledge training. In short, training the accessible units is helpful to\nboth the current and the previous tasks.\n\n\n\n10 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\n4 Experiments\n\nWe now evaluate KAN for continual document sentiment classification (SC). We\nfollow the standard continual learning evaluation procedure [11] as follows. We\nprovide a sequence of SC tasks with their training datasets for KAN to learn one\nby one. Each task learns to perform SC on reviews of a type of product. Once a\ntask is learned, its training data is discarded. After all tasks are learned, we test\nall task models using their respective test data. In training each task, we use its\nvalidation set to decide when to stop training.\n\n4.1 Experimental Data\n\nOur dataset consists of Amazon reviews from 24 different types of products,\nwhich make up the 24 tasks. Each task has 2500 positive (with 4 or 5 stars) and\n2500 negative (with 1 or 2 stars) reviews. We further split the reviews in each\ntask into training, testing and validation set in the ratio of 8:1:1. We didn\u2019t use\nthe datasets in [15] as they are all highly skewed with mostly positive examples.\nWithout doing anything, the system can achieve more than 80% of accuracy.\nWe also did not use the commonly employed sentiment analysis datasets from\nSemEval [19] because its reviews involve only two types of products\/services,\ni.e., laptop and restaurant, which are too few for continual learning.\n\n4.2 Baselines\n\nWe consider a wide range of baselines: (1) isolated learning of each task; (2)\nstate-of-the-art continual learning methods; (3) existing continual or lifelong\nsentiment classification models; and (4) a naive continual learning model.\n\nOne task learning (ONE) builds an isolated model for each task individu-\nally, independent of other tasks. There is no knowledge transfer. The network is\nthe same as KAN but without AC and accessibility mask, consisting of word em-\nbeddings, a conventional GRU layer and a fully connected layer for classification.\nThe same network is used in the other variant of KAN, i.e., N-CL.\n\nElastic Weight Consolidation (EWC) [9] is a popular regularization-\nbased continual learning method, which slows down learning for weights that\nare important to the previous tasks.\n\nHard Attention to Task (HAT) [24] learns pathways in a given base\nnetwork using the given task id (and thus task incremental). The pathways are\nthen used to obtain the task-specific networks. It is the state-of-the-art task\ncontinual learning (TCL) model for avoiding catastrophic forgetting (CF).\n\nOrthogonal Weights Modification (OWM) [32] is a state-of-the-art\nclass continual learning (CCL) method. Since OWM is a CCL method but we\nneed a TCL method, we adapt it for TCL. Specifically, we only train on the\ncorresponding head of the specific task id during training and only consider the\ncorresponding head\u2019s prediction during testing.\n\n\n\nContinual Learning with Knowledge Transfer for Sentiment Classification 11\n\nSentiment Classification by Retained Knowledge (SRK) [15] is the\nonly prior work on continual sentiment classification. It achieves only limited\nforward transfer as we discussed in Section 2.\n\nLifelong Learning for Sentiment Classification (LSC), proposed by\n[5], is a Naive Bayes based lifelong learning approach to SC. It does forward\ntransfer but not continual learning and thus has no CF issue. The main goal of\nthis traditional method is to improve only the performance of the new task.\n\nNaive continual learning (N-CL) greedily trains a sequence of SC tasks\nincrementally without dealing with CF. Note that this is equivalent to KAN\nafter removing AC and mask, i.e., it uses the same network as ONE.\n\n4.3 Network and Training\n\nUnless stated otherwise, we employ the embedding with 300 dimensions to rep-\nresent the input text. GRU\u2019s with 300 dimensions are used for both AC-RNN\nand KB-RNN. We adopt Glove 300d 5 as pre-trained word embeddings and fix\nthem during training of KAN. The fully connected layer with softmax output\nis used as the final layer(s), together with categorical cross-entropy loss. During\nKAN training, we initialize the hidden state for each element in the batch to\n0. We set smax to 140 in the s annealing algorithm, dropout to 0.5 between\nembedding and GRU layers for both MCL and AC training phases. We train\nall models with Adam using the learning rate of 0.001. We stop training when\nthere is no improvement in the validation accuracy for 5 consecutive epochs (i.e.,\nearly stopping with patience=5). The batch size is set to 64. During testing, we\nevaluate the final performance using MCL only. AC is not involved in testing\n(except the mask at generated with the task id t during training). For the base-\nlines SRK, HAT, OWM, and EWC, we use the code provided by their authors\n(customized for text if needed) and adopt their original parameters.\n\n4.4 Results\n\nAverage Results. We first report the average results of all compared models\nto show the effectiveness of the proposed KAN. Since the order of the tasks\nmay have an impact on the final results of CL, we randomly choose and run 10\nsequences and average their results. We also ensured that the last tasks in the 10\nsequences are different. Table 1 gives the average accuracy of all systems. Column\n2 gives the average result over 24 tasks for each model after all tasks are learned.\nColumn 3 gives the accuracy result of the last task for each model. Note here\nthat the Last Task results and the All Tasks results are not comparable because\neach Last Task result is the average of the 10 last tasks in the 10 random task\nsequences, while each ALL Tasks result is the average of all 24 tasks. Column\n4 gives the p-value of the significance test to show that KAN outperforms all\nbaselines (more discussion later). Column 5 gives the number of parameters of\n\n5 https:\/\/github.com\/stanfordnlp\/GloVe\n\n\n\n12 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\nModels All Tasks Last Tasks P-value #Paramters\n\nONE 0.7846 0.7809 1.025e-7 25.2M\nLSC 0.8219 0.8246 1.581e-2 \u2014\n\nN-CL 0.8339 0.8477 1.792e-3 25.2M\nEWC 0.6899 0.7187 1.542e-9 42.5M\nOWM 0.6983 0.7337 3.219e-15 30.0M\nHAT 0.6456 0.6938 1.861e-14 42.7M\nSRK 0.8282 0.8500 7.793e-6 3.4M\nKAN 0.8524 0.8799 \u2014 42.4M\n\nTable 1: Average accuracy of different models. #Parameters refers to the number\nof parameters. LSC is based on Naive Bayes whose number of parameters is the\nsum of the number of unique words (vocabulary) in each dataset multiplied by\nthe number of classes, which is 2 in our case. Paired t-test is used to test the\nsignificance of the result of KAN against that of each baseline for ALL Tasks.\nP-values are given in column 4.\n\neach model. Note that ONE and LSC are not continual learning (CL) systems\nand have no CF issue. The rest are continual learning systems.\n\nWe first observe that KAN\u2019s result is markedly better than that of every\nbaseline. Since the traditional lifelong learning method LSC mainly aims to\nimprove the last task\u2019s performance, for the All Tasks column, we give its best\nresult, i.e., putting each of the 24 tasks as the last task. Even under this favorable\nsetting, its result is considerably poorer than that of KAN.\n\nIt is interesting to know that comparing to ONE, naive CL (N-CL) does not\nshow accuracy degradation on average even without a mechanism to deal with\nCF (forgetting). N-CL is actually significantly better than ONE (they use exactly\nthe same network). As mentioned earlier, this is because sentiment analysis tasks\nare similar to each other and can mostly help one another. That is, CF is not a\nmajor issue in CL for sentiment analysis. In fact, N-CL also outperforms SRK\non ALL Tasks. This can be explained by the fact that SRK does not adapt the\npast knowledge or allow backward transfer as discussed in Section 2. SRK\u2019s main\ngoal was to improve the last task\u2019s performance rather than those of all tasks.\n\nRegarding the continual learning (CL) baselines, EWC, OWM and HAT, they\nperform poorly and are even worse than ONE, which is not surprising because\ntheir networks are primarily designed to preserve knowledge learned for each of\nthe previous tasks. This makes it hard for them to exploit knowledge sharing to\nimprove all tasks.\n\nSignificance Test. To show that our results from KAN are significantly\nbetter than those of baselines, we conduct a paired t-test. We test KAN against\neach of the baselines based on the results of All Tasks from the 10 random\nsequences. All p-values are far below 0.05 (see Table 1), which indicates that\nKAN is significantly better than every baseline.\n\nTable 1 also includes the number of parameters of each neural model (Column\n4). A large fraction of the parameters for KAN is due to the mask for each task.\n\n\n\nContinual Learning with Knowledge Transfer for Sentiment Classification 13\n\nTask (product category) SRK HAT OWM EWC N-CL ONE KAN\n\nAmazon Instant Video 0.7776 0.6297 0.6792 0.6589 0.7989 0.7859 0.8293\nApps for Android 0.8236 0.6351 0.7044 0.6634 0.8431 0.8101 0.8531\n\nAutomotive 0.8049 0.6728 0.6849 0.6874 0.8086 0.6917 0.8335\n\nBaby 0.8617 0.6947 0.7175 0.6997 0.8703 0.8020 0.8870\nBeauty 0.8758 0.6684 0.7333 0.6816 0.8752 0.8081 0.8912\nBooks 0.8517 0.6260 0.7078 0.7112 0.8165 0.8101 0.8337\n\nCDs and Vinyl 0.7740 0.5666 0.6487 0.6412 0.7937 0.7152 0.7970\nCell Phones and Accessories 0.8277 0.6363 0.7163 0.6844 0.8489 0.7899 0.8679\nClothing Shoes and Jewelry 0.8520 0.6678 0.7221 0.7124 0.8701 0.8727 0.8814\n\nDigital Music 0.7480 0.5656 0.6335 0.6159 0.7717 0.7232 0.7706\nElectronics 0.8457 0.6472 0.6895 0.6546 0.8242 0.7636 0.8359\n\nGrocery and Gourmet Food 0.8800 0.6664 0.7203 0.7354 0.8686 0.8242 0.8828\n\nHealth and Personal Care 0.8076 0.6095 0.6642 0.6605 0.8235 0.7131 0.8335\nHome and Kitchen 0.8577 0.6920 0.7289 0.7407 0.8595 0.8081 0.8812\n\nKindle Store 0.8500 0.6427 0.7060 0.6966 0.8324 0.8505 0.8584\n\nMovies and TV 0.8377 0.6317 0.7053 0.6986 0.8278 0.7879 0.8527\nMusical Instruments 0.8142 0.7595 0.7456 0.7570 0.8677 0.8351 0.8851\n\nOffice Products 0.8180 0.6195 0.6664 0.6592 0.8142 0.7374 0.8346\n\nPatio Lawn and Garden 0.8130 0.6406 0.6543 0.7033 0.8308 0.7833 0.8363\nPet Supplies 0.7936 0.6289 0.6840 0.6720 0.8255 0.7556 0.8481\n\nSports and Outdoors 0.8420 0.6631 0.7045 0.6953 0.8384 0.7939 0.8696\n\nTools and Home Improvement 0.8300 0.6530 0.6904 0.6832 0.8408 0.7515 0.8640\nToys and Games 0.8500 0.6700 0.7368 0.7379 0.8644 0.8202 0.8744\n\nVideo Games 0.8397 0.6502 0.7147 0.7067 0.8381 0.7980 0.8557\n\nTable 2: Individual task accuracy of each model, after having trained on all tasks.\nThe number in bold in each row is the best accuracy of the row.\n\nNote that the similar numbers of parameters of models by no means indicate\nthe models are similar (see Section 2).\n\nAblation Study. KAN has two components, AC and MCL. To evaluate the\neffectiveness of AC, we can remove AC to test KAN. However, the binary mask\nin KAN needs AC to train. Without AC, it will have no mask and KAN is the\nsame as N-CL. KAN is significantly better than N-CL as shown in Table 1. MCL\nis the main module that performs continual learning and cannot be removed.\n\nIndividual Task Results. To gain more insights, we report the individual task\nresults in Table 2 for all continual learning baselines and KAN. We also include\nONE for comparison. Note that each task result for a model is the average of\nthe results from the 10 random sequences (except ONE).\n\nTable 2 shows that KAN gives the best accuracy for 21 out of 24 tasks.\nIn those tasks where KAN does not give the best, KAN\u2019s performances are\n\n\n\n14 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\nTasks ONE\nN-CL KAN\n\nForward Backward Forward Backward\n\nFirst 6 tasks 0.7846 0.7937 0.7990 0.8068 0.8132\nFirst 12 tasks 0.7865 0.8135 0.8199 0.8314 0.8390\nFirst 18 tasks 0.7870 0.8253 0.8327 0.8424 0.8501\nFirst 24 tasks 0.7846 0.8302 0.8339 0.8471 0.8524\n\nTable 3: Effects of forward and backward knowledge transfer of KAN. We give\nprogressive results after 6, 12, 18, and 24 tasks have been learned respectively.\n\ncompetitive. Hence, we can conclude that KAN is both highly accurate and\nrobust.\n\nRegarding SRK, it performs the best in only 2 tasks. However, these two\ntasks\u2019 results are only slightly better than those of KAN. These clearly indicate\nSRK is weaker than KAN. For the other continual learning baselines: HAT,\nOWM and EWC, their performances are consistently worse even than ONE.\nThis is expected because their goal is to protect each of the ONE\u2019s results and\nsuch protections are not perfect and thus can still result in some CF (forgetting).\n\nEffectiveness of Forward and Backward Knowledge Transfer. From\nTables 1 and 2, we can already see that KAN is able to exploit shared knowledge\nto improve learning of similar tasks. Here, we want to show whether the forward\nknowledge transfer and the backward knowledge transfer are indeed effective.\n\nTable 3 shows the accuracy results progressively after every 6 tasks have been\nlearned. In the second row, we give the results after 6 tasks have been learned\nsequentially. Each accuracy result in the Forward column is the overall average\nof the 6 tasks when each of them was first learned in each of the 10 random\nruns, which indicate the forward transfer effectiveness because from the second\ntask, the system can already start to leverage the previous knowledge to help\nlearn the new task. By comparing with the corresponding results of ONE and\nN-CL, we can see forward transfer of KAN is indeed effective. N-CL also has\nthe positive forward transfer effect, but KAN does better. Each result in the\nBackward column shows the average test accuracy after all 6 tasks have been\nlearned (over 10 random runs). By comparing with the corresponding result in\nthe Forward column, we can see that backward transfer of KAN is also effective,\nwhich means that learning of later tasks can help improve the earlier tasks\nautomatically. The same is also true for N-CL, although KAN does better. Rows\n3, 4, and 5 show the corresponding results after 12, 18, and 24 tasks have been\nlearned, respectively.\n\nWe also observe that forward transfer is much more effective than the back-\nward transfer. This is expected because forward transfer leverages the previous\nknowledge first and backward transfer can improve only after the forward trans-\nfer has made significant improvements. Furthermore, backward transfer also has\nthe risk of causing some forgetting for the previous tasks because the previous\ntask data are no longer available to prevent it.\n\n\n\nContinual Learning with Knowledge Transfer for Sentiment Classification 15\n\n5 Conclusion and Future Work\n\nThis paper proposed KAN, a novel neural network for continual learning (CL) of\na sequence of sentiment classification (SC) tasks. Previous CL models primarily\nfocused on dealing with catastrophic forgetting (CF). As we have seen in the ex-\nperiment section, CF is not a major issue for continual sentiment classification\nbecause the SC tasks are similar to each other and have a significant amount of\nshared knowledge among them. KAN thus focuses on improving the learning ac-\ncuracy by exploiting the shared knowledge via forward and backward knowledge\ntransfer. KAN achieves these goals using a knowledge base and a knowledge ac-\ncessibility network. The effectiveness of KAN was demonstrated by empirically\ncomparing it with state-of-the-art CL approaches. KAN\u2019s bi-directional knowl-\nedge transfer for CL significantly improves its results for SC. Our future work\nwill improve its accuracy and adapt it for other types of data.\n\nAcknowledgments\n\nThis work was supported in part by two grants from National Science Founda-\ntion: IIS-1910424 and IIS-1838770, and a research gift from Northrop Grumman.\n\nReferences\n\n1. Aharoni, R., Goldberg, Y.: Morphological inflection generation with hard mono-\ntonic attention. In: ACL. pp. 2004\u20132015. Association for Computational Linguis-\ntics, Vancouver, Canada (2017)\n\n2. Chaudhry, A., Ranzato, M., Rohrbach, M., Elhoseiny, M.: Efficient lifelong learning\nwith A-GEM. In: ICLR (2019)\n\n3. Chen, Z., Liu, B.: Topic modeling using topics from many domains, lifelong learning\nand big data. In: ICML. pp. 507\u2013515 (2014)\n\n4. Chen, Z., Liu, B.: Lifelong machine learning. Synthesis Lectures on Artificial In-\ntelligence and Machine Learning 12(3), 1\u2013207 (2018)\n\n5. Chen, Z., Ma, N., Liu, B.: Lifelong learning for sentiment classification. In: ACL.\npp. 750\u2013756 (2015)\n\n6. Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A.A., Pritzel,\nA., Wierstra, D.: Pathnet: Evolution channels gradient descent in super neural\nnetworks. CoRR abs\/1701.08734 (2017)\n\n7. He, X., Jaeger, H.: Overcoming catastrophic interference using conceptor-aided\nbackpropagation. In: ICLR (2018)\n\n8. Kamra, N., Gupta, U., Liu, Y.: Deep generative dual memory network for continual\nlearning. CoRR abs\/1710.10368 (2017)\n\n9. Kirkpatrick, J., Pascanu, R., Rabinowitz, N.C., Veness, J., Desjardins, G., Rusu,\nA.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D.,\nClopath, C., Kumaran, D., Hadsell, R.: Overcoming catastrophic forgetting in neu-\nral networks. CoRR abs\/1612.00796 (2016)\n\n10. Kornell, N., Hays, M.J., Bjork, R.A.: Unsuccessful retrieval attempts enhance sub-\nsequent learning. Journal of Experimental Psychology: Learning, Memory, and\nCognition 35(4), 989 (2009)\n\n\n\n16 Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu\n\n11. Lange, M.D., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh,\nG.G., Tuytelaars, T.: Continual learning: A comparative study on how to defy\nforgetting in classification tasks. CoRR abs\/1909.08383 (2019)\n\n12. Lee, S., Kim, J., Jun, J., Ha, J., Zhang, B.: Overcoming catastrophic forgetting by\nincremental moment matching. In: NeurIPS. pp. 4652\u20134662 (2017)\n\n13. Liu, B.: Sentiment analysis: Mining opinions, sentiments, and emotions. Cambridge\nUniversity Press (2015)\n\n14. Lopez-Paz, D., Ranzato, M.: Gradient episodic memory for continual learning. In:\nNeurIPS. pp. 6467\u20136476 (2017)\n\n15. Lv, G., Wang, S., Liu, B., Chen, E., Zhang, K.: Sentiment classification by leverag-\ning the shared knowledge from a sequence of domains. In: DASFAA. pp. 795\u2013811\n(2019)\n\n16. Mallya, A., Lazebnik, S.: Packnet: Adding multiple tasks to a single network by\niterative pruning. In: CVPR. pp. 7765\u20137773 (2018)\n\n17. McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks:\nThe sequential learning problem. In: Psychology of learning and motivation, vol. 24,\npp. 109\u2013165. Elsevier (1989)\n\n18. Parisi, G.I., Kemker, R., Part, J.L., Kanan, C., Wermter, S.: Continual lifelong\nlearning with neural networks: A review. Neural Networks 113, 54\u201371 (2019)\n\n19. Pontiki, M., Galanis, D., Papageorgiou, H., Androutsopoulos, I., Manandhar, S.,\nAl-Smadi, M., Al-Ayyoub, M., Zhao, Y., Qin, B., De Clercq, O., et al.: Semeval-\n2016 task 5: Aspect based sentiment analysis. In: SemEval (2016)\n\n20. Rebuffi, S., Kolesnikov, A., Sperl, G., Lampert, C.H.: icarl: Incremental classifier\nand representation learning. In: CVPR. pp. 5533\u20135542 (2017)\n\n21. Rostami, M., Kolouri, S., Pilly, P.K.: Complementary learning for overcoming\ncatastrophic forgetting using experience replay. In: IJCAI. pp. 3339\u20133345 (2019)\n\n22. Ruvolo, P., Eaton, E.: ELLA: an efficient lifelong learning algorithm. In: ICML.\npp. 507\u2013515 (2013)\n\n23. Seff, A., Beatson, A., Suo, D., Liu, H.: Continual learning in generative adversarial\nnets. CoRR abs\/1705.08395 (2017)\n\n24. Serra\u0300, J., Suris, D., Miron, M., Karatzoglou, A.: Overcoming catastrophic forget-\nting with hard attention to the task. In: ICML. pp. 4555\u20134564 (2018)\n\n25. Shin, H., Lee, J.K., Kim, J., Kim, J.: Continual learning with deep generative\nreplay. In: NeurIPS. pp. 2990\u20132999 (2017)\n\n26. Shu, L., Xu, H., Liu, B.: Lifelong learning CRF for supervised aspect extraction.\nIn: ACL. pp. 148\u2013154 (2017)\n\n27. Silver, D.L., Yang, Q., Li, L.: Lifelong machine learning systems: Beyond learning\nalgorithms. In: AAAI (2013)\n\n28. Wang, H., Liu, B., Wang, S., Ma, N., Yang, Y.: Forward and backward knowledge\ntransfer for sentiment classification. In: ACML. pp. 457\u2013472 (2019)\n\n29. Wang, S., Lv, G., Mazumder, S., Fei, G., Liu, B.: Lifelong learning memory net-\nworks for aspect sentiment classification. In: IEEE International Conference on Big\nData, Big Data. pp. 861\u2013870 (2018)\n\n30. Xia, R., Jiang, J., He, H.: Distantly supervised lifelong learning for large-scale\nsocial media sentiment analysis. IEEE Trans. Affective Computing 8(4), 480\u2013491\n(2017)\n\n31. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel,\nR.S., Bengio, Y.: Show, attend and tell: Neural image caption generation with\nvisual attention. In: ICML. pp. 2048\u20132057 (2015)\n\n32. Zeng, G., Chen, Y., Cui, B., Yu, S.: Continuous learning of context-dependent\nprocessing in neural networks. Nature Machine Intelligence (2019)\n\n\n\tContinual Learning with Knowledge Transfer for Sentiment Classification\n\n","3":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge Dataset of Cognates and False Friend Pairs from Indian Languages\n\nDiptesh Kanojia\u2020,\u2663,?, Pushpak Bhattacharyya\u2020, Malhar Kulkarni\u2020, and Gholamreza Haffari?\n\u2020Indian Institute of Technology Bombay, India\n\n\u2663IITB-Monash Research Academy, India\n?Monash University, Australia\n\n\u2020{diptesh, pb, malhar}@iitb.ac.in, ?gholamreza.haffari@monash.edu\n\nAbstract\nCognates are present in multiple variants of the same text across different languages (e.g., \u201chund\u201d in German and \u201chound\u201d in English\nlanguage mean \u201cdog\u201d). They pose a challenge to various Natural Language Processing (NLP) applications such as Machine Translation,\nCross-lingual Sense Disambiguation, Computational Phylogenetics, and Information Retrieval. A possible solution to address this\nchallenge is to identify cognates across language pairs. In this paper, we describe the creation of two cognate datasets for twelve Indian\nlanguages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. We\ndigitize the cognate data from an Indian language cognate dictionary and utilize linked Indian language Wordnets to generate cognate\nsets. Additionally, we use the Wordnet data to create a False Friends\u2019 dataset for eleven language pairs. We also evaluate the efficacy\nof our dataset using previously available baseline cognate detection approaches. We also perform a manual evaluation with the help of\nlexicographers and release the curated gold-standard dataset with this paper.\n\nKeywords: cognate sets, Indian languages, cognate dataset, true cognates, false friends, gold data\n\n1. Introduction and Motivation\nCognates are words that have a common etymological\norigin. For e.g., the French and English word pair, Liberte\u0301 -\nLiberty, reveals itself to be a cognate through orthographic\nsimilarity. Automatic Cognate Detection (ACD) is the task\nof detecting such etymologically related words or word\nsets among different languages1. They share a formal\nand\/or semantic affinity. Cognate words can facilitate the\nSecond Language Acquisition (SLA) process, particularly\nbetween related languages. They have similar meanings\nand, therefore, can support the acquisition and\/or learning\nof a non-native language. However, although they can\naccelerate vocabulary acquisition, learners also have to be\naware of false friends and partial cognates. False friends\nare similar words that have distinct, unrelated meanings.\nFor example, \u201cgift\u201d in German means \u201cpoison\u201d unlike\nits English meaning. We illustrate the occurrence of one\nsuch example each for cognates and false friends\u2019 for a\npair of Indian languages in Table 1. In other cases, there\nare partial cognates i.e., similar words that have a common\nmeaning only in some contexts. For example, the word\n\u201cpolice\u201d in French can translate to \u201cpolice\u201d, \u201cpolicy\u201d\nor \u201cfont\u201d, depending on the context. Dictionaries often\ninclude information about cognates and false friends, and\nthere are dictionaries (Hammer and Monod, 1976; Prado,\n1993) exclusively devoted to them.\n\nIn this paper, we describe the creation of three cognate\ndatasets. First, we describe the digitization of one such\nCognate dictionary named, \u201cTatsama Shabda Kosha\u201d and\nits annotation with linked Wordnet IDs. With the help of\na lexicographer, we perform the digitization of this dictio-\nnary. Further, we annotate the cognate sets from the dic-\n\n1Cognates can also exist in the same language. Such word\npairs\/sets are commonly referred to as doublets.\n\nHindi (Hi) Marathi (Mr) Hindi Meaning Marathi Meaning\n\nCognate ank ank Number Number\nFalse Friend shikshA shikshA Education Punishment\n\nTable 1: An example each of a cognate pair and a false\nfriend pair from the closely related Indian languages Hindi\n(Hi) and Marathi (Mr)\n\ntionary with Wordnet synset IDs based on manual valida-\ntion, where the lexicographer checks each Wordnet in the\nexisting linked sense.Based on Kanojia et al. (2019b)\u2019s ap-\nproach, we use linked Indian Wordnets to generate true cog-\nnate data and create another cognate dataset. Additionally,\nwe use the same Wordnet data to produce a list of False\nFriends and release2 all the three datasets publicly. Our\ncognate sets can be utilized for lookup in phrase tables pro-\nduced during Machine Translation to assess the quality of\nthe translation system in question. They can be utilized as\ncandidate translations for words, and our false friends\u2019 list\ncan be utilized by language learners to avoid pitfalls dur-\ning the acquisition of a second language. False Friend and\nCognate detection techniques can use these lists to train au-\ntomatic cognate detection approaches for Indian languages.\nAutomatic Cognate Detection (ACD) techniques help phy-\nlogenetic inference by helping isolate diachronic sound\nchanges and thus detecting the words of a common ori-\ngin (Rama, 2014). The Indo-Aryan and Dravidian lan-\nguage families prevalent in South Asia are examples of lan-\nguage families with a few ancestors (Sanskrit\/Persian and\nProto-Dravidian, respectively). Indian language pairs bor-\nrow a large number of cognates and false friends due to this\nshared ancestry. Knowing and utilising these cognates\/false\nfriends can help improve the performance of computational\nphylogenetics (Rama et al., 2018) as well as cross-lingual\n\n2Released Data: Github Link\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n09\n52\n\n6v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n7 \nD\n\nec\n 2\n\n02\n1\n\nhttps:\/\/github.com\/dipteshkanojia\/challengeCognateFF\n\n\nFigure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word\nA and Word Z) explained for creating our Datasets (D2 and D3).\n\ninformation retrieval (Meng et al., 2001) in the Indian set-\nting, thus encouraging us to investigate this problem for\nthis linguistic area3. Some other applications of cognate\ndetection in NLP have been sentence alignment (Simard et\nal., 1993; Melamed, 1999), inducing translation lexicons\n(Mann and Yarowsky, 2001; Tufis, 2002), improving statis-\ntical machine translation models (Al-Onaizan et al., 1999),\nand identification of confusable drug names (Kondrak and\nDorr, 2004). All these applications depend on an effec-\ntive method of identifying cognates by computing a numer-\nical score that reflects the likelihood that the two words are\ncognates. Our work provides cognate sets for Indian lan-\nguages, which can help the automated cognate detection\nmethodologies and can also be used as possible translation\ncandidates for applications such as MT.\n\n2. Related Work\nWu and Yarowsky (2018) release cognate sets for Romance\nlanguage family and provide a methodology to complete\nthe cognate chain for related languages. Our work re-\nleases similar data for Indian languages. Such a cognate set\ndata has not been released previously for Indian languages,\nto the best of our knowledge. Additionally, we release\nlists of false friends\u2019 for language pairs. These cognates\ncan be used to challenge the previously established cog-\nnate detection approaches further. Kanojia et al. (2019a)\nperform cognate detection for some Indian languages, but\na prominent part of their work includes manual verifica-\ntion and segratation of their output into cognates and non-\ncognates. Identification of cognates for improving IR has\nalready been explored for Indian languages (Makin et al.,\n2007). Orthographic\/String similarity-based methods are\noften used as baseline methods for cognate detection, and\nthe most commonly used method amongst them is the Edit\ndistance-based similarity measure (Melamed, 1999).\nResearch in automatic cognate detection using various as-\npects involves computation of similarity by decomposing\n\n3The term linguistic area or Sprachbund (Emeneau, 1956)\nrefers to a group of languages that have become similar in some\nway as a result of proximity and language contact, even if they be-\nlong to different families. The best-known example is the Indian\n(or South Asian) linguistic area.\n\nphonetically transcribed words (Kondrak, 2000), acoustic\nmodels (Mielke et al., 2012), clustering based on seman-\ntic equivalence (Hauer and Kondrak, 2011), and aligned\nsegments of transcribed phonemes (List, 2012). Rama\n(2016) employs a Siamese convolutional neural network to\nlearn the phonetic features jointly with language related-\nness for cognate identification, which was achieved through\nphoneme encodings. Ja\u0308ger et al. (2017) use SVM for pho-\nnetic alignment and perform cognate detection for various\nlanguage families. Various works on orthographic cog-\nnate detection usually take alignment of substrings within\nclassifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu\nand Dinu, 2015) or HMM (Bhargava and Kondrak, 2009).\nCiobanu and Dinu (2014) employ dynamic programming\nbased methods for sequence alignment. Among cognate\nsets, common overlap set measures like set intersection,\nJaccard (Ja\u0308rvelin et al., 2007) or XDice (Brew et al., 1996)\ncould be used to measure similarities and validate the mem-\nbers of the set.\n\n3. Dataset Creation\nWe create three different datasets to help the NLP tasks of\ncognate and false friends\u2019 detection. In this section, we de-\nscribe the creation of these three datasets for twelve Indian\nlanguages, namely Sanskrit, Hindi, Assamese, Oriya, Kan-\nnada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi,\nand Malayalam.\n\n3.1. D1 - True Cognate Sets\nThe first dataset is created with the help of manual anno-\ntation. We digitize the book \u201cTatsama Shabda Kosh\u201d with\nthe help of a lexicographer. The dictionary is a collection of\ncognates from 15 Indian languages, but focus our work on\n12 languages due to the unavailability of Wordnets for the\nrest of the languages. The lexicographer then also annotates\neach cognate set with a Wordnet sense ID after manual val-\nidation of each cognate in the twelve linked Wordnets. This\nhelps us capture an appropriate sense for the cognate word\nprovided via the dictionary. The annotation was performed\nmanually with the data collected in a CSV format in a text\neditor. By definition, cognates are supposed to spell and\nmean the same. Our manual annotation using the Wordnet\n\n\n\nLanguage Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or\nPotential Candidates 50959 81834 47718 25044 33921 18084 5203 16230 14240 12480 54014\n\nCognates (D2) 15312 17021 15726 14097 21710 9235 3363 936 3478 4103 11894\nPercent Agreement 0.9877 0.9849 0.9838 0.9754 0.9617 0.9223 0.9033 0.9553 0.9167 0.9122 0.8833\n\nCohen\u2019s kappa 0.7851 0.7972 0.8628 0.7622 0.7351 0.7046 0.6436 0.7952 0.7591 0.7953 0.8333\n\nTable 2: Number of Potential Cognates, Number of cognates retained on both annotators\u2019 agreement [Cognates (D2)],\nPercent agreement among the annotators and Cohen\u2019s kappa score for each language pair in our dataset\n\nLanguage Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or\nPotential Candidates 11128 10378 14430 9062 9285 5192 1018 7149 9374 3384 5011\n\nFalse Friends (D3) 4380 6204 5826 4489 2193 1076 783 699 3872 926 2602\nPercent Agreement 0.8912 0.9122 0.9233 0.9500 0.9018 0.8125 0.9288 0.8492 0.8825 0.9367 0.9133\n\nCohen\u2019s kappa 0.8827 0.8245 0.7815 0.9255 0.9452 0.9064 0.7244 0.8901 0.8432 0.8167 0.9548\n\nTable 3: Number of Potential False Friends, Number of False Friend pairs retained on both annotators\u2019 agreement [False\nFriends (D3)], Percent agreement among the annotators and Cohen\u2019s kappa score for each language pair in our dataset\n\nNouns Verbs Adjectives Adverbs\n\nD1 78.20 0.06 19.00 0.60\nD2 76.35 2.41 20.11 1.10\n\nTable 4: The percentage share of parts-of-speech categories\nin cognate datasets D1 and D2\n\nIDs helps provide an appropriate sense to each cognate set\nin the dataset and thus can help automatic cognate detection\ntechniques utilize the synset information.\nThis dataset consists of 1021 cognate sets with a total of\n12252 words. The book consisted of a total of 1556 cog-\nnate sets, but during manual validation, 535 were found\nto be partial cognates and have been ignored from this\ndataset. The percentage share of parts-of-speech categories\nfor Wordnet annotated cognates released is shown in Ta-\nble 4. Partial cognates, as previously explained, mean dif-\nferently in different contexts. NLP tasks such as Machine\nTranslation will benefit the most from gold-standard trans-\nlation candidates. Keeping the application of our dataset in\nmind, we ignore the inclusion of partial cognates from this\ndataset.\n\n3.2. D2 - True Cognate Pairs via IndoWornet\nIn their paper, Kanojia et al. (2019b) identify IndoWord-\nnet (Bhattacharyya, 2017) as a potential resource for the\ntask of cognate detection. They utilize deep neural net-\nwork based approaches to validate their approach for cog-\nnate detection. We build this dataset using a simple ortho-\ngraphic similarity based approach from the IndoWordnet\ndataset. Our approach combines Normalized Edit Distance\n(NED) (Nerbonne and Heeringa, 1997) and Cosine Sim-\nilarity (CoS) (Salton and Buckley, 1988) between words.\nWe compare synset words from every language pair using\nNED and populate a list of cognate sets where NED score\nis 0.7 and above. Similarly, we populate another list of cog-\nnate sets from every language pair using a shingle (n-gram)\nbased Cosine Similarity with the same threshold. Due to\nthe different methods using which NED and CoS similar-\nity techniques compute scores, both NED and CoS output\na different number of word pairs. We choose a common in-\n\ntersection of cognate pairs from among both the lists, and\npopulate a final \u2018potential cognate set\u2019 for eleven Indian\nlanguage pairs. We take the help of two lexicographers and\nmanually validate this output. We are aided by two lexi-\ncographers for each of the language pairs of Hindi (source)\n- (target) \u2018other Indian languages\u20194 Each lexicographer was\nrequested to annotate whether the given word pair is cog-\nnate or not, given the Wordnet synset information, which\ncontained the definition of the concept and an example sen-\ntence. We retain in the final dataset, only cognate pairs,\nwhich were marked to be true cognates by both annotators.\nWe provide the language pair wise cognate data statistics,\npercent agreement, and Cohen\u2019s Kappa (IAA) values for\nthe lexicographers\u2019 annotation in Table 2. The percentage\nshare of parts-of-speech categories for Wordnet annotated\ncognates released is shown in Table 4.\n\n3.3. D3 - False Friends\u2019 Pairs\nThe creation of such a False Friends\u2019 dataset is another one\nof our novel contributions in this paper. We search for false\nfriend candidate pairs by searching for commonly spelled\nwords through the non-parallel synsets. These candidate\npairs initially included partial cognates as well, since words\nwhich are commonly spelled and belong to different senses,\ncould occur in both the contexts. We further prune this list\nby ensuring that these commonly spelled words do not oc-\ncur in parallel synsets and also do not occur in the corre-\nsponding linked synset on either the source or the target\nside. Figure 1 explains our heuristic where Word Y is a\nFalse Friend among Synsets 1 and 2, Word X and Word\nP are True Cognates chosen for D2, and Word A \/ Word\nZ are ignored because they are partial cognates. Once we\nfind out such unique False Friend pairs, which are exact\nmatches in spelling but do not occur in parallel synsets, on\neither side, we populate our list of false friend pairs. We\npopulate this list for eleven language pairs where Hindi is\nalways the source language. Please note that false friends\ndo not follow transitivity, i.e., if A and B are false friend\npairs in languages X and Y, and A and C are false friends\n\n4We intended to isolate the lexicographers of clues from other\nlanguage cognate pairs. Hence, we create cognate data in lan-\nguage pairs.\n\n\n\nin X and Z, it is not necessary that B and C would be false\nfriends. Hence, we populate eleven different false friend\nlists and take lexicographers help for each language pair to\nmanually validate this output. Post-manual validation we\nchoose retain the false friend pair which were annotated as\nfalse friends\u2019 by both the annotators. We report the statis-\ntics for D3 in Table 3, which include the number of poten-\ntial false friend candidates, False friend pairs after manual\nvalidation of these potential candidates, Percent agreement\namong both the annotators and Cohen\u2019s Kappa (IAA) score\nfor the annotation performed.\n\n4. Experiment Setup for Evaluation\nWe evaluate the challenge posed by our datasets using\nthe tasks of automatic cognate detection and false friend\ndetection, based on previously available approaches. In\nthis section, we describe the task setup and approaches\nwhich show the challenges posed by these tasks. We also\ndiscuss how our dataset is a challenging dataset for these\ntasks, and better approaches are needed to tackle the prob-\nlems posed by a morphologically richer dataset of cognates.\n\nWe combine D1 and D2 based on Wordnet Sense IDs and\nremove duplicates to form a single dataset of true cognates,\nwhich we evaluate through the task of cognate detection.\nWe use various approaches to perform the cognate detec-\ntion task viz. baseline cognate detection approaches like\northographic similarity based, phonetic similarity based,\nphonetic vectors with Siamese-CNN based proposed by\nRama (2016), and deep neural network based approaches\nproposed by Kanojia et al. (2019b). We use the same\nhyperparameters and architectures, as discussed in these\npapers. For the Orthographic similarity based approach,\nwe use the orthographic similarity between words as a\nfeature. For the Phonetic similarity based approach, we\ncompute the phonetic similarity between two words using\nphonetic vectors available via the IndicNLP Library5. To\nclassify cognate pairs, we use a simple feed forward neural\nnetwork with the respective feature scores passed to a fully\nconnected layer with ReLU activations, followed by a\nsoftmax layer (in the first two approaches). We replicate\nthe best reported systems from Rama (2016) i.e., Siamese\nConvolutional Neural Network with phonetic vectors as\nfeatures. To replicate Kanojia et al. (2019b)\u2019s approach,\nwe use the Recurrent Neural Network architecture with\na combination of Normalized Edit Distance, Cosine\nSimilarity, and Jaro-Winkler Distance as reported in their\npaper. We have already discussed the manual validation\nof our datasets in the previous section, which allows us to\ncreate a more curated dataset. We use the computational\napproaches on this curated dataset post manual validation.\nFor the training dataset, we use the data provided by\nKanojia et al. (2019b) and create training and validation\nsets with an 80-20 split. We then test the aforementioned\napproaches on our dataset.\n\nFor the False Friends detection task, since no such dataset\nis available for Indian languages, we annotate the data cre-\n\n5https:\/\/anoopkunchukuttan.github.io\/\nindic_nlp_library\/\n\nated by us with positive labels and divide it into train and\ntest sets. We then add true cognates to the training dataset\nwith negative labels since intuitively, they are the best\ncandidates for misclassification due to common spellings\njust like false friends do, but in case of true cognates, they\nalso mean the same. We test our false friends\u2019 dataset using\nsimilar approaches with baseline features like orthographic\nsimilarity, and phonetic similarity. We use a simple Feed\nForward neural network as the classifier with the respective\nfeature scores passed to a fully connected layer with ReLU\nactivations, followed by a softmax layer. To ensure the\nlearning algorithm has \u2018context\u2019 available to decipher\nthe meaning among false friends, we use the notion of\ndistributional semantics and employ a word vectors based\napproach proposed by Castro et al. (2018). We use their\napproach to test the efficacy of our dataset and show better\napproaches need to be devised for morphologically richer\nlanguages.\n\nSince the approach proposed by Castro et al. (2018) re-\nquires monolingual word embeddings to be used, we train\nthe monolingual word embeddings using the standard Wiki-\nmedia dumps6. We extract text from the Wiki dumps and\ntokenize the data. We, then, train twelve monolingual word\nembedding models for each Indian language we are deal-\ning with. In the next section, we discuss the results of our\ndataset evaluations.\n\n5. Results of Our Evaluation\nIn table 5, we show the results for the cognate detection\ntask. We observe that on our combined cognate dataset,\nthe current approaches do not perform well. These ap-\nproaches have reported better performance for their own\ndatasets. In most of the cases (Language pairs), Kanojia et\nal. (2019b)\u2019s approach performs better, but for the Hindi-\nTelugu language pair, Rama (2016)\u2019s approach performs\nbetter. Although both the approaches perform the same\nfor Hi-As, Hi-Mr, and Hi-Ta language pairs, the scores are\nstill lower than what has been previously reported. We be-\nlieve that these approaches perform well when on a lim-\nited dataset, moreover, when the dataset consists of words\nwhich are stripped on morphological inflections. NLP tasks\nsuch as Machine Translation and Cross-lingual Information\nRetrieval do not use synthetic data, which is stripped of\nmorphological information. If the cognate detection task\nhas to be a part of a pipeline aiding the NLP tasks, then ap-\nproaches that perform the task should be robust enough to\ntackle a dataset such as ours. Hence, we claim our dataset to\nbe a more challenging dataset, which should help develop\nbetter approaches.\nIn table 6, we report the results for the task of False Friends\u2019\ndetection. We observe that the approach proposed by Cas-\ntro et al. (2018) does not perform as well as it does for\nSpanish and Portuguese, as reported previously. We be-\nlieve that this approach inherently lacks the linguistic intu-\nition which is needed for the false friends\u2019 detection task.\nPlease recall that False friends are word pairs that spell the\nsame but do not mean the same. But for the approach to\n\n6as on 15th October, 2019\n\nhttps:\/\/anoopkunchukuttan.github.io\/indic_nlp_library\/\nhttps:\/\/anoopkunchukuttan.github.io\/indic_nlp_library\/\n\n\nApproaches Hi-Bn Hi-As Hi-Or Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-Kn\nOrthographic Similarity 0.36 0.34 0.38 0.25 0.29 0.21 0.24 0.28 0.20 0.16 0.19\n\nPhonetic Similarity 0.42 0.38 0.39 0.29 0.32 0.24 0.25 0.31 0.24 0.22 0.25\nRama et. al. (2016) 0.65 0.71 0.61 0.67 0.72 0.47 0.53 0.62 0.53 0.65 0.57\n\nKanojia et. al. (2019) 0.68 0.71 0.62 0.75 0.72 0.73 0.72 0.66 0.53 0.63 0.58\n\nTable 5: Results of the Cognate Detection Task (in terms of F-Scores) for D1+D2. We use the same architecture, features\nand hyperparameters as discussed in the papers for Rama et. al. (2016) and Kanojia et. al. (2019) and observe that these\nsystems do not perform as well on our dataset, as claimed by the authors.\n\nLanguage Pairs Hi-Bn Hi-As Hi-Or Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-Kn\nOrthographic Similarity 0.36 0.45 0.49 0.51 0.53 0.44 0.52 0.24 0.29 0.30 0.50\n\nPhonetic Similarity 0.60 0.66 0.67 0.62 0.59 0.69 0.61 0.54 0.48 0.50 0.57\nCastro et. al. (2018) 0.66 0.64 0.59 0.65 0.69 0.73 0.72 0.65 0.52 0.69 0.64\n\nTable 6: Results of the False Friends\u2019 Detection Task (in terms of F-Scores) for D3. We use the same architecture, features\nand hyperparameters as discussed in the paper by Castro et. al. (2018) and observe that these systems do not perform as\nwell on our False Friends\u2019 dataset.\n\nperform well, monolingual embeddings may not be an ap-\npropriate feature. Cross-lingual word embeddings project\nmonolingual word embeddings into a common space and\nthus should be able to decipher the \u2018meaning\u2019 or the \u2018sense\u2019\nof two different words better, when they belong to different\nlanguages. Given the recent advancements in word rep-\nresentation models, cross-lingual word embedding based\nmodels should be employed for such a task. Please also\nnote that we do not propose a new approach for the task of\nFalse friends\u2019 detection and hence do not perform any ex-\nperimentation with cross-lingual word embeddings. How-\never, Merlo and Rodriguez (2019) show that cross-lingual\nword embeddings obtained using the VecMap (Artetxe et\nal., 2016) approach have shown promise and can be used\nto obtain a semantic comparison between two words from\ndifferent languages.\n\n6. Conclusion and Future Work\nIn this paper, we describe the creation of a challenging\ndataset of true cognates which encompasses of cognates\nfrom two different sources. First, we digitize a cognate\ndictionary and annotate it with Wordnet Sense IDs for\ntwelve Indian languages to create Dataset 1 (D1). We also\nuse linked Indian Wordnets to create a true cognate dataset,\nas described in the paper. For both the datasets, we ensure\na quality check with the help of manual validation. We\nreport the percent agreement and Inter-annotator agreement\nfor D1 and D2 in this paper, and for D2, we retain the\ncognate pairs, which were marked to be cognates by both\nthe annotators; we were aided by two annotators for each\nlanguage pair. Additionally, we release a curated list of\nFalse friends for eleven language pairs where the Hindi\nlanguage is always the source, and other Indian languages\nare the target languages. We evaluate the efficacy of all\nthese datasets using previously available approaches for\nthe tasks of Cognate and False Friends\u2019 detection. We\nshow that these approaches do not perform as well on our\ndataset, given the same hyperparameters and settings as\ndescribed in their papers. We discuss these results in the\nprevious section. We also believe that this work provides a\nchallenging gold-standard dataset for the tasks for Cognate\nand False Friends\u2019 detection, which can also be used to\n\naid the NLP tasks of Machine Translation, Cross-lingual\nInformation Retrieval, and Computational Phylogenetics.\nWe hope better approaches are developed for these tasks\nwhich can perform well on our challenge dataset.\n\nIn the near future, we shall include partial cognates in our\ndataset creation approach and release another dataset on the\nsame repository. Partial cognates mean different given dif-\nferent contexts and can confuse an NLP task. Hence, we\nbelieve it is also important to have a challenging dataset for\npartial cognates as well which can be evaluated via the blin-\ngual bootstrapping approach described by Frunza (2006).\nWe would also like to evaluate our dataset on other NLP\ntasks and report its efficacy in aiding the tasks of MT, CLIR,\nCross-lingual Question Answering etc.\n\n7. Bibliographical References\nAl-Onaizan, Y., Curin, J., Jahr, M., Knight, K., Lafferty, J.,\n\nMelamed, D., Och, F.-J., Purdy, D., Smith, N. A., and\nYarowsky, D. (1999). Statistical machine translation. In\nFinal Report, JHU Summer Workshop, volume 30.\n\nArtetxe, M., Labaka, G., and Agirre, E. (2016). Learning\nprincipled bilingual mappings of word embeddings while\npreserving monolingual invariance. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2289\u20132294, Austin, Texas,\nNovember. Association for Computational Linguistics.\n\nBhargava, A. and Kondrak, G. (2009). Multiple word\nalignment with profile hidden markov models. In Pro-\nceedings of Human Language Technologies: The 2009\nAnnual Conference of the North American Chapter of the\nAssociation for Computational Linguistics, Companion\nVolume: Student Research Workshop and Doctoral Con-\nsortium, pages 43\u201348. Association for Computational\nLinguistics.\n\nBhattacharyya, P. (2017). Indowordnet. In The WordNet in\nIndian Languages, pages 1\u201318. Springer.\n\nBrew, C., McKelvie, D., et al. (1996). Word-pair extrac-\ntion for lexicography. In Proceedings of the 2nd Inter-\nnational Conference on New Methods in Language Pro-\ncessing, pages 45\u201355.\n\n\n\nCastro, S., Bonanata, J., and Rosa\u0301, A. (2018). A high\ncoverage method for automatic false friends detection\nfor spanish and portuguese. In Proceedings of the Fifth\nWorkshop on NLP for Similar Languages, Varieties and\nDialects (VarDial 2018), pages 29\u201336.\n\nCiobanu, A. M. and Dinu, L. P. (2014). Automatic detec-\ntion of cognates using orthographic alignment. In Pro-\nceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers),\nvolume 2, pages 99\u2013105.\n\nCiobanu, A. M. and Dinu, L. P. (2015). Automatic dis-\ncrimination between cognates and borrowings. In Pro-\nceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Vol-\nume 2: Short Papers), volume 2, pages 431\u2013437.\n\nEmeneau, M. B. (1956). India as a lingustic area. Lan-\nguage, 32(1):3\u201316.\n\nFrunza, O. (2006). Automatic identification of cognates,\nfalse friends, and partial cognates [tesis de maestr\u0131\u0301a]. Ot-\ntawa, University of Ottawa, Master on Computer Sci-\nence.\n\nHammer, P. and Monod, M. (1976). English-french cog-\nnate dictionary. In English-French Cognate Dictionary.\nERIC.\n\nHauer, B. and Kondrak, G. (2011). Clustering semantically\nequivalent words into cognate sets in multilingual lists.\nIn Proceedings of 5th international joint conference on\nnatural language processing, pages 865\u2013873.\n\nJa\u0308ger, G., List, J.-M., and Sofroniev, P. (2017). Using sup-\nport vector machines and state-of-the-art algorithms for\nphonetic alignment to identify cognates in multi-lingual\nwordlists. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational\nLinguistics: Volume 1, Long Papers, volume 1, pages\n1205\u20131216.\n\nJa\u0308rvelin, A., Ja\u0308rvelin, A., and Ja\u0308rvelin, K. (2007). s-grams:\nDefining generalized n-grams for information retrieval.\nInformation Processing & Management, 43(4):1005\u2013\n1019.\n\nKanojia, D., Kulkarni, M., Bhattacharyya, P., and Haffari,\nG. (2019a). Cognate identification to improve phyloge-\nnetic trees for indian languages. In Proceedings of the\nACM India Joint International Conference on Data Sci-\nence and Management of Data, pages 297\u2013300. ACM.\n\nKanojia, D., Patel, K., Bhattacharyya, P., Kulkarni, M.,\nand Haffari, R. (2019b). Utilizing wordnets for cognate\ndetection among indian languages. In Global Wordnet\nConference (2019).\n\nKondrak, G. and Dorr, B. (2004). Identification of con-\nfusable drug names: A new approach and evaluation\nmethodology. In Proceedings of the 20th international\nconference on Computational Linguistics, page 952. As-\nsociation for Computational Linguistics.\n\nKondrak, G. (2000). A new algorithm for the alignment\nof phonetic sequences. In Proceedings of the 1st North\nAmerican chapter of the Association for Computational\nLinguistics conference, pages 288\u2013295. Association for\nComputational Linguistics.\n\nList, J.-M. (2012). Lexstat: Automatic detection of cog-\nnates in multilingual wordlists. In Proceedings of the\nEACL 2012 Joint Workshop of LINGVIS & UNCLH,\npages 117\u2013125. Association for Computational Linguis-\ntics.\n\nMakin, R., Pandey, N., Pingali, P., and Varma, V. (2007).\nApproximate string matching techniques for effective\nclir among indian languages. In International Work-\nshop on Fuzzy Logic and Applications, pages 430\u2013437.\nSpringer.\n\nMann, G. S. and Yarowsky, D. (2001). Multipath trans-\nlation lexicon induction via bridge languages. In Pro-\nceedings of the second meeting of the North American\nChapter of the Association for Computational Linguis-\ntics on Language technologies, pages 1\u20138. Association\nfor Computational Linguistics.\n\nMelamed, I. D. (1999). Bitext maps and alignment via pat-\ntern recognition. Computational Linguistics, 25(1):107\u2013\n130.\n\nMeng, H. M., Lo, W.-K., Chen, B., and Tang, K. (2001).\nGenerating phonetic cognates to handle named entities\nin english-chinese cross-language spoken document re-\ntrieval. In IEEE Workshop on Automatic Speech Recog-\nnition and Understanding, 2001. ASRU\u201901., pages 311\u2013\n314. IEEE.\n\nMerlo, P. and Rodriguez, M. A. (2019). Cross-lingual\nword embeddings and the structure of the human bilin-\ngual lexicon. In Proceedings of the 23rd Conference\non Computational Natural Language Learning (CoNLL),\npages 110\u2013120.\n\nMielke, M. M., Roberts, R. O., Savica, R., Cha, R.,\nDrubach, D. I., Christianson, T., Pankratz, V. S., Geda,\nY. E., Machulda, M. M., Ivnik, R. J., et al. (2012). As-\nsessing the temporal relationship between cognition and\ngait: slow gait predicts cognitive decline in the mayo\nclinic study of aging. Journals of Gerontology Series A:\nBiomedical Sciences and Medical Sciences, 68(8):929\u2013\n937.\n\nNerbonne, J. and Heeringa, W. (1997). Measuring di-\nalect distance phonetically. In Computational Phonol-\nogy: Third Meeting of the ACL Special Interest Group\nin Computational Phonology.\n\nPrado, M. (1993). Ntc\u2019s dictionary of spanish false cog-\nnates. In NTC\u2019s dictionary of Spanish false cognates.\nNTC.\n\nRama, T., List, J.-M., Wahle, J., and Ja\u0308ger, G. (2018). Are\nautomatic methods for cognate detection good enough\nfor phylogenetic reconstruction in historical linguistics?\narXiv preprint arXiv:1804.05416.\n\nRama, T. (2014). Gap-weighted subsequences for auto-\nmatic cognate identification and phylogenetic inference.\narXiv preprint arXiv:1408.2359.\n\nRama, T. (2016). Siamese convolutional networks for cog-\nnate identification. In Proceedings of COLING 2016, the\n26th International Conference on Computational Lin-\nguistics: Technical Papers, pages 1018\u20131027.\n\nSalton, G. and Buckley, C. (1988). Term-weighting ap-\nproaches in automatic text retrieval. Information pro-\ncessing & management, 24(5):513\u2013523.\n\n\n\nSimard, M., Foster, G. F., and Isabelle, P. (1993). Using\ncognates to align sentences in bilingual corpora. In Pro-\nceedings of the 1993 conference of the Centre for Ad-\nvanced Studies on Collaborative research: distributed\ncomputing-Volume 2, pages 1071\u20131082. IBM Press.\n\nTufis, D. (2002). A cheap and fast way to build useful\ntranslation lexicons. In COLING 2002: The 19th Inter-\nnational Conference on Computational Linguistics.\n\nWu, W. and Yarowsky, D. (2018). Creating large-scale\nmultilingual cognate tables. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018).\n\n\n\t1. Introduction and Motivation\n\t2. Related Work\n\t3. Dataset Creation\n\t3.1. D1 - True Cognate Sets\n\t3.2. D2 - True Cognate Pairs via IndoWornet\n\t3.3. D3 - False Friends' Pairs\n\n\t4. Experiment Setup for Evaluation\n\t5. Results of Our Evaluation\n\t6. Conclusion and Future Work\n\t7. Bibliographical References\n\n","4":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing Cross-lingual Features to Improve Cognate Detection for\nLow-resource Languages\n\nDiptesh Kanojia\u2020,\u2663,?, Raj Dabre\ufffd, Shubham Dewangan\u2020,\nPushpak Bhattacharyya\u2020, Gholamreza Haffari?, and Malhar Kulkarni\u2020\n\n\u2020IIT Bombay, India, \ufffdNICT, Japan\n\u2663IITB-Monash Research Academy, India\n\n?Monash University, Australia\n\u2020{diptesh,pb,malhar}@iitb.ac.in, \ufffdraj.dabre@nict.go.jp\n\n\u2020sdofficial1996@gmail.com, ?gholamreza.haffari@monash.edu\n\nAbstract\n\nCognates are variants of the same lexical form across different languages; for example \u201cfonema\u201d\nin Spanish and \u201cphoneme\u201d in English are cognates, both of which mean \u201ca unit of sound\u201d. The\ntask of automatic detection of cognates among any two languages can help downstream NLP\ntasks such as Cross-lingual Information Retrieval, Computational Phylogenetics, and Machine\nTranslation. In this paper, we demonstrate the use of cross-lingual word embeddings for detecting\ncognates among fourteen Indian Languages. Our approach introduces the use of context from a\nknowledge graph to generate improved feature representations for cognate detection. We then\nevaluate the impact of our cognate detection mechanism on neural machine translation (NMT),\nas a downstream task. We evaluate our methods to detect cognates on a challenging dataset of\ntwelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil,\nTelugu, Punjabi, Bengali, Marathi, and Malayalam. Additionally, we create evaluation datasets\nfor two more Indian languages, Konkani and Nepali1. We observe an improvement of up to\n18% points, in terms of F-score, for cognate detection. Furthermore, we observe that cognates\nextracted using our method help improve NMT quality by up to 2.76 BLEU. We also release2\n\nour code, newly constructed datasets and cross-lingual models publicly.\n\n1 Introduction\n\nIndia is a multilingual, multi-script country with 22 scheduled languages and 12 written script forms pri-\nmarily belonging to 6 different language families. More than a billion people use these languages as their\nfirst language. A significant amount of news and information is found on the web in these languages,\nwhich is inaccessible to people of other regions within the country. Most of the Indian language texts\nfound online have several words that have originated from Sanskrit, Persian, and English. While, in many\ncases, one might argue that such occurrences do not belong to an Indian language, the frequency of such\nusage indicates a wide acceptance of these foreign language words as Indian language words. In numer-\nous cases, these words also are morphologically altered as per the Indian language morphological rules to\ngenerate new variants of existing words. Detection of such variants or \u2018Cognates\u2019 across languages helps\nCross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation\n(MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenet-\nics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011).\nHowever, NLP applications are concerned with the set of cognate words which have similarities in their\nspelling and their meaning. For example, the French and English word pair, Liberte\u0301 - Liberty, reveals\nitself to be a true cognate through orthographic similarity. In some cases, similar words have a common\nmeaning only in some contexts; such words are called partial cognates. For example, the word \u201cpolice\u201d\nin French can translate to \u201cpolice\u201d, \u201cpolicy\u201d or \u201cfont\u201d, depending on the context3. Manual detection\n\n1It is primarily spoken in Nepal, but is also adopted in the list of scheduled languages of the Republic of India.\n2Link: Data, code and models\n3Cognates can also exist in the same language. Such word pairs\/sets are commonly referred to as doublets.\n\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:\/\/\ncreativecommons.org\/licenses\/by\/4.0\/.\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n08\n78\n\n9v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n6 \nD\n\nec\n 2\n\n02\n1\n\nhttp:\/\/www.cfilt.iitb.ac.in\/coling2020diptesh\nhttp:\/\/creativecommons.org\/licenses\/by\/4.0\/\nhttp:\/\/creativecommons.org\/licenses\/by\/4.0\/\n\n\nof such cognate sets requires a human expert with a good linguistic background in multiple languages.\nMoreover, manual annotation of cognate sets is a costly task in terms of time and human effort.\n\nThe task of cognate detection across languages requires one to detect word pairs which are etymolog-\nically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu\nand Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these meth-\nods have a limitation since they do not take into consideration the notion of semantic similarity across\nlanguages. A key question that we try to answer in this paper is,\n\n\u201cCan semantic information be leveraged from Cross-lingual models to improve cognate detec-\ntion amongst low-resource languages?\u201d\n\nWe hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets\nand cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the seman-\ntic information from cross-lingual word embeddings. Cross-lingual word embeddings can be obtained\nby training monolingual embeddings for individual languages and then projecting them in a shared space\nusing a bilingual dictionary. In the absence of such a bilingual dictionary for low-resource languages,\nadversarial training can be used over identical words to generate the projections. We build cross-lingual\nmodels for thirteen language pairs with Hindi as the source (L1) and thirteen target Indian languages\n(L2). We use the context information from a knowledge graph to build the context dictionaries for\neach pair. The cross-lingual models help us obtain embeddings for the word-pair and the respective\ncontext dictionaries, from a shared space. We hypothesize that using this approach should provide a\nmore accurate semantic measure for the detection of cognate pairs. The use of orthographic and pho-\nnetic similarity-based methods to perform the same task provides us with baselines for a comparative\nevaluation.\n\nA motivation to investigate this task for low-resource Indian languages stems from the fact that most of\nthe Indian languages borrow cognates or \u201cloan words\u201d from the Sanskrit language. It is, for the most part,\nconsidered a historical antecedent of almost all the Indian languages. Indo-Aryan languages like Hindi,\nBengali, Gujarati, Punjabi borrow from Sanskrit. They borrow many lexical forms and language proper-\nties from Sanskrit. Dravidian languages are highly agglutinative and morphologically rich like Sanskrit,\nwhich makes them tough to parse computationally. Marathi and Hindi suffer from the same ailment\neven though Hindi is not considered as agglutinative as Marathi, but it does exhibit compounding4 which\nmakes it, yet again, difficult to parse for CLIR and MT systems, and to detect cognates based solely on\northographic similarity. Given that CLIR and MT are usually based on a full-form lexicon, one of the\npossible issues in the generation of cognates concerns the similarity of words in their root form vs the\nsimilarity in their lexical form. For example, the Sanskrit word \u201cmatra\u201d and the English word \u201cMother\u201d\nare known cognates from the Proto-Indo-European language family where the root and the meaning are\nidentical, but the lexical form is considerably different. Our approach handles such cases by inculcating\nthe sub-word information while building the embeddings and helps reduce the out-of-vocabulary (OOV)\nwords, which have proven to be a challenge for well-established CLIR systems (Udupa et al., 2009).\n\nThis paper is organized as follows. Section 2 briefly describes the previous work done in the area\nof automatic cognate detection. Section 3 describes the dataset source, our additions to it, and the\nexperimental setup. Section 4 presents the approaches used in terms of feature sets and classification\nmethodologies. The results obtained are described in Section 5 along with a discussion on the qualitative\nanalysis of our output. Section 6 concludes this article with possible future work in the area.\n\n2 Related Work\n\nThe two main existing approaches for the detection of cognates belong to the generative and discrimina-\ntive paradigms. The first set of approaches is based on the computation of a similarity score between po-\ntential candidate pairs. This score can be based on orthographic similarity (Ja\u0308ger et al., 2017; Melamed,\n1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a dis-\ntance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiede-\n\n4Compounding means when two or more words or signs are joined to make a longer word or sign.\n\n\n\nmann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are\nbased on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations\nof the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009).\n\nCognate Detection has been explored vastly in terms of classification methodologies. Previously,\nRama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with\nlanguage relatedness for cognate identification, which was achieved through phoneme encodings. Ja\u0308ger\net al. (2017) use SVM for phonetic alignment and perform cognate detection for various language fami-\nlies. Various works on orthographic cognate detection usually take alignment of substrings within clas-\nsifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak,\n2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment.\nKanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their\nwork includes manual verification and segratation of their output into cognates and non-cognates. Kano-\njia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and\nnon-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al.\n(2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition,\neven in tasks manually performed by humans. They discuss how the need for recognizing semantic sim-\nilarity arises for non-identical cognates, based on the reaction time from human annotators. Similarly,\nMerlo and Andueza Rodriguez (2019) show that cross-lingual models exhibit the semantic properties of\nfor bilingual lexicons despite their structural simplicities, which leads us to perform our investigation for\nlow-resource Indian languages. Uban et al. (2019) discuss the semantic change in languages by studying\nthe change in cognate words across Romance languages using cross-lingual similarity. All of the previ-\nous approaches discussed above, lack the use of an appropriate cross-lingual similarity-based measure\nand do not work well for Indian languages as shown in this work. This paper discusses the quantita-\ntive and qualitative results using our approach and then, applies our output to different neural machine\ntranslation architectures.\n\nLanguage Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko*\nCognates 15312 17021 15726 14097 21710 9235 3363 936 3478 4103 11894 2560 11295\n\nNon-Cognates 16119 15057 15983 15166 23029 8976 4005 1084 4101 3810 13027 1918 9826\n\nTable 1: Number of cognates and non-cognates for each language pair in the dataset. Hi-Ne* and Hi-Ko*\nwere generated via replicating their approach (Kanojia et. al., 2020).\n\nLanguage Hi Bn Gu Mr Pa Sa Ml Ta Te Ne As Kn Ko Or\nCorpus Size 48142K 1564K 439K 520K 505K 553K 495K 909K 1023K 706K 504K 159K 214K 744K\n\nSTTR (n=1000) 0.5821 0.5437 0.4587 0.6108 0.4314 0.5350 0.7339 0.6411 0.4950 0.4883 0.5968 0.5338 0.5614 0.4160\n\nTable 2: Corpus Statistics where corpus size is the approximate number of lines, and STTR is the moving\naverage type-token ratio on a windows of 1000 sentences.\n\n3 Dataset and Experimental Setup\n\nIn this section, we describe our primary dataset for the cognate detection task. We\nalso describe the datasets used for building cross-lingual word embedding mod-\nels, and the parallel corpora used for the Neural Machine Translation (NMT).\n\nFigure 1: Dataset Augmentation with Context and\nTwo Language Pairs using IndoWordnet.\n\nFor our experiments, we use the publicly re-\nleased challenge dataset (Kanojia et al., 2020)\nof cognates. This dataset provides labelled cog-\nnate and non-cognate pairs for twelve Indian\nlanguages namely, Sanskrit (Sa), Hindi (Hi), As-\nsamese (As), Oriya (Or), Kannada (Kn), Gu-\njarati (Gu), Tamil (Ta), Telugu (Te), Punjabi\n(Pa), Bengali (Bn), Marathi (Mr), and Malay-\nalam (Ml). We reproduce their approach to add\ntwo more languages, Konkani (Ko) and Nepali\n\n\n\n(Ne), to this dataset. For building context dic-\ntionaries, we use linked Indian language wordnets (Bhattacharyya, 2017) and concatenate the concept\ndefinition and example sentences. We remove stop words from the context dictionaries and append them\nwith their respective word pairs. The lexical overlap between the language pairs ranges from 13% (for\nHi-Te) to only 23% (Hi-Mr). Figure 1 shows an accurate description of the dataset creation process. The\ncognate dataset statistics are described in Table 1.\n\nMonolingual Corpora for Word Embeddings\n\nThe dataset for training cross-lingual models is obtained from various sources. Word embeddings require\na large quantity of monolingual corpora for efficient training of a usable model with high accuracy. We\nextract corpora for these fourteen Indian Languages from various sources and collect them in a single\nrepository. We extract Wikimedia dumps5 for all languages and add Indian Language Corpora Initiative\n(ILCI) corpora (Jha, 2010) for these languages to each of them. For Hindi, Marathi, Nepali, Bengali,\nTamil, and Gujarati we add crawled corpus of film reviews and news websites6 to their corpus. For\nHindi, we also add HinMonoCorp 0.5 (Bojar et al., 2014) to our corpus adding approximately 44 million\nsentences. For Sanskrit, we download a raw corpus of proses7 and add it to our corpus. Training corpus\nstatistics (approximate number of total lines) are shown in Table 2.\n\nParallel Corpora for NMT\n\nTo validate the application of cognates for the Machine Translation task, we choose the Neural Machine\nTranslation setting and use the Indian Languages Corpora Initiative (ILCI) Phase 1 corpus. This corpus\ncontains approximately 50K parallel sentences across 11 languages (English and 10 Indian Languages),\nfrom health and tourism domains. For every language pair, the parallel corpus was split up into a training\nset of 46,277 sentences, a test set of 2000 sentences and development set of 500 sentences. The train,\ntest and development splits were ensured to be parallel across all language pairs involved. The language\npair intersection for our cognate detection work and this parallel corpus limited our MT experimentation\nto the following languages namely, Hindi (Hi), Punjabi (Pa), Bengali (Bn), Gujarati (Gu), Marathi (Mr),\nTamil (Ta), Telugu (Te) and Malayalam (Ml). We keep Hi as the source and remaining languages as the\ntarget languages for our experiments. We describe the experimental setup for our task below.\n\n3.1 Unicode Offset based Transliteration\n\nIndian languages use different scripts, and lexical similarity-based metrics cannot be directly used on any\ntext for character matching. For standardization, we choose to convert any other script to the Devanagari\nscript. We perform Unicode transliteration using Indic NLP Library8 to convert scripts for Bn, As, Or,\nGu, Pa, Ml, Ta, Kn and Te to Devanagari for standardization. Hi, Mr, Ko, Ne, and Sa are already based\non the Devanagari script. We perform this for script transliteration for both the cognate dataset (Table 1)\nand the corpus (Table 2). We describe the creation of cross-lingual word embeddings below.\n\n3.2 Cross-lingual Word Embedding Methodologies\n\nUsing the monolingual corpora described above, we build monolingual word embeddings using the Fast-\nText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is ben-\neficial for a task such as ours where sub-words play an important role, and spelling variations can lead\nto different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or M-\nBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliter-\nated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each\nlanguage using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two\ncharacters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words.\n\n5Link: Wikimedia Dumps; as on April 22, 2020\n6Link: Additional Monolingual Corpus\n7Link: JNU Sanskrit Proses Corpus\n8Link: Indic NLP Library\n9Link: FastText - GitHub\n\nhttps:\/\/dumps.wikimedia.org\/\nhttps:\/\/github.com\/goru001?tab=repositories\nhttp:\/\/sanskrit.jnu.ac.in\/currentSanskritProse\/\nhttps:\/\/anoopkunchukuttan.github.io\/indic_nlp_library\/\nhttps:\/\/github.com\/facebookresearch\/fastText\n\n\nWe use three different methodologies for training the cross-lingual word embedding models on all\nthe language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology\nuses the supervised method named MUSE (Conneau et al., 2017)10 which utilizes a manually curated\nbilingual lexicon11 for alignments. We use Hindi as a pivot language due to the ease of computation and\navailability of resources (Corpora and WordNet size). We use the monolingual models described above\nand train 13 cross-lingual word embedding models (thirteen language pairs over 100 dimensions) using\nthis approach.\n\nThe second cross-lingual methodology uses VecMap (Artetxe et al., 2018), which utilizes the mono-\nlingual models created above. VecMap uses an optional normalization feature while it builds the map-\npings between any two monolingual models. It performs orthogonal transformation and maps semanti-\ncally related words, similar to MUSE, which was used in our first approach for building cross-lingual\nmodels. Additionally, it also reduces the dimensions of the embeddings models, which, is optional. We\ntrain it using the same hyperparameters as described above, for consistency while evaluating. We used the\nsupervised approach for training these models as well, and the training dictionary was similar to the one\nprovided to the MUSE method. We obtain thirteen models, one for each language pair, using VecMap.\nThe third methodology utilizes contextual embeddings which have shown to outperform the conven-\ntional word embeddings based models for many tasks (Devlin et al., 2018). We choose the most recent\nmethodology for building a single cross-lingual model for all the languages. XLM-R (Conneau et al.,\n2019) uses previously proposed approaches of XLM (Lample and Conneau, 2019) and RoBERTa (Liu\net al., 2019) to attain a very high performing cross-lingual model, especially for low-resource languages.\nWe use our transliterated corpora described above and concatenate it into a single large corpus required\nfor training the model. We then use the unsupervised training method of XLM-R and train a model over\nsix days and a couple more hours with a reduced batch size.\n\nTo put it more concisely, we trained cross-lingual models using three different methodologies (MUSE,\nVecMap and XLM-R) where the cross-lingual mapping obtained for MUSE and VecMap were generated\nvia the monolingual embeddings, as described above. We obtained thirteen models using each of these\ntwo methods. A single cross-lingual model was, however, trained using XLM-R and used for the third\ncross-lingual approach whose training methodology has been described above. We utilize the last layer\nfrom the XLM-R model to generate representations for each token.\n\n4 Approaches\n\nWe use various approaches to perform the cognate detection task viz. baseline cognate detection ap-\nproaches like orthographic similarity-based, phonetic similarity-based, phonetic vectors with Siamese-\nCNN based proposed by Rama (2016), and Recurrent neural network-based approach proposed by Kano-\njia et al. (2019b). We use the same hyperparameters and architectures, as discussed in these papers. We\ndescribe each of these feature sets in this section.\n\n4.1 Weighted Lexical Similarity (WLS)\n\nThe Normalized Edit Distance (NED) approach computes the edit distance (Nerbonne and Heeringa,\n1997) for all word pairs in our dataset. Each of the operations has unit cost (except that substitution of\na character by itself has zero cost), so NED is equal to the minimum number of operations to transform\n\u2018word a\u2019 to \u2018word b\u2019. We use a similarity score provided by NED, which is calculated as (1 - NED\nScore). We combine NED with q-gram distance (Shannon, 1948) for a better similarity score. The q-\ngrams (\u2018n-grams\u2019) are simply substrings of length q. This distance measure has been applied previously\nfor various spelling correction approaches (Owolabi and McGregor, 1988; Kohonen, 1978). Kanojia et\nal. (2019b) propose this metric and we replicate it to generate features for their baseline approach. For\nany word pair with words p and q, it is as follows:\n\nWLSpq = (NEDpq \u2217 0.75) + (QDpq \u2217 0.25) (1)\n10Link: MUSE - GitHub\n11Link: Bilingual Lexicon\n\nhttps:\/\/github.com\/facebookresearch\/MUSE\nhttp:\/\/www.cfilt.iitb.ac.in\/Downloads.html\n\n\nNow that this approach can be used to compute a score between each word pair, we use it to find two\nscores, which are used as features - \u2018word-pair similarity\u2019 and \u2018contextual similarity\u2019. Each candidate\nword-pair generates a score i.e., score1, and the average of scores among all words in the context dictio-\nnary generates another score i.e., score2, which are normalized as follows:\n\nS1 = score1\/ (score1 + score2)\n\nS2 = score2\/ (score1 + score2)\n(2)\n\nWe use S1 and S2 as features for this orthographic similarity-based baseline approach.\n\n4.2 Phonetic Vectors and Similarity (PVS)\nThe IndicNLP Library provides phonetic features based vector for each character in various Indian lan-\nguage scripts. We utilize this library to compute a feature vector for each word by computing an average\nover character vectors. We compute vectors for both words in the candidate cognate pairs (PVS and\nPVT ) and also compute contextual vectors (PCVS and PCVT ) by averaging the vectors for all the con-\ntext dictionaries on each side (source and target), generating a total of four vectors. We also calculate the\ncosine similarity among PVS and PVT , and among PCVS and PCVT to generate two similarity scores\n(PS1, and PS2) which are normalized using (2) and, additionally, used as features during classification.\nIt should be noted that using phonetic vectors and their similarity scores has already been proposed in\nthe previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to\nbe our novel contribution.\n\n4.3 Cross-lingual Vectors & Similarity\nAs described above, we train cross-lingual embedding models by aligning two disjoint monolingual\nvector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval\net al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this\ndictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual\nembedding models trained on Indian language pairs. We obtain vectors for word-pairs (WVS and WVT )\nand averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain\nvectors for each candidate pair and their context using all the three cross-lingual methodologies.\n\nAdditionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their con-\ntexts. Angular similarity distinguishes nearly parallel vectors much better as small changes in vector\nvalues yield considerable distances. For each word pair vector and its context vectors, we compute the\n\u2018word-pair similarity\u2019 and \u2018contextual similarity\u2019. We use arccos to obtain angular cosine similarity\n(asim) among vectors \u2018u\u2019 and \u2018v\u2019, as shown below:\n\nasim(u, v) =\n\n(\n1\u2212 arccos\n\n(\nu.v\n\n\u2016u\u2016\u2016v\u2016\n\n)\n\/\u03c0\n\n)\n(3)\n\nEach candidate word-pair generates a score i.e., score1, and the average of scores among all words in the\ncontext dictionary generates another score i.e., score2, which are also normalized using (2).\n\n4.4 Classification Methodology\nWe pose the task of detecting cognates as a binary classification problem. We employ both classi-\ncal machine learning-based models and a simple feed-forward neural network. To compare our work\nwith the previously proposed approaches, we replicate the best-reported systems from Rama (2016) i.e.,\nSiamese Convolutional Neural Network with phonetic vectors as features and also replicate Kanojia et\nal. (2019b)\u2019s approach which uses a Recurrent Neural Network architecture with a weighted lexical sim-\nilarity (WLS) as a feature set. The input to our classifiers is the feature sets described above for each\ncandidate pair. The candidates are the complete data described in Table 1. Cognates from Table 1 are\nlabelled positive, and non-cognates are labelled negative. We perform 5-fold stratified cross-validation,\nwhich divides the data into train and test folds, randomly. An architecture diagram for our classification\napproach is shown in Figure 2.\n\n\n\nFigure 2: Cognate Detection task with different feature sets and classification approaches.\n\nAmong the classical machine learning models, we use Support Vector Machines (SVM) and Logistic\nRegression (LR). We experiment with the use of both linear SVMs and kernel SVMs (Gaussian and\nPolynomial). We perform a grid-search to find the best hyper-parameter value for C over the range of\n0.01 to 1000. We deploy the Feed Forward Neural Network (FFNN) with one hidden layer. We perform\ncross-validation with different settings for activation function (tanh, hardtanh, sigmoid and relu) and\nthe hidden layer dimension in the network (30, 50, 100, and 150). We use binary cross-entropy as the\noptimization algorithm. Finally, we choose the hyper-parameter configuration with the best validation\naccuracy. We train the model with the selected configuration with an initial learning rate of 0.4, and we\nhalve the learning rate when the error on the validation split increases. We stop the training once the\nlearning rate falls below 0.001. We perform our experiments with the feature sets (Orthographic (WLS),\nPhonetic (PVS), and three different cross-lingual embeddings based feature sets) described above for all\nthe thirteen language pairs. We also perform an ablation test with various feature sets and report the\nresults for the best feature combination in the next section. The results of our classification task can be\nseen in Table 3 and are discussed in the next section, in detail.\n\n4.5 Cognate-aware Neural Machine Translation (NMT) Task\n\nFor the NMT task, we use the OpenNMT-Py toolkit (Klein et al., 2017) to perform our experiments.\nWe use a Bidirectional RNN Encoder-Decoder architecture with attention (Bahdanau et al., 2014). We\nchoose three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The\nhidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learn-\ning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000\nsteps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge\noperations, initially, in an endeavour to find the best baseline model with an optimal number of merge\noperations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni\net al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of\nmerge operation results in the supplementary material. We call this the NMT-BPE Baseline.\n\nTo validate our hypothesis that our approach can help the NMT task, we inject the cognates detected\nusing our approach to the parallel corpus for their respective language pairs, as single word sentences.\nLexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al.,\n2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are\napproximately around 1M tokens (Arthur et al., 2016). Our detected cognate list size varies from 930\ncognates (Hi-Te) to 15834 (Hi-Mr). Due to the addition of more parallel instances to the corpus, the\nvocabulary size for NMT increases. Hence, we experiment further by varying the BPE merges, in a close\nrange, to the optimal merge point obtained earlier. We report the results of the best optimal merge setting,\nfor both NMT-BPE Baseline model and the cognate injected NMT-BPE model, in the section below. A\nmore detailed set of results for all the merge operations is available in the supplementary material.\n\n\n\nBaseline Approaches Cross-lingual Embeddings based Approaches Best Combination\n\nLP\nWLS w\/ FFNN\n\nPVS\nw\/\n\nSiamese CNN\n(Rama, 2016)\n\nWLS w\/ RNN\n(Kanojia et al.,\n\n2019)\n\nXLM-R\nw\/ FFNN\n\nMUSE\nw\/ FFNN\n\nVecMap\nw\/ FFNN\n\nMUSE + WLS\nw\/\n\nFFNN\n\nP R F P R F P R F P R F P R F P R F P R F\n\nHi-Bn 0.51 0.28 0.36 0.68 0.62 0.65 0.67 0.69 0.68 0.81 0.76 0.78 0.77 0.75 0.76 0.72 0.74 0.73 0.80 0.75 0.77\nHi-As 0.48 0.26 0.34 0.72 0.71 0.71 0.72 0.70 0.71 0.70 0.72 0.71 0.80 0.75 0.77 0.74 0.73 0.73 0.84 0.75 0.79\nHi-Or 0.51 0.30 0.38 0.65 0.58 0.61 0.66 0.58 0.62 0.65 0.61 0.63 0.72 0.68 0.70 0.67 0.70 0.68 0.81 0.69 0.75\nHi-Gu 0.43 0.16 0.23 0.70 0.65 0.67 0.81 0.71 0.76 0.80 0.73 0.76 0.80 0.84 0.82 0.77 0.74 0.75 0.83 0.85 0.84\nHi-Ne 0.50 0.16 0.24 0.72 0.84 0.78 0.78 0.73 0.75 0.75 0.75 0.75 0.86 0.83 0.84 0.78 0.73 0.75 0.86 0.83 0.84\nHi-Mr 0.51 0.20 0.29 0.70 0.68 0.69 0.74 0.70 0.72 0.76 0.71 0.73 0.70 0.73 0.71 0.71 0.71 0.71 0.72 0.73 0.72\nHi-Ko 0.47 0.24 0.32 0.63 0.63 0.63 0.63 0.59 0.61 0.66 0.58 0.62 0.69 0.73 0.71 0.61 0.60 0.60 0.70 0.75 0.72\nHi-Pa 0.28 0.17 0.21 0.51 0.44 0.47 0.76 0.72 0.74 0.75 0.71 0.73 0.83 0.78 0.80 0.71 0.74 0.72 0.83 0.78 0.80\nHi-Sa 0.34 0.19 0.24 0.55 0.51 0.53 0.73 0.71 0.72 0.75 0.70 0.72 0.77 0.76 0.76 0.73 0.71 0.72 0.80 0.77 0.78\nHi-Ml 0.49 0.20 0.28 0.59 0.66 0.62 0.66 0.66 0.66 0.72 0.63 0.67 0.76 0.71 0.73 0.69 0.71 0.70 0.77 0.71 0.74\nHi-Ta 0.22 0.19 0.20 0.49 0.58 0.53 0.49 0.58 0.53 0.63 0.51 0.56 0.72 0.68 0.70 0.66 0.72 0.69 0.72 0.70 0.71\nHi-Te 0.18 0.15 0.16 0.60 0.71 0.65 0.62 0.71 0.66 0.65 0.70 0.67 0.70 0.72 0.71 0.67 0.67 0.67 0.73 0.72 0.72\nHi-Kn 0.19 0.18 0.18 0.54 0.60 0.57 0.58 0.60 0.59 0.60 0.58 0.59 0.69 0.73 0.71 0.65 0.64 0.64 0.70 0.73 0.71\n\nTable 3: Results of the cognate detection task, in terms of weighted F-scores (5-fold) with baseline\nfeatures and previous approaches, and our approaches using Cross-lingual similarity based features, for\nall the language pairs (LP).\n\n5 Results and Discussion\n\nFrom Table 3, among the baseline approaches, we observe high precision but very low recall scores when\nWeighted Lexical Similarity (WLS) based features are used. In fact, for language pairs which contain\nthe Dravidian languages (Hi-Ml, Hi-Ta, Hi-Te, and Hi-Kn), even the precision scores are observed to be\nvery low. The classifiers are not able to predict a significant amount of positively labelled cognate pairs,\ncorrectly. Even simple lexical variants such as \u201cAag (Fire)\u201d (Hindi) and \u201cAgni (Fire)\u201d (Telugu) were\nclassified incorrectly, as non-cognates. Phonetic vectors paired with a Siamese CNN (Rama, 2016),\nhowever, mitigate such misclassifications and are shown to perform well with much higher recall, for all\nthe language pairs. Kanojia et al. (2019b)\u2019s approach, however, outperforms the phonetic vectors based\napproach. We observe marginal improvements in F-scores for almost all the language pairs (except Hi-\nKo and Hi-Ne) when their RNN based approach is used. As for our approaches, SVM and Logistic\nRegression based classification methodologies were consistently outperformed by the FFNN method.\nHence, we report precision (P), recall (R), and F-scores (F) for only FFNN based approaches in Table 3.\n\nOur cross-lingual similarity-based approaches, however, significantly outperform all the baseline ap-\nproaches. We observe a stark improvement in both precision and recall scores for all the language pairs.\nThe cross-lingual approach, which uses the vectors from VecMap based models, fails to outperform both\nMUSE and XLM-R based models. XLM-R model exclusively achieves the best f-score for two language\npairs (Hi-Bn and Hi-Mr). We believe its performance can be attributed to the closeness of the language\npairs as they belong to the same language family (Indo-Aryan). Moreover, XLM-R is a transformer\narchitecture-based model which requires relatively larger corpora sizes and a decent amount of corpus\nwas available to build word embedding models for these target languages (Table 2). The cross-lingual\nmodels built above are used to provide vectors for calculating the similarity between words and contexts,\nbringing in the notion of semantic similarity for the task of cognate detection. Please note that by the\ndefinition of cognates, they are semantically similar despite the lexical variance. We observe that MUSE\nbased feature representations paired with FFNN, obtain the best F-scores. This observation stands true\neven when the target language belongs to the Dravidian language family, where our baseline approaches\nlack severely in performance. For example, \u201cmkarand-maKarantam (pollen)\u201d (Hi-Ta), a cognate pair\nwas classified correctly only using the MUSE based approach.\n\nAdditionally, we perform an ablation test with our feature sets for further experimentation. We observe\nthat the combination of WLS and vectors from the MUSE model performs even better. An improvement\nis observed for eight language pairs out of thirteen ranging from 1% point (Hi-Ko, Hi-Ml, Hi-Ta, Hi-\n\n\n\nTe) to 5% points (Hi-Or). It should be noted that this is the only combination where no degradation in\nperformance was observed for any language pair and hence, is reported in Table 3. Any other combina-\ntion (MUSE + VecMap, MUSE + XLM-R, MUSE + PVS, and so on) degrades the performance of the\ncognate detection task, on at least one language pair.\n\nApproaches \/ LP Hi-Pa Hi-Bn Hi-Gu Hi-Mr Hi-Ta Hi-Te Hi-Ml\n\nNMT-BPE Baseline 62.79 28.75 52.17 31.66 13.78 19.18 10.4\nCognate-aware NMT-BPE 65.55 29.43 52.39 32.41 13.85 19.58 11.18\n\nTable 4: Results of the Cognate-aware Neural Machine Translation\nTask, in terms of BLEU scores, for the language pairs (LP) with avail-\nable parallel data.\n\nThe average improvement\nobserved by using our best\nmodel (MUSE + WLS) over\nthe strongest baseline ap-\nproach (Kanojia et al., 2019b)\nis 9% points with the highest\nbeing 18% points (Hi-Ta).\nOver the weakest baseline\napproach (WLS), our best\nmodel obtains an average improvement of 50%, peaking at 61% points (Hi-Or).\n\nWe present the results of Cognate-aware NMT in Table 4. For the Hi-Pa language pair, an improve-\nment of 2.76 BLEU is observed, where 15001 cognates were detected including the misclassified pairs.\nAmongst a consistent improvement for all the language pairs, even when 930 cognate pairs (Hi-Te) are\nadded, an improvement of 0.4 BLEU can be seen. The maximum number of cognate pairs injected into\nthe NMT pipeline is 15834 pairs for the Hi-Mr language pair. Surprisingly, we do not observe the most\nsignificant improvement for Hi-Mr despite the largest number of cognates injected. We believe that this\nis because Marathi is a morphologically rich language which exhibits agglutination.\n\n6 Conclusion and Future Work\n\nIn this paper, we harness cross-lingual embeddings to improve the task of cognate detection for thirteen\nIndian language pairs. We propose the use of a linked knowledge graph to augment a publicly released\ncognate dataset with a context dictionary. We reproduce the proposed approach and add two additional\nlanguage pairs to the same dataset and perform experiments using various approaches for a comparative\nevaluation. We reproduce the previously proposed approaches (Rama, 2016; Kanojia et al., 2019b) for\nthis task to perform a further evaluation. We obtain monolingual Indian language corpora for all the\nfourteen languages (Section 3), from various sources to build monolingual models and use a bilingual\ndictionary to supervise the task of cross-lingual models generation (MUSE and VecMap), for thirteen\nlanguage pairs (Hi-Mr, Hi-Ta and so on). We also train a single cross-lingual model using the contextual\nembedding based approach (XLM-R).\n\nOur experiments use three different approaches to generate better feature representations for the cog-\nnate detection task, and all of them show improvements over previously proposed approaches. We ob-\nserve consistent improvements in terms of precision, recall and F-scores. We also perform an ablation\nstudy and show that augmenting WLS baseline feature with MUSE based features provide us with the\nbest results. Over the strongest baseline, this model shows improvements up to 18% points, in terms of\nF-score. Our best F-score is observed for the Hi-Gu and Hi-Ne language pairs (0.84) which can still be\nimproved and warrants further investigation into the task. Additionally, we use the detected cognate pairs\nand use a simple approach to inject them into the neural machine translation pipeline. Our Cognate-aware\nNMT-BPE results also show a consistent improvement for all the Indian language pairs. Furthermore,\nwe release this augmented dataset, along with our code and cross-lingual models for further research.\n\nIn future, we aim further to investigate the performance of contextual embeddings for this task. Recent\ntrends show that contextual embeddings based models outperform conventional word embeddings for\nmost tasks. We, however, do not observe this and attribute this primarily due to the dataset size used\nto train the contextual embeddings. We shall add more data to our monolingual corpora and perform\nmore experiments using XLM-R. Future experiments with cognate-aware NMT using the Transformer\narchitecture (Vaswani et al., 2017) should further help in showing the importance of our extracted cognate\npairs. We also aim to investigate the task of cognate detection for other Indian language pairs, along with\nIndo-European language pairs, in the near future.\n\n\n\nAcknowledgements\n\nWe thank the lexicographers and annotators at the CFILT Lab, IIT Bombay for their efforts in creating\nthe dataset for this study. We acknowledge the computational resources provided by NLP Lab at Monash\nUniversity, and CFILT, IIT Bombay for performing the experiments. We also thank all the reviewers for\ntheir critique, which helped us shape up the article.\n\nReferences\nYaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David\n\nPurdy, Noah A Smith, and David Yarowsky. 1999. Statistical machine translation. In Final Report, JHU\nSummer Workshop, volume 30.\n\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017. Learning bilingual word embeddings with (almost) no\nbilingual data. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 451\u2013462, Vancouver, Canada, July. Association for Computational Linguistics.\n\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 789\u2013798.\n\nPhilip Arthur, Graham Neubig, and Satoshi Nakamura. 2016. Incorporating discrete translation lexicons into\nneural machine translation. arXiv preprint arXiv:1606.02006.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to\nalign and translate. cite arxiv:1409.0473Comment: Accepted at ICLR 2015 as oral presentation.\n\nAditya Bhargava and Grzegorz Kondrak. 2009. Multiple word alignment with profile hidden markov models. In\nProceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of\nthe Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral\nConsortium, pages 43\u201348. Association for Computational Linguistics.\n\nPushpak Bhattacharyya. 2017. Indowordnet. In The WordNet in Indian Languages, pages 1\u201318. Springer.\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146.\n\nOndrej Bojar, Vojtech Diatka, Pavel Rychly\u0300, Pavel Strana\u0301k, V\u0131\u0301t Suchomel, Ales Tamchyna, and Daniel Zeman.\n2014. Hindencorp-hindi-english and hindi-only corpus for machine translation. In LREC, pages 3550\u20133555.\n\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario\nGuajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder. arXiv preprint\narXiv:1803.11175.\n\nAlina Maria Ciobanu and Liviu P Dinu. 2014. Automatic detection of cognates using orthographic alignment.\nIn Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers), volume 2, pages 99\u2013105.\n\nAlina Maria Ciobanu and Liviu P Dinu. 2015. Automatic discrimination between cognates and borrowings. In\nProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Processing (Volume 2: Short Papers), volume 2, pages 431\u2013437.\n\nAlexis Conneau, Guillaume Lample, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herve\u0301 Je\u0301gou. 2017. Word\ntranslation without parallel data. arXiv preprint arXiv:1710.04087.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzma\u0301n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-\nlingual representation learning at scale. arXiv preprint arXiv:1911.02116.\n\nDavid Crystal. 2011. A dictionary of linguistics and phonetics, volume 30. John Wiley & Sons.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n\nTon Dijkstra, Koji Miwa, Bianca Brummelhuis, Maya Sappelli, and Harald Baayen. 2010. How cross-language\nsimilarity and task demands affect cognate recognition. Journal of Memory and language, 62(3):284\u2013301.\n\n\n\nYerai Doval, Jose Camacho-Collados, Luis Espinosa-Anke, and Steven Schockaert. 2018. Improving cross-lingual\nword embeddings by meeting in the middle. arXiv preprint arXiv:1808.08780.\n\nOana Frunza and Diana Inkpen. 2009. Identification and disambiguation of cognates, false friends, and partial\ncognates using machine learning techniques. International Journal of Linguistics, 1(1):1\u201337.\n\nDong Han, Junhui Li, Yachao Li, Min Zhang, and Guodong Zhou. 2019. Explicitly modeling word translations in\nneural machine translation. ACM Transactions on Asian and Low-Resource Language Information Processing\n(TALLIP), 19(1):1\u201317.\n\nSepp Hochreiter and Ju\u0308rgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735\u20131780,\nNovember.\n\nGerhard Ja\u0308ger, Johann-Mattis List, and Pavel Sofroniev. 2017. Using support vector machines and state-of-the-art\nalgorithms for phonetic alignment to identify cognates in multi-lingual wordlists. In Proceedings of the 15th\nConference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers,\nvolume 1, pages 1205\u20131216.\n\nGirish Nath Jha. 2010. The TDIL program and the Indian langauge corpora intitiative (ILCI). In Proceedings of\nthe Seventh International Conference on Language Resources and Evaluation (LREC\u201910), Valletta, Malta, May.\nEuropean Language Resources Association (ELRA).\n\nSittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2010. Integrating joint n-gram features into a\ndiscriminative training framework. In Human Language Technologies: The 2010 Annual Conference of the\nNorth American Chapter of the Association for Computational Linguistics, pages 697\u2013700. Association for\nComputational Linguistics.\n\nDiptesh Kanojia, Malhar Kulkarni, Pushpak Bhattacharyya, and Gholemreza Haffari. 2019a. Cognate identifica-\ntion to improve phylogenetic trees for indian languages. In Proceedings of the ACM India Joint International\nConference on Data Science and Management of Data, pages 297\u2013300. ACM.\n\nDiptesh Kanojia, Kevin Patel, Pushpak Bhattacharyya, Malhar Kulkarni, and Gholemreza Haffari. 2019b. Utiliz-\ning wordnets for cognate detection among indian languages. In Global Wordnet Conference (2019).\n\nDiptesh Kanojia, Malhar Kulkarni, Pushpak Bhattacharyya, and Gholamreza Haffari. 2020. Challenge dataset of\ncognates and false friend pairs from indian languages. In Proceedings of The 12th Language Resources and\nEvaluation Conference, pages 3096\u20133102.\n\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017. Opennmt: Open-source\ntoolkit for neural machine translation. arXiv preprint arXiv:1701.02810.\n\nTeuvo Kohonen. 1978. A very fast associative method for the recognition and correction of misspelt words,\nbased on redundant hash addressing. In Proceedings of the fourth International Joint Conference on Pattern\nRecognition, 1978.\n\nGrzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003. Cognates can improve statistical translation models.\nIn Companion Volume of the Proceedings of HLT-NAACL 2003-Short Papers, pages 46\u201348.\n\nGrzegorz Kondrak. 2000. A new algorithm for the alignment of phonetic sequences. In Proceedings of the 1st\nNorth American chapter of the Association for Computational Linguistics conference, pages 288\u2013295. Associ-\nation for Computational Linguistics.\n\nGrzegorz Kondrak. 2001. Identifying cognates by phonetic and semantic similarity. In Second Meeting of the\nNorth American Chapter of the Association for Computational Linguistics.\n\nGrzegorz Kondrak. 2005. Cognates and word alignment in bitexts. Proceedings of the tenth machine translation\nsummit (mt summit x), pages 305\u2013312.\n\nGuillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. Advances in Neural\nInformation Processing Systems (NeurIPS).\n\nJohann-Mattis List. 2012. Lexstat: Automatic detection of cognates in multilingual wordlists. In Proceedings\nof the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 117\u2013125. Association for Computational\nLinguistics.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv\npreprint arXiv:1907.11692.\n\n\n\nRanbeer Makin, Nikita Pandey, Prasad Pingali, and Vasudeva Varma. 2008. Experiments in cross-lingual ir among\nindian languages. Advances in Multilingual and Multimodal Information Retrieval. Springer Berlin\/Heidelberg.\n\nGideon S Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In Pro-\nceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics\non Language technologies, pages 1\u20138. Association for Computational Linguistics.\n\nI Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107\u2013\n130.\n\nHelen M Meng, Wai-Kit Lo, Berlin Chen, and Karen Tang. 2001. Generating phonetic cognates to handle named\nentities in english-chinese cross-language spoken document retrieval. In IEEE Workshop on Automatic Speech\nRecognition and Understanding, 2001. ASRU\u201901., pages 311\u2013314. IEEE.\n\nPaola Merlo and Maria Andueza Rodriguez. 2019. Cross-lingual word embeddings and the structure of the\nhuman bilingual lexicon. In Proceedings of the 23rd Conference on Computational Natural Language Learning\n(CoNLL), pages 110\u2013120, Hong Kong, China, November. Association for Computational Linguistics.\n\nAndrea Mulloni and Viktor Pekar. 2006. Automatic detection of orthographics cues for cognate recognition. In\nLREC, pages 2387\u20132390.\n\nJohn Nerbonne and Wilbert Heeringa. 1997. Measuring dialect distance phonetically. In Computational Phonol-\nogy: Third Meeting of the ACL Special Interest Group in Computational Phonology.\n\nOlumide Owolabi and DR McGregor. 1988. Fast approximate string matching. Software: Practice and Experi-\nence, 18(4):387\u2013393.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics,\npages 311\u2013318. Association for Computational Linguistics.\n\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.\n\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001, Florence, Italy,\nJuly. Association for Computational Linguistics.\n\nTaraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard Ja\u0308ger. 2018. Are automatic methods for\ncognate detection good enough for phylogenetic reconstruction in historical linguistics? arXiv preprint\narXiv:1804.05416.\n\nTaraka Rama. 2016. Siamese convolutional networks for cognate identification. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational Linguistics: Technical Papers, pages 1018\u20131027.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword\nunits. arXiv preprint arXiv:1508.07909.\n\nClaude E Shannon. 1948. A mathematical theory of communication. The Bell system technical journal,\n27(3):379\u2013423.\n\nJorg Tiedemann. 1999. Automatic construction of weighted string similarity measures. In 1999 Joint SIGDAT\nConference on Empirical Methods in Natural Language Processing and Very Large Corpora.\n\nAna Uban, Alina Maria Ciobanu, and Liviu P. Dinu. 2019. Studying laws of semantic divergence across languages\nusing cognate sets. In Proceedings of the 1st International Workshop on Computational Approaches to Histori-\ncal Language Change, pages 161\u2013166, Florence, Italy, August. Association for Computational Linguistics.\n\nRaghavendra Udupa, K Saravanan, Anton Bakalov, and Abhijit Bhole. 2009. \u201cthey are out there, if you know\nwhere to look\u201d: Mining transliterations of oov query terms for cross-language information retrieval. In Euro-\npean Conference on Information Retrieval, pages 437\u2013448. Springer.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems, NIPS\u201917, page 6000\u20136010, Red Hook, NY, USA. Curran Associates\nInc.\n\n\n\t1 Introduction\n\t2 Related Work\n\t3 Dataset and Experimental Setup\n\t3.1 Unicode Offset based Transliteration\n\t3.2 Cross-lingual Word Embedding Methodologies\n\n\t4 Approaches\n\t4.1 Weighted Lexical Similarity (WLS)\n\t4.2 Phonetic Vectors and Similarity (PVS)\n\t4.3 Cross-lingual Vectors & Similarity\n\t4.4 Classification Methodology\n\t4.5 Cognate-aware Neural Machine Translation (NMT) Task\n\n\t5 Results and Discussion\n\t6 Conclusion and Future Work\n\n","5":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCognition-aware Cognate Detection\n\nDiptesh Kanojia\u2020,\u2663,?, Prashant Sharma\ufffd, Sayali Ghodekar\u2021,\nPushpak Bhattacharyya\u2020, Gholamreza Haffari?, and Malhar Kulkarni\u2020\n\n\u2020IIT Bombay, India, \ufffdHitachi CRL, Japan, \u2021RingCentral, India\n\u2663IITB-Monash Research Academy, India\n\n?Monash University, Australia\n\u2020{diptesh,pb,malhar}@iitb.ac.in, \ufffdprashaantsharmaa@gmail.com\n\u2021sayalighodekar26@gmail.com, ?gholamreza.haffari@monash.edu\n\nAbstract\n\nAutomatic detection of cognates helps down-\nstream NLP tasks of Machine Translation,\nCross-lingual Information Retrieval, Computa-\ntional Phylogenetics and Cross-lingual Named\nEntity Recognition. Previous approaches for\nthe task of cognate detection use orthographic,\nphonetic and semantic similarity based fea-\ntures sets. In this paper, we propose a novel\nmethod for enriching the feature sets, with cog-\nnitive features extracted from human readers\u2019\ngaze behaviour. We collect gaze behaviour\ndata for a small sample of cognates and show\nthat extracted cognitive features help the task\nof cognate detection. However, gaze data col-\nlection and annotation is a costly task. We use\nthe collected gaze behaviour data to predict\ncognitive features for a larger sample and show\nthat predicted cognitive features, also, signifi-\ncantly improve the task performance. We re-\nport improvements of 10% with the collected\ngaze features, and 12% using the predicted\ngaze features, over the previously proposed\napproaches. Furthermore, we release the col-\nlected gaze behaviour data along with our code\nand cross-lingual models.\n\n1 Introduction\n\nCognates are word pairs, across languages, which\nhave a common etymological origin. For example,\nthe French and English word pair, Liberte\u0301 - Liberty,\nreveals itself to be a cognate through orthographic\nsimilarity. The task of automatic cognate detection\nacross languages1 requires one to detect word pairs\nwhich are etymologically related, and carry the\nsame meaning. Such word pairs share a formal\nand\/or semantic affinity and facilitate the second\nlanguage acquisition process, particularly between\nrelated languages. Although they can accelerate\nvocabulary acquisition, language learners also need\n\n1Cognates can also exist in the same language. Such word\npairs\/sets are commonly referred to as doublets.\n\nto be aware of false friends and partial cognates, at\ntimes, leading to unrelated semantic coupling. For\nexample, \u201cgift\u201d in German means \u201cpoison\u201d, which\nis a known example of a False Friend pair. For an\nexample of a partial cognate, the word \u201cpolice\u201d can\ntranslate to \u201cpolice\u201d, \u201cpolicy\u201d or \u201cfont\u201d, in French,\ndepending on the context in which it was used.\n\nManual detection of such cognate sets requires a\nhuman expert with a good linguistic background in\nmultiple languages. Moreover, manual annotation\nof cognate sets is a costly task in terms of time and\nhuman effort. Automatic Cognate Detection (ACD)\nis a well-known task, which has been explored for\na range of languages using different methods; and\nhas shown to help Cross-lingual Information Re-\ntrieval (Meng et al., 2001), Machine Translation\n(MT) (Al-Onaizan et al., 1999), and Computational\nPhylogenetics (Rama et al., 2018). In the tradi-\ntional approaches to automatic cognate detection,\nwords with similar meanings or forms are used as\nprobable cognates (Ja\u0308ger et al., 2017; Rama, 2016;\nKondrak, 2001). From such pairs, the ones that ex-\nhibit a high phonological, lexical and\/or semantic\nsimilarity, are analyzed in order to find true cog-\nnates. Merlo and Andueza Rodriguez (2019) per-\nform an investigation to evaluate the use of cross-\nlingual embeddings and show that these models\ninherit a lexical structure matching the bilingual\nlexicon. This study establishes that cross-lingual\nmodels can provide an effective similarity score\ncompared to their monolingual counterparts, for\nboth cognates and false friends. However, they do\nnot evaluate machine learning (ML) approaches\nthat distinguish between cognates and false friends.\nThe absence of such an evaluation motivates us to\nuse cross-lingual similarity scores with ML algo-\nrithms. Our work2 reports whether these scores can\nprovide an adequate distinction or not.\n\n2Dataset and Code\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n08\n08\n\n7v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n5 \nD\n\nec\n 2\n\n02\n1\n\nhttp:\/\/www.cfilt.iitb.ac.in\/eacl2021diptesh\n\n\nHindi (Hi) Marathi (Mr) Hindi Meaning Marathi Meaning\n\nCognate ank ank Number Number\nFalse Friend shikshA shikshA Education Punishment\n\nTable 1: An example each of a cognate and a false\nfriend pair from Indian languages, Hindi and Marathi.\n\nInspired from their work, we investigate the use\nof cross-lingual word embeddings and cognitive\nfeatures to distinguish between cognates and false\nfriends, for the Indian language pair of Hindi and\nMarathi. Cognitively inspired features have shown\nto improve various NLP tasks (Mishra et al., 2018a).\nHowever, most of their work involves collecting\nthe gaze behaviour data first on a large sample,\nand then splitting the data into training and testing\ndata, before performing their experiments. While\ntheir work does show significant improvements\nover baseline approaches, across multiple NLP\ntasks, collecting gaze behaviour over a large cog-\nnate dataset can be costly, both in terms of time and\nmoney. Our approach tries to reduce annotation\ncost by predicting gaze behaviour data for a large\nsample based on the smaller sample of collected\ngaze data. Our investigations use three recently\nproposed cross-lingual word embeddings based ap-\nproaches to generate features for the task of cognate\ndetection. We also generate cognitive features from\nparticipants\u2019 gaze behaviour data and show that\ngaze behaviour helps the task of cognate detection.\nAdditionally, we use the collected gaze behaviour\ndata and predict gaze-based features for a much\nlarger sample. We believe that using gaze features\nwill be more applicable only if gaze features can\nbe predicted for unseen samples. We believe that\ncollection of gaze data cannot be performed in all\nthe scenarios and hence hypothesize that predicting\nsuch data if it helps improve the task of cognate\ndetection, should be a viable solution.\n\nMotivation\n\nConsider a scenario where an NLP task comes\nacross the false friend pair in Table 1. Orthographic\nsimilarity or even phonetic similarity-based tech-\nniques will fail to detect the difference between\nthe Hindi meaning of the word \u201cshikhshA\u201d and its\nMarathi counterpart. Here, semantic approaches\nshould detect the distinction in meaning, but\nmonolingual embeddings are trained using a\nlarge corpus from the same language. In such\ncases, it becomes imperative that a cross-lingual\nword embeddings model be utilized. However,\nIndian languages are known to be low-resource\n\nlanguages compared to English or even many\nEuropean languages like French, Italian, German\netc. Acquiring additional clean data for training\ncross-lingual models is, yet again, a painful task.\nIn such a scenario then, we ask ourselves,\n\n\u201cCan cognitive features be used to help the\ntask of Cognate Detection?\u201d\n\nfurthermore,\n\n\u201cUsing gaze features collected on a small set\nof data points, can we predict the same features on\na larger set of data points to alleviate the need for\ncollecting gaze data?\u201d\n\nWith this work, we try to answer both the ques-\ntions stated above and present the rest of the paper\nas follows. We discuss the current literature on the\ncognate detection task and cognitive NLP in Sec-\ntion 2. Section 3 discusses the dataset acquisition,\nincluding the description and analysis of our gaze\nbehaviour data. We describe the feature sets used\nin section 4. Our approaches to the task of cognate\ndetection are discussed in Section 5. The results\nof our work are discussed in Section 6. Finally,\nwe conclude the study with possible future work in\nthis direction in Section 7.\n\nTerminology\n\nAn interest area (IA) is an area of the annotation\nscreen to be processed by the human reader. In our\nexperiments, it is an area where a word is shown to\nthe reader. A fixation is an event where the reader\nfocuses within an \u201cinterest area\u201d. A saccade is the\nmovement of the eye from one fixation point to\nanother. If the saccades move from an earlier IA to\na later IA, such a saccade is called a progression.\nA regression is the saccade path when the reader\nmoves back to a previous IA. We also use the terms\nreader and participant interchangeably.\n\n2 Related Work\n\nCurrent literature which uses gaze behaviour to\nsolve downstream NLP tasks has been applied to\nthe NLP tasks of sentiment analysis (Mishra et al.,\n2018a; Barrett et al., 2018; Long et al., 2019), sar-\ncasm detection (Mishra et al., 2016), grammat-\nical error detection (Barrett et al., 2018), hate\nspeech detection (Barrett et al., 2018), named entity\nrecognition (Hollenstein and Zhang, 2019), part-\n\n\n\nof-speech tagging (Barrett et al., 2016), sentence\nsimplification (Klerke et al., 2016), and readabil-\nity (Gonza\u0301lez-Gardun\u0303o and S\u00f8gaard, 2018; Singh\net al., 2016). A comprehensive overview is pro-\nvided by Mishra and Bhattacharyya (2018). The\nprimary motivation of using cognitive features for\nNLP tasks is derived from the eye-mind hypothe-\nsis (Just and Carpenter, 1980), which establishes\na direct correlation between a reader\u2019s comprehen-\nsion of the text with the time taken to read the text.\nThis hypothesis has initiated a large body of psy-\ncholinguistic research that shows a relationship be-\ntween text processing and gaze behaviour. Yaneva\net al. (2020) discuss the use of gaze features for the\ntask of anaphora resolution. Their findings show\nthat gaze data can substitute the classical text pro-\ncessing approaches along with the fact that human\ndisambiguation process overlaps with the informa-\ntion carried in linguistic features. Rohanian (2017)\nuse gaze data to automatically identify multiword\nexpressions and observe that gaze features help im-\nprove the accuracy of the task when combined with\ntraditional linguistic features used for the task.\n\nMathias et al. (2020) describe an approach\nto scoring essays in a multi-task learning frame-\nwork automatically. Their approach relies on col-\nlecting gaze behaviour for essay reading for a\nsmall set of essays and then predicting the rest\nof the dataset\u2019s gaze behaviour in a multi-task\nlearning setup. Similarly, Barrett et al. (2016)\nuse token level averages of cognitive features at\nrun time, to mitigate the need for these features\nat run time. Singh et al. (2016) and Long et al.\n(2019) predict gaze behaviour at the token-level as\nwell. Mishra et al. (2018a), Gonza\u0301lez-Gardun\u0303o and\nS\u00f8gaard (2018), Barrett et al. (2018), and Klerke\net al. (2016), use multi-task learning to solve the pri-\nmary NLP task, where learning the gaze behaviour\nis an auxiliary task.\n\nOrthographic\/String similarity-based methods\nare often used as baseline methods for the cog-\nnate detection task, and the most commonly used\nmethod amongst them is the Edit distance-based\nsimilarity measure (Melamed, 1999; Mulloni and\nPekar, 2006). Research in automatic cognate de-\ntection using various aspects involves the compu-\ntation of similarity by decomposing phonetically\ntranscribed words (Kondrak, 2000; Dellert, 2018),\nacoustic models (Mielke et al., 2012), clustering\nbased on semantic equivalence (Hauer and Kon-\ndrak, 2011), and aligned segments of transcribed\n\nphonemes (List, 2012). Rama (2016) employs a\nSiamese convolutional neural network to learn the\nphonetic features jointly with language related-\nness for cognate identification. Ja\u0308ger et al. (2017)\nuse SVM for phonetic alignment and perform cog-\nnate detection for various language families. Vari-\nous works on orthographic cognate detection usu-\nally take alignment of substrings within classi-\nfiers like SVM (Ciobanu and Dinu, 2014, 2015)\nor HMM (Bhargava and Kondrak, 2009). Ciobanu\nand Dinu (2014) employ dynamic programming\nbased methods for sequence alignment. Cognate\nfacilitation in second language learners has previ-\nously been explored with the help of eye-tracking\nstudies (Blumenfeld and Marian, 2005; Van Assche\net al., 2009; Bosma and Nota, 2020) but the task\nof cognate detection has not been performed with\ngaze features obtained from the cognitive data in\nany of the previously available literature.\n\nFor the task of cognate detection, however, the\nuse of cognitive features has not been established\npreviously. The task of cognate detection is cross-\nlingual, and a reader\u2019s cognitive load should vary\nwhile trying to comprehend the meaning of con-\ncepts, from different languages. Our work tries\nto exploit the difference noted in terms of time\ntaken and eye-movement patterns in cognates vs\nfalse friends, to generate additional features for the\nACD task. Moreover, for Indian languages such\nas Marathi, where agglutination3 varies the word\nlength, the task becomes tougher, computationally.\n\n3 Dataset Acquisition & Analysis\n\nWe pose the problem of cognate detection as a bi-\nnary classification task in a supervised setting. We\nuse a recently released challenging dataset (Kano-\njia et al., 2020) of cognates and false friends. We\nextract the Hindi-Marathi cognate and false friend\npairs. The number of cognate and false friend pairs\nreleased by the paper cited above is 15726, and\n5826. We select an equal number of cognates at\nrandom to reduce this skew, thus producing a bal-\nanced dataset for the classification task. For any\nfurther experiments in our paper, we use this artifi-\ncially class-balanced dataset of 5826 (cognates) +\n5826 (false friends) data points. We also augment\nthe complete dataset with context information from\nthe IndoWordnet (Bhattacharyya, 2017). The con-\n\n3Agglutination is a linguistic process pertaining to deriva-\ntional morphology in which complex words are formed by\nstringing together morphemes without changing them in\nspelling or phonetics.\n\n\n\nCognates (1) False Friends (0)\n\nKanojia et. al. (2020) 15726 5826\nD1 5826 5826\nD2 100 100\n\nTable 2: Dataset Statistics for Cognate Detection Task\n\ntext information contains a gloss and an example\nsentence from the Wordnet data. The dataset re-\nleased by Kanojia et al. (2020) contains the Synset\nIDs for each word pair, which helps us locate exact\nconcept information from the dataset. We provide\npositive labels to cognates and negative labels to\nfalse friend pairs obtained from this data and con-\nstruct what we call \u201cD1\u201d.\n\nWe extract 100 pairs, at random, from each of\nthe positive and negative labels for collecting gaze\nbehaviour data, to construct what we call \u201cD2\u201d.\nThis data, extracted from D1, is used to collect\ngaze behaviour and annotation. Although we have\ngold labels for the data extracted, we ask the par-\nticipants to annotate the data by asking them if the\nconcepts shown on the screen mean the same. The\nannotation screen provides them with contextual\nclues obtained from Wordnet data on the screen, as\nshown in Figure 1. The complete dataset statistics\nare shown in Table 2.\n\n3.1 Gaze Data Collection and Annotation\n\nThe task assigned to annotators was to read word\npairs and the contextual clues provided on the\nscreen, one pair at a time. The annotators were\nrequested to label the pairs with a binary score\nindicating the similarity in meaning (i.e., posi-\ntive\/negative). It should be noted that the partic-\nipants were not instructed to annotate whether a\npair is a cognate or a false friend, to rule out the\nPriming Effect, (i.e., if the exact task (cognates vs\nfalse friends) is expected beforehand, processing\ncognate pairs will become relatively easier (Sa\u0301chez-\nCasas et al., 1992)). This ensures the ecological\nvalidity of our experiment in two ways: (1) The\nparticipant does not have any clue so that they can\ntreat cognates with special attention (done by ask-\ning them to annotate based solely on meaning simi-\nlarity) (2) Cognate pairs are mixed with false friend\npairs and the participant does not have any prior\nknowledge about whether the next word pair would\nbe a cognate or not. This also ensures that the\nparticipants pay attention to the task and do not\njust skim through the word-pair presented on the\nscreen.\n\n\u00b5 Pos \u03c3 Pos \u00b5 Neg \u03c3 Neg p\n\nP1 9.720 17.867 8.677 4.281 0.028\nP2 8.596 10.526 7.619 13.794 0.049\nP3 7.770 6.664 7.044 3.900 0.027\nP4 9.686 17.729 8.664 4.306 0.031\nP5 8.861 8.611 8.099 5.246 0.042\nP6 7.854 6.286 7.184 3.442 0.033\nP7 8.564 5.499 7.918 3.540 0.033\nP8 8.018 5.955 7.340 3.742 0.031\nP9 9.720 17.867 8.703 4.305 0.028\n\nTable 3: T-test statistics for average fixation duration\ntime per word for Positive labels (Cognates) and Nega-\ntive labels (False Friends) for participants P1-P9.\n\nIt should be noted that all our participants are\nprimarily Marathi speakers and have learnt Hindi at\nthe school level. Hindi and Marathi are considered\nto be relatively closer languages due to their shared\nvocabulary and the geographical location of the de-\nmographic. All the participants speak, understand\nand write - both Hindi and Marathi.\n\nThe collection of gaze data is conducted by fol-\nlowing the standard norms in eye-movement re-\nsearch (Holmqvist et al., 2011). While reading,\nan SR-Research Eyelink-1000 eye-tracker (monoc-\nular remote mode, sampling rate 500Hz) records\nseveral eye-movement parameters like fixations (a\nlong stay of gaze) and saccade (quick jumping of\ngaze between two positions of rest) and pupil size.\nFor this experiment, the default value of 4ms was\nused for a gaze to be counted as fixation. We re-\nquest a total of 15 participants to perform the anno-\ntation task, out of which only 11 participants could\nperform the data collection4.\n\nOut of the 11 completed annotations, we dis-\ncarded the data from 2 participants as their gaze\nbehaviour was erratic (the fixations were too far\naway from the IAs). The participants are graduates\nwith science and engineering background. They\nare bilingual speakers who know both Hindi and\nMarathi. Our participants were given a set of in-\nstructions beforehand and were advised to seek\nclarifications before they proceed. The instructions\nmention the nature of the task as discussed above,\nannotation input method, and the necessity of head\nmovement minimization during the experiment.\n\n4We could not perform gaze data collection with the re-\nmaining 4 participants due to the COVID-19 pandemic.\n\n\n\nFigure 1: Screen capture showing collection of gaze features (via eye tracking) while displaying word pairs along\nwith respective definitions and examples. The figure shows the cognate pair (BiDha - BiDha) where both mean\n\u201cpierced\u201d in the context of hunting. The figure also shown the glosses and example sentences provided to the\nannotator for this cognate pair in their respective languages (Hindi and Marathi)\n\n3.2 Gaze Behaviour Data Analysis\n\nThe accuracy of similarity annotation by partici-\npants lies between 98% to 99.5% for individual\nannotators. Out of the 1800 annotations (9 annota-\ntors over 200 word-pairs), only 40 were predicted\nincorrectly. Annotation errors may be attributed\nto: (a) lack of patience\/attention while reading, (b)\nissues related to word-pair comprehension, and (c)\nconfusion\/indecisiveness caused due to lack of con-\ntextual clues. In our analysis, we do not discard the\ndata obtained from these incorrect annotations.\n\nWe observe distinct eye-movement patterns for\ncognate pairs in terms of fixation duration of the\nhuman readers. Our analysis shows that fixation\nduration normalized over word count is relatively\nlarger for cognate pairs. For cognate pairs, we\nobserve that average fixation duration amongst all\nparticipants is 1.3 times more than that of false\nfriend pairs. To test the statistical significance,\nwe conduct a two-tailed t-test (assuming unequal\nvariance) to compare the average fixation duration\nper word for cognate and false friend pairs. The\nhypothesized mean difference is set to 0, and the\nerror tolerance limit (\u03b1) is set to 0.05. The t-test\nanalysis, presented in Table 3, shows that for all\nparticipants, a statistically significant difference\nexists between the average fixation duration per\nword for cognate pairs vs false friend pairs.\n\nWe believe this difference in average fixation du-\nration is because the bilingual speakers who partic-\nipated in the experiment can clearly distinguish be-\ntween cognates and false friends and decide quickly\nin either case. The duration for which they fixate\non either of the cases differs significantly for each\nparticipant. As per our observation, the participants\n\ntake more time over cognate pairs to ensure similar-\nity in meaning. Given their knowledge of both the\nlanguages and contextual clues, they were \u2019quickly\u2019\nable to decide these word pairs did not mean the\nsame. It should be noted that they were unaware\nof the \u2019cognate\u2019 or \u2019false friend\u2019 distinction con-\ncerning the experiment. They were simply asked to\nnote the meaning of both the words given context,\nand annotate accordingly, as described in the paper.\n\n3.3 Cross-lingual Word Embeddings\n\nFor this task, we use the cross-lingual word em-\nbedding models released by Kumar et al. (2020).\nThe Hindi-Marathi cross-lingual models released\nwith this paper are based on both MUSE (Conneau\net al., 2017) and XLM5 (Lample and Conneau,\n2019). Additionally, we build the cross-lingual\nword embeddings model for Hindi-Marathi using\nVecMap (Artetxe et al., 2017). The model uses\nmonolingual corpora released by Kunchukuttan\net al. (2020) and a bilingual dictionary6 required\nfor the supervised method by Artetxe et al. (2017).\nThese three cross-lingual models provide us with\nthree feature sets for the task of cognate detection.\n\n4 Feature Sets for Cognate Detection\n\nIn this section, we discuss the various features used\nfor the task of cognate detection. It is to be noted\nthat false friends are spelt similarly across lan-\nguages but mean differently. Using false friends\nas data points with negative labels restricts us\nto the use of semantic similarity based features,\n\n5Word representations extracted from the last layer of the\ncontextual XLM model.\n\n6Bilingual Lexicon\n\nhttp:\/\/www.cfilt.iitb.ac.in\/Downloads.html\n\n\nas orthographic or phonetic similarity-based mea-\nsures would fail to detect sufficient distinction be-\ntween them. Hence, we use the features proposed\nby Rama (2016) and Kanojia et al. (2019a) as base-\nline features for a comparative evaluation.\n\n4.1 Phonetic Features\nThe IndicNLP Library provides phonetic features\nbased vector for each character in various Indian\nlanguage scripts. We utilize this library to compute\na feature vector for each word by computing an\naverage over character vectors. We compute vec-\ntors for both words in the candidate cognate pairs\n(PVS and PVT ) and also compute contextual vec-\ntors (PCVS and PCVT ) by averaging the vectors\nfor all the contextual clues, generating a total of\nfour vectors. We use these vectors as features for\ncomputing the baseline scores using the Siamese\nConvolutional Neural Network architecture pro-\nposed by Rama (2016).\n\n4.2 Weighted Lexical Similarity (WLS)\nThe Normalized Edit Distance (NED) approach\ncomputes the edit distance (Nerbonne and\nHeeringa, 1997) for all word pairs in our dataset. A\ncombination of NED with q-gram distance (Shan-\nnon, 1948) for a better similarity score. The\nq-grams (\u2018n-grams\u2019) are simply substrings of\nlength q. This distance measure has been ap-\nplied previously for various spelling correction ap-\nproaches (Owolabi and McGregor, 1988; Kohonen,\n1978). Kanojia et al. (2019b) proposed Weighted\nLexical Similarity (WLS) and we use it with the\ncharacter-based Recurrent Neural Network archi-\ntecture proposed by them to compute another set\nof baseline scores.\n\n4.3 Cross-lingual Vectors & Similarity\nAs discussed above, we use the pre-trained cross-\nlingual embedding models for generating feature\nvectors for MUSE and XLM based approaches.\nThese models are generated by aligning two dis-\njoint monolingual vector spaces through linear\ntransformations, using a small bilingual dictionary\nfor supervision (Doval et al., 2018; Artetxe et al.,\n2017). Additionally, the cross-lingual embeddings\nmodel trained using Artetxe et al. (2017)\u2019s ap-\nproach provides us with the third set of feature\nvectors.\n\nWe use these models to obtain vectors for word-\npairs (WVS and WVT ) and averaged context vec-\ntors (CVS and CVT ) from the contextual clues,\n\nto create three different feature sets. We obtain\nvectors for each candidate pair and their context us-\ning all the three cross-lingual methodologies. The\nuse of cross-lingual models has been proposed for\ndifferentiating between cognates and false friends\nby Merlo and Andueza Rodriguez (2019), but eval-\nuation with the cognate detection task had not been\nperformed. We perform this evaluation on our\ndatasets (D1, D2 and D1+D2) using various classi-\nfication methods and discuss them later.\n\n4.4 Cognitive Features from Gaze Data\nGaze behaviour of participants, characterized by\nfixations, forward saccades, skips and regressions,\ncan be used as features for NLP tasks (Mishra et al.,\n2018b). Since these gaze features relate to the\ncognitive process in reading (Altmann, 1994), we\nconsider these as features in our model. The current\ngaze data extraction adds up all the interest areas\non the screen (word + context). The software used\nto analyze the gaze data, currently, provides us with\ncollated results for all the gaze-based features.\n\nFrom the gaze behaviour data collected, we ex-\ntract a total of 18 features for each of the 1800\ndata points. Using supervised feature selection ap-\nproach, we are able to select eight best features via\ngrid search using Logistic Regression. We use the\nSelectKBest implementation along with hyperpa-\nrameter tuning via GridSearchCV, present in the\nsklearn (Pedregosa et al., 2011) library. Here on-\nwards, we refer to these eight features when dis-\ncussing cognitive or gaze-based features in this\npaper. These eight best features, along with their\ndescription, are listed in Table 4.\n\n4.5 Gaze Feature Prediction\nCollection of gaze data for a large number of sam-\nples can be a costly task. We propose a neural\nmodel for cognitive features prediction. Our neural\nmodel is a feed-forward neural network to perform\na regression task and predict gaze features. We\ncollect gaze data for only 200 word-pairs (D2) with\nthe help of 9 annotators which provides us with a\ntotal of 1800 data points for training and validation.\nAs reported in Table 5, the initial results on D1\nusing different cross-lingual embeddings show that\nXLM based contextual features perform the best\namongst all the cross-lingual models.\n\nAs an input to the network, we provide the fea-\nture vectors from the XLM model. This network\u2019s\noutput is the predicted gaze features for the D1\ndataset, using the gaze features as gold predictions\n\n\n\nGaze Feature Description\nAverage Fixation Duration The average of all fixation duration across all interest areas present on the screen.\n\nAverage Saccade Amplitude Saccade amplitude is the amplitude of going back and forth measured in terms of duration.\nFixation Count Counting the number of times user\u2019s eyes are fixated on the screen.\n\nFixation Duration Max Maximum time for a single fixation on any Interest Area.\nFixation Duration Min Minimum time for a single fixation on any Interest Area.\n\nIA Count Interest Area Count (no. of IAs on the screen)\nRun Count Consecutive counts for same Interest Area are ignored in Run Count\n\nSaccade Count Total counts of Saccades\n\nTable 4: Gaze Features used for the task of Cognate Detection\n\nFigure 2: Predicted feature values ( blue ) vs. Gold fea-\nture values ( orange ) for the average fixation duration\nfeature, on 100 samples.\n\nfrom the D2 dataset. This network contains three\nlinear hidden layers with 128, 64 and 32 dimen-\nsions. After each layer, we use the sigmoid activa-\ntion function and dropout after each sigmoid with\na dropout value of 0.2. We use 0.1 as the learn-\ning rate and use the Mean Squared Error (MSE)\nloss function. A graph comparing the values for\nthe predictions vs the actual values for the average\nsaccade amplitude for 100 samples, can be seen in\nFigure 2.\n\n5 The Cognate Detection Task\n\nWe employ both classical machine learning-based\nmodels and a simple feed-forward neural network.\nTo compare our work with the previously proposed\napproaches, we replicate the best-reported systems\nfrom Rama (2016) i.e., Siamese Convolutional Neu-\nral Network with phonetic vectors as features and\nalso replicate Kanojia et al. (2019b)\u2019s approach\nwhich uses a Recurrent Neural Network architec-\nture with a weighted lexical similarity (WLS) as the\nfeature set. The input to our classifiers is the feature\nsets described above for each candidate pair.\n\nAmong the classical machine learning models,\nwe use Support Vector Machines (SVM) and Logis-\ntic Regression (LR). We experiment with the use of\nboth linear SVMs and kernel SVMs (Gaussian and\n\nPolynomial). We perform a grid-search to find the\nbest hyper-parameter value for C over the range of\n0.01 to 1000. We deploy the Feed Forward Neu-\nral Network (FFNN) with one hidden layer. We\nperform cross-validation with different activation\nfunction settings (tanh, hardtanh, sigmoid and relu)\nand the hidden layer dimension in the network (30,\n50, 100, and 150). We use binary cross-entropy as\nthe optimization algorithm. Finally, we choose the\nhyper-parameter configuration with the best valida-\ntion accuracy. We train the model with the selected\nconfiguration with an initial learning rate of 0.4,\nand we halve the learning rate when the error on\nthe validation split increases. We stop the train-\ning once the learning rate falls below 0.001. We\nperform 5-fold stratified cross-validation, which\ndivides the data into train and test folds, randomly.\n\nInitially, we perform our experiments with the\nfeature sets from three different cross-lingual em-\nbeddings (MUSE, XLM, and VecMap) for the\ndataset D1, then with the smaller dataset D2 and\nlater on the combined dataset D1+D2. We, then,\nperform the same task for the smaller dataset D2\nby combining cognitive features with individual\ncross-lingual feature sets. We also observe the per-\nformance of standalone gaze features for the D2\ndataset. Finally, we evaluate the predicted gaze fea-\ntures on the combined dataset by combining them\nwith cross-lingual features and a standalone feature\nset, using the feed-forward neural network. We\nreport the results of the cognate detection task in\nthe next section and discuss them in detail.\n\n6 Results and Dicussion\n\nWe report the results of the cognate detection task\nin Table 5. We use the original implementations of\nRama (2016) and Kanojia et al. (2019b) on the com-\nbined (D1+D2) dataset. In our initial evaluation on\nthe D1 dataset, cross-lingual model-based features\n(XLM, MUSE, and VecMap) can be seen to out-\nperform the baseline systems which use phonetic\nand orthographic features. Using the XLM-based\n\n\n\nP R F P R F P R F P R F\n\nFeature Set \u2192 Phonetic WLS\n\nRama et. al., 2016 (D1+D2) 0.71 0.69 0.70 - - -\nKanojia et. al., 2019 (D1+D2) - - - 0.76 0.72 0.74\n\nFeature Set \u2192 XLM MUSE VecMap\n\nLinear SVM (D1+D2) 0.83 0.71 0.77 0.72 0.68 0.70 0.70 0.65 0.67\nLogisticRegression (D1+D2) 0.85 0.74 0.79 0.80 0.71 0.75 0.70 0.66 0.68\n\nFFNN (D1+D2) 0.82 0.84 0.83 0.83 0.79 0.81 0.75 0.76 0.75\n\nFeature Set \u2192 XLM+Gaze MUSE+Gaze VecMap+Gaze Gaze\n\nLinear SVM (D2) 0.81 0.69 0.75 0.72 0.73 0.72 0.70 0.75 0.72 0.77 0.76 0.76\nLogisticRegression (D2) 0.84 0.75 0.79 0.76 0.72 0.74 0.81 0.71 0.76 0.80 0.75 0.77\n\nFFNN (D2) 0.83 0.85 0.84 0.83 0.78 0.80 0.86 0.83 0.84 0.81 0.71 0.76\n\nPredicted Gaze Features On D1 (11652 samples) and Collected Gaze Features on D2 (200 samples)\n\nFeature Set \u2192 XLM+Gaze MUSE+Gaze VecMap+Gaze Gaze\n\nFFNN (D1+D2) 0.84 0.88 0.86 0.85 0.78 0.81 0.83 0.85 0.84 0.77 0.76 0.76\n\nFFNN (D1) [Only Predicted Gaze] 0.83 0.84 0.83 0.82 0.79 0.80 0.80 0.86 0.83 0.76 0.77 0.76\n\nTable 5: Classification results in terms of weighted Precision (P), Recall (R), and F-scores (F) using 5-fold cross-\nvalidation using different feature sets as described above.\n\nfeatures, we observe an improvement of 9% over\nthe stronger baseline (Kanojia et al., 2019b) and\n13% over the system by Rama (2016). It can be\nseen that MUSE and VecMap based features also\nperform better on the combined dataset. In terms\nof both precision and recall, cross-lingual features\nare shown to outperform the baseline systems. The\ncross-lingual approach, with representations from\nVecMap-based model, fails to perform as well as\nMUSE and XLM-based models. The contextual\nXLM model achieves the best scores in almost all\nthe settings. We believe its performance can be at-\ntributed to the linguistic closeness of the language\npair and context from the contextual clues provided.\nFor example, the false friend pair \u201ckaccHa\u201d (mean-\ning inexperienced) - \u201ckaccHa\u201d [raw (food)] is clas-\nsified correctly by XLM but incorrectly by both the\nbaseline models, and VecMap-based classification.\nThis signifies that fine-grained semantic difference\nbetween such false friend pairs can be captured\nvia cognitive features. We report the additional re-\nsults on individual datasets (D1 and D2), for all the\nbaseline and cross-lingual approaches, in Table 6.\n\nFor all the classifiers, the gaze features are aver-\naged across participants and augmented with cross-\nlingual features. The gaze fixation duration collects\nthe total time spent, as fixations, on each interest\narea including the context clues. We were hopeful\nthat the participants would focus only on important\ncontextual clues and not the stop words with our\n\nexperiment design. However, the sample points are\nnot enough to concretely discuss this aspect of our\nstudy. These results are reported for all the classi-\nfiers with D2 dataset. Our feature combinations out-\nperform the baselines with an F-score improvement\nof 10% points over the stronger baseline (WLS).\nWe also report the precision, recall and F-score\nvalues when only gaze features are used to pre-\ndict the labels for our candidate pairs. We observe\nthat standalone gaze features are not as effective as\nwhen combined with cross-lingual feature vectors.\nWhen gaze features are predicted using the method-\nology described in Section 4.5, the model perfor-\nmance for FFNN on D1 remains the same with\nXLM+Gaze, decreases slightly for MUSE+Gaze\nand significantly improves for VecMap+Gaze. We\nobserve that predicted gaze features do not signif-\nicantly drop the performance, and hence, add the\ncollected gaze data samples (D2) to D1.\n\nOn the combined dataset with collected gaze fea-\ntures (on D2) and predicted gaze features (on D1),\nwe report our best system [FFNN (D1+D2)] which\nshown an improvement of 12% over the stronger\nbaseline (WLS), and 16% over the weaker baseline\n(Phonetic). This system also outperforms the best\nreported cross-lingual features-based approach by\n3%, as shown in Table 5.\n\nFor example, the cognate pair \u201cutPaNa\u201d (Hindi) -\n\u201cutpAaDit\u201d (Marathi) (both meaning manufactured)\nis classified correctly by this system, but incorrectly\n\n\n\nPhonetic WLS\n\nP R F P R F P R F\n\nRama et. al., 2016 (D1) 0.70 0.68 0.69 - - -\nKanojia et. al., 2019 (D1) - - - 0.74 0.70 0.72\n\nRama et. al., 2016 (D2) 0.64 0.57 0.60 - - -\nKanojia et. al., 2019 (D2) - - - 0.61 0.66 0.63\n\nXLM MUSE VecMap\n\nLinear SVM (D1) 0.81 0.71 0.76 0.70 0.68 0.69 0.70 0.65 0.67\nLogisticRegression (D1) 0.80 0.75 0.77 0.72 0.74 0.73 0.70 0.73 0.71\n\nFFNN (D1) 0.80 0.84 0.82 0.81 0.76 0.78 0.77 0.76 0.76\n\nLinear SVM (D2) 0.72 0.65 0.68 0.65 0.60 0.62 0.62 0.57 0.59\nLogisticRegression (D2) 0.78 0.69 0.73 0.67 0.67 0.67 0.63 0.61 0.62\n\nFFNN (D2) 0.79 0.81 0.80 0.76 0.71 0.73 0.74 0.71 0.72\n\nTable 6: Additional results in terms of weighted Precision (P), Recall (R), and F-scores (F) using 5-fold cross-\nvalidation using different feature sets as described above. These are additional results on the individual datasets\nD1 and D2 for which the combined results (D1 and D2) are already shown in Table 5 for a fair comparison.\n\nby both the baselines, and all the cross-lingual sys-\ntems. Furthermore, we also show that gaze features\ncan be predicted based on a small sample data, and\nimproved performance can be attained with the\nhelp of cross-lingual features, reported with our\nwork.\n\n7 Conclusion and Future Work\n\nIn this paper, we harness cross-lingual embeddings\nand gaze-based features to improve the task of cog-\nnate detection for the Indian language pair of Hindi-\nMarathi. We create a novel framework that derives\ninsights from human cognition, that manifests over\neye movement patterns. We hypothesize that by\naugmenting cross-lingual features with features ob-\ntained from the gaze data, the task of cognate de-\ntection can be improved. We use a linked knowl-\nedge graph (IndoWordnet) to augment a publicly\nreleased cognate dataset with contextual clues. We\ncollect the gaze behaviour data from nine partic-\nipants over 200 samples and perform the task of\ncognate detection for both our datasets (with gaze\ndata and without gaze data). Then, we use a neural\nnetwork to predict gaze features for unseen samples\nand perform the task of cognate detection to show\nimproved performance, despite a small sample of\ncollected gaze data.\n\nWe reproduce the previously proposed baseline\napproaches and perform experiments using addi-\ntional features obtained via cross-lingual models\n\nfor a comparative evaluation. The previously pro-\nposed approaches (Rama, 2016; Kanojia et al.,\n2019b) for this task are shown to be outperformed\nby cross-lingual features and the combination of\nthese features with the obtained gaze data. Our\nexperiments use three different approaches to gen-\nerate feature representations for the cognate detec-\ntion task, and all of them show improvements over\npreviously proposed approaches. We observe con-\nsistent improvements in terms of precision, recall\nand F-scores. Over the stronger baseline, our best\nsystem shows an improvement of 12% points and\n16% points over the weaker baseline. This system\nalso outperforms the cross-lingual features based\napproaches by 3%, over the combined dataset. We\nrelease this augmented dataset, along with our code\nand cross-lingual models for further research.\n\nIn future, we aim to add more language pairs and\nleverage contextual information from knowledge\ngraphs using sequence-based neural models. We\nalso aim to collect gaze data and then model the\ngaze predictions in a multi-task setting. We plan\nto investigate other multilingual contextual embed-\ndings\u2019 performance for this task (e.g., M-BERT,\nIndicBERT, MuRIL). We also plan to look for a\nmethod to differentiate between different interest\nareas and see if a markup facility is present in the\nsoftware used to analyze the gaze data. We also aim\nto investigate the task of cognate detection for the\nIndo-European language family, in the near future.\n\n\n\nReferences\nYaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin\n\nKnight, John Lafferty, Dan Melamed, Franz-Josef\nOch, David Purdy, Noah A Smith, and David\nYarowsky. 1999. Statistical machine translation. In\nFinal Report, JHU Summer Workshop, volume 30.\n\nGerry TM Altmann. 1994. Regression-contingent anal-\nyses of eye movements during sentence processing:\nReply to rayner and sereno. Memory & Cognition,\n22(3):286\u2013290.\n\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451\u2013462,\nVancouver, Canada. Association for Computational\nLinguistics.\n\nMaria Barrett, Joachim Bingel, Nora Hollenstein,\nMarek Rei, and Anders S\u00f8gaard. 2018. Sequence\nclassification with human attention. In Proceedings\nof the 22nd Conference on Computational Natural\nLanguage Learning, pages 302\u2013312, Brussels, Bel-\ngium. Association for Computational Linguistics.\n\nMaria Barrett, Joachim Bingel, Frank Keller, and An-\nders S\u00f8gaard. 2016. Weakly supervised part-of-\nspeech tagging using eye-tracking data. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 579\u2013584, Berlin, Germany. Association\nfor Computational Linguistics.\n\nAditya Bhargava and Grzegorz Kondrak. 2009. Multi-\nple word alignment with profile hidden markov mod-\nels. In Proceedings of Human Language Technolo-\ngies: The 2009 Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics, Companion Volume: Student Re-\nsearch Workshop and Doctoral Consortium, pages\n43\u201348. Association for Computational Linguistics.\n\nPushpak Bhattacharyya. 2017. Indowordnet. In The\nWordNet in Indian Languages, pages 1\u201318. Springer.\n\nHenrike K Blumenfeld and Viorica Marian. 2005.\nCovert bilingual language activation through cog-\nnate word processing: An eye-tracking study. In\nProceedings of the Annual Meeting of the Cognitive\nScience Society, volume 27.\n\nEvelyn Bosma and Naomi Nota. 2020. Cognate facili-\ntation in frisian\u2013dutch bilingual children\u2019s sentence\nreading: An eye-tracking study. Journal of experi-\nmental child psychology, 189:104699.\n\nAlina Maria Ciobanu and Liviu P Dinu. 2014. Auto-\nmatic detection of cognates using orthographic align-\nment. In Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 2: Short Papers), volume 2, pages 99\u2013105.\n\nAlina Maria Ciobanu and Liviu P Dinu. 2015. Auto-\nmatic discrimination between cognates and borrow-\nings. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural\nLanguage Processing (Volume 2: Short Papers), vol-\nume 2, pages 431\u2013437.\n\nAlexis Conneau, Guillaume Lample, Marc\u2019Aurelio\nRanzato, Ludovic Denoyer, and Herve\u0301 Je\u0301gou. 2017.\nWord translation without parallel data. arXiv\npreprint arXiv:1710.04087.\n\nJohannes Dellert. 2018. Combining information-\nweighted sequence alignment and sound correspon-\ndence models for improved cognate detection. In\nProceedings of the 27th international conference on\ncomputational linguistics, pages 3123\u20133133.\n\nYerai Doval, Jose Camacho-Collados, Luis Espinosa-\nAnke, and Steven Schockaert. 2018. Improving\ncross-lingual word embeddings by meeting in the\nmiddle. arXiv preprint arXiv:1808.08780.\n\nAna V Gonza\u0301lez-Gardun\u0303o and Anders S\u00f8gaard. 2018.\nLearning to predict readability using eye-movement\ndata from natives and learners. In Thirty-Second\nAAAI Conference on Artificial Intelligence.\n\nBradley Hauer and Grzegorz Kondrak. 2011. Cluster-\ning semantically equivalent words into cognate sets\nin multilingual lists. In Proceedings of 5th interna-\ntional joint conference on natural language process-\ning, pages 865\u2013873.\n\nNora Hollenstein and Ce Zhang. 2019. Entity recog-\nnition at first sight: Improving NER with eye move-\nment information. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1\u201310, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n\nKenneth Holmqvist, Marcus Nystro\u0308m, Richard Anders-\nson, Richard Dewhurst, Halszka Jarodzka, and Joost\nVan de Weijer. 2011. Eye tracking: A comprehen-\nsive guide to methods and measures. OUP Oxford.\n\nGerhard Ja\u0308ger, Johann-Mattis List, and Pavel Sofroniev.\n2017. Using support vector machines and state-of-\nthe-art algorithms for phonetic alignment to identify\ncognates in multi-lingual wordlists. In Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 1, Long Papers, volume 1, pages 1205\u20131216.\n\nMarcel A Just and Patricia A Carpenter. 1980. A the-\nory of reading: From eye fixations to comprehension.\nPsychological review, 87(4):329.\n\nDiptesh Kanojia, Malhar Kulkarni, Pushpak Bhat-\ntacharyya, and Gholamreza Haffari. 2020. Chal-\nlenge dataset of cognates and false friend pairs from\nindian languages. In Proceedings of The 12th Lan-\nguage Resources and Evaluation Conference, pages\n3096\u20133102.\n\nhttps:\/\/doi.org\/10.18653\/v1\/P17-1042\nhttps:\/\/doi.org\/10.18653\/v1\/P17-1042\nhttps:\/\/doi.org\/10.18653\/v1\/K18-1030\nhttps:\/\/doi.org\/10.18653\/v1\/K18-1030\nhttps:\/\/doi.org\/10.18653\/v1\/P16-2094\nhttps:\/\/doi.org\/10.18653\/v1\/P16-2094\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1001\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1001\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1001\n\n\nDiptesh Kanojia, Malhar Kulkarni, Pushpak Bhat-\ntacharyya, and Gholemreza Haffari. 2019a. Cognate\nidentification to improve phylogenetic trees for in-\ndian languages. In Proceedings of the ACM India\nJoint International Conference on Data Science and\nManagement of Data, pages 297\u2013300. ACM.\n\nDiptesh Kanojia, Kevin Patel, Pushpak Bhattacharyya,\nMalhar Kulkarni, and Gholemreza Haffari. 2019b.\nUtilizing wordnets for cognate detection among in-\ndian languages. In Global Wordnet Conference\n(2019).\n\nSigrid Klerke, Yoav Goldberg, and Anders S\u00f8gaard.\n2016. Improving sentence compression by learning\nto predict gaze. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1528\u20131533, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\n\nTeuvo Kohonen. 1978. A very fast associative method\nfor the recognition and correction of misspelt words,\nbased on redundant hash addressing. In Proceedings\nof the fourth International Joint Conference on Pat-\ntern Recognition, 1978.\n\nGrzegorz Kondrak. 2000. A new algorithm for the\nalignment of phonetic sequences. In Proceedings\nof the 1st North American chapter of the Asso-\nciation for Computational Linguistics conference,\npages 288\u2013295. Association for Computational Lin-\nguistics.\n\nGrzegorz Kondrak. 2001. Identifying cognates by pho-\nnetic and semantic similarity. In Second Meeting of\nthe North American Chapter of the Association for\nComputational Linguistics.\n\nSaurav Kumar, Saunack Kumar, Diptesh Kanojia, and\nPushpak Bhattacharyya. 2020. \u201ca passage to India\u201d:\nPre-trained word embeddings for Indian languages.\nIn Proceedings of the 1st Joint Workshop on Spoken\nLanguage Technologies for Under-resourced lan-\nguages (SLTU) and Collaboration and Computing\nfor Under-Resourced Languages (CCURL), pages\n352\u2013357, Marseille, France. European Language Re-\nsources association.\n\nAnoop Kunchukuttan, Divyanshu Kakwani, Satish\nGolla, Avik Bhattacharyya, Mitesh M Khapra,\nPratyush Kumar, et al. 2020. Ai4bharat-indicnlp cor-\npus: Monolingual corpora and word embeddings for\nindic languages. arXiv preprint arXiv:2005.00085.\n\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems (NeurIPS).\n\nJohann-Mattis List. 2012. Lexstat: Automatic de-\ntection of cognates in multilingual wordlists. In\nProceedings of the EACL 2012 Joint Workshop of\nLINGVIS & UNCLH, pages 117\u2013125. Association\nfor Computational Linguistics.\n\nYunfei Long, Rong Xiang, Qin Lu, Chu-Ren Huang,\nand Minglei Li. 2019. Improving attention model\nbased on cognition grounded data for sentiment anal-\nysis. IEEE Transactions on Affective Computing.\n\nSandeep Mathias, Rudra Murthy, Diptesh Kanojia,\nAbhijit Mishra, and Pushpak Bhattacharyya. 2020.\nHappy are those who grade without seeing: A multi-\ntask learning approach to grade essays using gaze\nbehaviour. arXiv preprint arXiv:2005.12078.\n\nI Dan Melamed. 1999. Bitext maps and alignment\nvia pattern recognition. Computational Linguistics,\n25(1):107\u2013130.\n\nHelen M Meng, Wai-Kit Lo, Berlin Chen, and Karen\nTang. 2001. Generating phonetic cognates to han-\ndle named entities in english-chinese cross-language\nspoken document retrieval. In IEEE Workshop on\nAutomatic Speech Recognition and Understanding,\n2001. ASRU\u201901., pages 311\u2013314. IEEE.\n\nPaola Merlo and Maria Andueza Rodriguez. 2019.\nCross-lingual word embeddings and the structure\nof the human bilingual lexicon. In Proceedings\nof the 23rd Conference on Computational Natural\nLanguage Learning (CoNLL), pages 110\u2013120, Hong\nKong, China. Association for Computational Lin-\nguistics.\n\nMichelle M Mielke, Rosebud O Roberts, Rodolfo Sav-\nica, Ruth Cha, Dina I Drubach, Teresa Christian-\nson, Vernon S Pankratz, Yonas E Geda, Mary M\nMachulda, Robert J Ivnik, et al. 2012. Assess-\ning the temporal relationship between cognition and\ngait: slow gait predicts cognitive decline in the mayo\nclinic study of aging. Journals of Gerontology Se-\nries A: Biomedical Sciences and Medical Sciences,\n68(8):929\u2013937.\n\nAbhijit Mishra and Pushpak Bhattacharyya. 2018.\nCognitively Inspired Natural Language Processing:\nAn Investigation Based on Eye-tracking. Springer.\n\nAbhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal\nDey, and Pushpak Bhattacharyya. 2016. Harnessing\ncognitive features for sarcasm detection. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1095\u20131104, Berlin, Germany. Associa-\ntion for Computational Linguistics.\n\nAbhijit Mishra, Srikanth Tamilselvam, Riddhiman\nDasgupta, Seema Nagar, and Kuntal Dey. 2018a.\nCognition-cognizant sentiment analysis with multi-\ntask subjectivity summarization based on annotators\u2019\ngaze behavior. In Thirty-Second AAAI Conference\non Artificial Intelligence.\n\nPushkar Mishra, Marco Del Tredici, Helen Yan-\nnakoudakis, and Ekaterina Shutova. 2018b. Author\nprofiling for abuse detection. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1088\u20131098, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\n\nhttps:\/\/doi.org\/10.18653\/v1\/N16-1179\nhttps:\/\/doi.org\/10.18653\/v1\/N16-1179\nhttps:\/\/www.aclweb.org\/anthology\/N01-1014\nhttps:\/\/www.aclweb.org\/anthology\/N01-1014\nhttps:\/\/www.aclweb.org\/anthology\/2020.sltu-1.49\nhttps:\/\/www.aclweb.org\/anthology\/2020.sltu-1.49\nhttps:\/\/doi.org\/10.18653\/v1\/K19-1011\nhttps:\/\/doi.org\/10.18653\/v1\/K19-1011\nhttps:\/\/doi.org\/10.18653\/v1\/P16-1104\nhttps:\/\/doi.org\/10.18653\/v1\/P16-1104\nhttps:\/\/www.aclweb.org\/anthology\/C18-1093\nhttps:\/\/www.aclweb.org\/anthology\/C18-1093\n\n\nAndrea Mulloni and Viktor Pekar. 2006. Automatic\ndetection of orthographics cues for cognate recogni-\ntion. In LREC, pages 2387\u20132390.\n\nJohn Nerbonne and Wilbert Heeringa. 1997. Measur-\ning dialect distance phonetically. In Computational\nPhonology: Third Meeting of the ACL Special Inter-\nest Group in Computational Phonology.\n\nOlumide Owolabi and DR McGregor. 1988. Fast ap-\nproximate string matching. Software: Practice and\nExperience, 18(4):387\u2013393.\n\nFabian Pedregosa, Gae\u0308l Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, et al. 2011. Scikit-learn:\nMachine learning in python. the Journal of machine\nLearning research, 12:2825\u20132830.\n\nTaraka Rama. 2016. Siamese convolutional networks\nfor cognate identification. In Proceedings of COL-\nING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages\n1018\u20131027.\n\nTaraka Rama, Johann-Mattis List, Johannes Wahle, and\nGerhard Ja\u0308ger. 2018. Are automatic methods for\ncognate detection good enough for phylogenetic re-\nconstruction in historical linguistics? arXiv preprint\narXiv:1804.05416.\n\nMorteza Rohanian. 2017. Multi-document summariza-\ntion of Persian text using paragraph vectors. In Pro-\nceedings of the Student Research Workshop Asso-\nciated with RANLP 2017, pages 35\u201340, Varna. IN-\nCOMA Ltd.\n\nRosa M Sa\u0301chez-Casas, Jose\u0301 E Garc\u0131\u0301a-Albea, and\nChristopher W Davis. 1992. Bilingual lexical pro-\ncessing: Exploring the cognate\/non-cognate distinc-\ntion. European Journal of Cognitive Psychology,\n4(4):293\u2013310.\n\nClaude E Shannon. 1948. A mathematical theory of\ncommunication. The Bell system technical journal,\n27(3):379\u2013423.\n\nAbhinav Deep Singh, Poojan Mehta, Samar Husain,\nand Rajkumar Rajakrishnan. 2016. Quantifying\nsentence complexity based on eye-tracking mea-\nsures. In Proceedings of the Workshop on Com-\nputational Linguistics for Linguistic Complexity\n(CL4LC), pages 202\u2013212, Osaka, Japan. The COL-\nING 2016 Organizing Committee.\n\nEva Van Assche, Wouter Duyck, Robert J Hartsuiker,\nand Kevin Diependaele. 2009. Does bilingual-\nism change native-language reading? cognate ef-\nfects in a sentence context. Psychological science,\n20(8):923\u2013927.\n\nVictoria Yaneva, Le An Ha, Richard Evans, and\nRuslan Mitkov. 2020. Classifying referential and\nnon-referential it using gaze. arXiv preprint\narXiv:2006.13327.\n\nhttps:\/\/doi.org\/10.26615\/issn.1314-9156.2017_005\nhttps:\/\/doi.org\/10.26615\/issn.1314-9156.2017_005\nhttps:\/\/www.aclweb.org\/anthology\/W16-4123\nhttps:\/\/www.aclweb.org\/anthology\/W16-4123\nhttps:\/\/www.aclweb.org\/anthology\/W16-4123\n\n","6":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLex Rosetta: Transfer of Predictive Models Across Languages, Jurisdictions, and Legal Domains\n\n\nLex Rosetta: Transfer of Predictive Models Across Languages,\nJurisdictions, and Legal Domains\n\nJaromir Savelka\njsavelka@cs.cmu.edu\n\nCarnegie Mellon University\nUSA\n\nHannes Westermann\nKarim Benyekhlef\nUniversit\u00e9 de Montr\u00e9al\n\nCanada\n\nCharlotte S. Alexander\nJayla C. Grant\n\nGeorgia State University\nUSA\n\nDavid Restrepo Amariles\nRajaa El Hamdani\n\nHEC Paris\nFrance\n\nS\u00e9bastien Mee\u00f9s\nAurore Troussel\n\nHEC Paris\nFrance\n\nMicha\u0142 Araszkiewicz\nUniwersytet Jagiello\u0144ski\n\nPoland\n\nKevin D. Ashley\nAlexandra Ashley\n\nUniversity of Pittsburgh\nUSA\n\nKarl Branting\nMITRE Corporation\n\nUSA\n\nMattia Falduti\nLibera Universit\u00e0 di Bolzano\n\nItaly\n\nMatthias Grabmair\nTechnische Universit\u00e4t M\u00fcnchen\n\nGermany\n\nJakub Hara\u0161ta\nTereza Novotn\u00e1\n\nMasarykova univerzita\nCzech Republic\n\nElizabeth Tippett\nShiwanni Johnson\nUniversity of Oregon\n\nUSA\n\nABSTRACT\nIn this paper, we examine the use of multi-lingual sentence embed-\ndings to transfer predictive models for functional segmentation of\nadjudicatory decisions across jurisdictions, legal systems (common\nand civil law), languages, and domains (i.e. contexts). Mechanisms\nfor utilizing linguistic resources outside of their original context\nhave significant potential benefits in AI & Law because differences\nbetween legal systems, languages, or traditions often block wider\nadoption of research outcomes. We analyze the use of Language-\nAgnostic Sentence Representations in sequence labeling models\nusing Gated Recurrent Units (GRUs) that are transferable across\nlanguages. To investigate transfer between different contexts we\ndeveloped an annotation scheme for functional segmentation of\nadjudicatory decisions. We found that models generalize beyond\nthe contexts on which they were trained (e.g., a model trained on\nadministrative decisions from the US can be applied to criminal law\ndecisions from Italy). Further, we found that training the models on\nmultiple contexts increases robustness and improves overall perfor-\nmance when evaluating on previously unseen contexts. Finally, we\nfound that pooling the training data from all the contexts enhances\nthe models\u2019 in-context performance.\n\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner\/author(s).\nICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil\n\u00a9 2021 Copyright held by the owner\/author(s).\nACM ISBN 978-1-4503-8526-8\/21\/06.\nhttps:\/\/doi.org\/10.1145\/3462757.3466149\n\nCCS CONCEPTS\n\u2022 Applied computing \u2192 Law; Annotation; \u2022 Information sys-\ntems \u2192 Document structure; Structure and multilingual text\nsearch; Data mining.\n\nKEYWORDS\nmulti-lingual sentence embeddings, transfer learning, domain adap-\ntation, adjudicatory decisions, document segmentation, annotation\n\nACM Reference Format:\nJaromir Savelka, HannesWestermann, KarimBenyekhlef, Charlotte S. Alexan-\nder, Jayla C. Grant, David Restrepo Amariles, Rajaa El Hamdani, S\u00e9bastien\nMee\u00f9s, Aurore Troussel, Micha\u0142 Araszkiewicz, Kevin D. Ashley, Alexandra\nAshley, Karl Branting, Mattia Falduti, Matthias Grabmair, Jakub Hara\u0161ta,\nTereza Novotn\u00e1, Elizabeth Tippett, and Shiwanni Johnson. 2021. Lex Rosetta:\nTransfer of Predictive Models Across Languages, Jurisdictions, and Legal\nDomains. In Eighteenth International Conference for Artificial Intelligence\nand Law (ICAIL\u201921), June 21\u201325, 2021, S\u00e3o Paulo, Brazil. ACM, New York,\nNY, USA, 10 pages. https:\/\/doi.org\/10.1145\/3462757.3466149\n\n1 INTRODUCTION\nThis paper explores the ability of multi-lingual sentence embed-\ndings to enable training of predictive models that generalize beyond\nindividual languages, legal systems, jurisdictions, and domains (i.e.,\ncontexts). We propose a new type schema for functional segmenta-\ntion of adjudicatory decisions (i.e., decisions of trial and appellate\ncourt judges, arbitrators, administrative judges and boards) and use\nit to annotate legal cases across eight different contexts (7 countries,\n6 languages). We release the newly created dataset (807 documents\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n07\n88\n\n2v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n5 \nD\n\nec\n 2\n\n02\n1\n\nhttps:\/\/doi.org\/10.1145\/3462757.3466149\nhttps:\/\/doi.org\/10.1145\/3462757.3466149\n\n\nICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil Savelka and Westermann, et al.\n\nwith 89,661 annotated sentences) including the annotation schema\nto the public.1\n\nIn the area of AI & Law, research typically focuses on a single\ncontext, such as decisions of a specific court on a specific issue\nwithin a specific time range. This is justified by the complexity of\nlegal work and the need for nuanced solutions to particular prob-\nlems. At the same time, this narrow focus can limit the applicability\nof the research outcomes, since a proposed solution might not be\nreadily transferable to a different context. In text classification, for\nexample, a model might simply memorize a particular vocabulary\ncharacteristic of a given context, rather than acquiring the seman-\ntics of a predicted type. Adaptation of such a model to a new context\nwould then require the assembly of a completely new dataset. This\nmay be both time-consuming and expensive, since the annotation\nof legal documents relies on legal expertise.\n\nCertain tasks appear to be of interest to researchers frommultiple\ncountries with different legal traditions (e.g., deontic classification\nof legal norms embodied in statutory law, argument extraction from\ncase law, summarization\/simplification of legal documents, etc.).\nThis suggests that there may be several core tasks in AI & Law that\nare of general interest in almost any context. One such task is a\nfunctional segmentation of adjudicatory decisions, which has been\nthe subject of numerous studies in the past (see Section 2). In this\npaper, we show that for this particular task it is possible to leverage\nlinguistic resources created in multiple contexts.\n\nThis has wide-reaching implications for AI & Law research. Since\nannotation of training data is expensive, models that are able to use\nexisting data from other contexts might be instrumental in enabling\nreal-world applications that can be applied across contexts. Such\napproaches may further enable international collaboration of re-\nsearchers, each annotating their own part of a dataset to contribute\nto a common pool (as we do in this work) that could be used to\ntrain strong models able to generalize across contexts.\n\n1.1 Functional Segmentation\nWe investigate the task of segmenting adjudicatory decisions based\non the functional role played by their parts. While there are signifi-\ncant differences in how decisions are written in different contexts,\nwe hypothesize that selected elements might be universal, such as,\nfor example, sections:\n\n(1) describing facts that give rise to a dispute;\n(2) applying general legal rules to such facts;\n(3) stating an outcome of the case (i.e., how was it decided).\n\nThis conjecture is supported by the results of the comparative\nproject titled Interpreting Precedents [23], which aimed to analyze\n(among other things) the structure in 11 different jurisdictions. The\nfindings of this project suggest that the structure indicated above\nmay be considered a general model followed in the investigated\njurisdictions, although variations exist that are characteristic of\nparticular legal systems and types of courts and their decisions.\n\nThe ability to segment cases automatically could be beneficial\nfor many tasks. It could support reading and understanding of\nlegal decisions by students, legal practitioners, researchers, and\nthe public. It could facilitate empirical analyses of the discourse\nstructure of decisions. It could enhance the performance, as well\n1https:\/\/github.com\/lexrosetta\/caselaw_functional_segmentation_multilingual\n\nas the user experience, of legal search tools. For example, if a user\nsearches for an application of a legal rule, they might restrict the\nsearch to the section where a judge applies the rule to a factual\nsituation. Judges themselves might find the technique useful, within\ntheir own jurisdictions but also in transnational disputes involving\nthe application of different legal standards. The same benefits apply\nto non-court settings, e.g., international arbitration, where many\njurisdictions\u2019 laws and their interpretation matter. Further, the\nsegmentation of decisions into meaningful sections could serve as\nan important step in many legal document processing pipelines.\n\n1.2 Hypotheses\nTo investigate how well predictive models, based on multi-lingual\nsentence embeddings, learn to segment cases into functional parts\nacross different contexts, we evaluated the following hypotheses:\n\n(H1) A model trained on a single context can generalize when\ntransferred to other, previously unseen, contexts.\n\n(H2) A model trained on data pooled from multiple contexts is\nmore robust and generalizes better to unseen contexts than\na model trained on a single context.\n\n(H3) A context-specificmodel benefits from pooling the in-domain\ndata with data from other contexts.\n\n1.3 Contributions\nBy carrying out this work, we provide the following contributions\nto the AI & Law research community:\n\n\u2022 Detailed definition and analysis of a functional segmentation\ntask that is widely applicable across different contexts.\n\n\u2022 A new labeled dataset consisting of 807 documents (89,661\nsentences) from seven countries in six different languages.\n\n\u2022 Evidence of the effectiveness of multi-lingual embeddings\non processing legal documents.\n\n\u2022 Release of the code used for data preparation, analysis, and\nthe experiments in this work.\n\n2 RELATEDWORK\nSegmenting court decisions into smaller elements according to their\nfunction or role is an important task in legal text processing. Prior\nresearch utilizing supervised machine learning (ML) approaches\nor expert crafted rules can roughly be distinguished into two cat-\negories. First, the task could be to segment the text into a small\nnumber of contiguous parts typically comprising multiple para-\ngraphs (this work). Different variations of this task were applied\nto several legal domains from countries, such as Canada [15], the\nCzech Republic [17], France [8], or the U.S. [27]. Second, the task\ncould instead be labeling smaller textual units, often sentences,\naccording to some predefined type system (e.g., rhetorical roles,\nsuch as evidence, reasoning, conclusion). Examples from several\ndomains and countries include administrative decisions from the\nU.S. [33, 41], multi-domain court decisions from India [6], inter-\nnational arbitration decisions [9], or even multi-{domain,country}\nadjudicatory decisions in English [28]. Identifying a section that\nstates an outcome of the case has also received considerable atten-\ntion separately [25, 38]. To the best of our knowledge, existing work\non functional segmentation of court decisions is limited to a single\n\nhttps:\/\/github.com\/lexrosetta\/caselaw_functional_segmentation_multilingual\n\n\nLex Rosetta: Transfer of Predictive Models Across Languages, Jurisdictions, and Legal Domains ICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil\n\nlanguage\u2014ours being the first paper exploring the task jointly on\nlegal documents in multiple languages.\n\nIn NLP, the success of word embeddings was followed by an\nincreasing interest in learning continuous vector representations of\nlonger linguistic units, such as sentences (a trend that has been re-\nflected in AI & Law research as well [34, 41]). Multi-lingual represen-\ntations recently attracted ample attention. While most of the earlier\nwork was limited to a few close languages or pairwise joint em-\nbeddings for English and one foreign language, several approaches\nto obtain general-purpose massively multi-lingual sentence repre-\nsentations were proposed [5, 11, 13]. Such representations were\nutilized in many downstream applications, such as document clas-\nsification [21], machine translation [2], question answering [22],\nhate speech detection [4], or information retrieval (IR) in the legal\ndomain [40]. Our work is one of the first such applications in the\nlegal domain and to the best of our knowledge the first dealing with\nmore than two languages.\n\nApproaches other than language-agnostic sentence embeddings\n(this work) were used in AI & Law research focused on texts in\nmultiple languages. A recent line of work mapped recitals to arti-\ncles in EU directives and normative provisions in Member states\u2019\nlegislation [24]. There, mono-lingual models were used (i.e., one\nmodel per language). Other published applications in multi-lingual\nlegal IR were based on thesauri [14, 29]. A common technique to\nbridge the language gap was the use of ontologies and knowledge\ngraphs [1, 3, 7, 16]. The multi-lingual environments, such as EU\nor systems established by international treaties, attracted work on\nmachine translation [20], meaning equivalence verification [32],\nand building of parallel corpora [30, 31].\n\n3 DATASET\nIn creating the dataset, the first goal was to identify a task that\nwould be useful across different contexts. After extensive litera-\nture review, we identified the task of functional segmentation of\nadjudicatory decisions as a viable candidate. To make the task gen-\neralizable, we decided to include only a small number of core types.\n\n(1) Out of Scope \u2013 Parts outside of the main document body (e.g.,\nmetadata, editorial content, dissents, end notes, appendices).\n\n(2) Heading \u2013 Typically an incomplete sentence or marker start-\ning a section (e.g., \u201cDiscussion,\u201d \u201cAnalysis,\u201d \u201cII.\u201d).\n\n(3) Background \u2013 The part where the court describes procedural\nhistory, relevant facts, or the parties\u2019 claims.\n\n(4) Analysis \u2013 The section containing reasoning of the court,\nissues, and application of law to the facts of the case.\n\n(5) Introductory Summary \u2013 A brief summary of the case at the\nbeginning of the decision.\n\n(6) Outcome \u2013 A few sentences stating how the case was decided\n(i.e, the overall outcome of the case).\n\nWe created detailed annotation guidelines defining the individ-\nual types as well as describing the annotation workflow (tooling,\nsteps taken during annotation). Eight teams of researchers from six\ndifferent countries (14 persons) were trained in the annotation pro-\ncess through online meetings. After this, each annotator conducted\na dry-run annotation on 10 cases and received detailed feedback.\nThen, each team was tasked with assembling approximately 100\n\nadjudicatory decisions. Each team developed specifications for the\ndecisions to be included in their part of the dataset.\n\nFour of the contexts were double-annotated by two annotators\n(Canada, Czech R., France, U.S.A. I); the remaining four by just\none. Each team had at least one member with a completed law\ndegree. When a team had more than one member, law students\nwere allowed to be included.\n\nA high-level description of the resulting dataset is provided in\nTable 1. It consists of eight contexts from seven different countries\n(two parts are from the U.S.) with 807 documents in six languages\n(three parts are in English). Most of the contexts include judicial\ndecisions, while U.S.A. II was the only context that consisted solely\nof administrative decisions.There are considerable variations in the\nlength of the documents. While an average document in the U.S.A. I\ncontext comprises of 530.6 sentences, an average document in the\nFrance context is about ten times shorter (59.0 sentences).\n\nThe four double-annotated parts enabled us to examine the inter-\nannotator agreement. Table 2 shows the raw agreement on a charac-\nter level. While it appears that recognizing the Outcome was rather\nstraightforward in the France and U.S.A. I contexts, it was more\ncomplicated in case of Canada and the Czech R. This might be due\nto a presence\/absence of some structural clue. We also observe that\nin the Czech R. context it was presumably much easier to distin-\nguish between the Background and Analysis than in case of the\nother three contexts.\n\nIn this paper, we focus on prediction of the Background, Analysis,\nand Outcome types. We decided to exclude the Introductory Sum-\nmary type, since it is mainly present in the data from the United\nStates. For the double-annotated datasets, we picked the annota-\ntions that appeared to be of higher quality (either by consensus\nbetween the annotators themselves or by a decision of a third unbi-\nased expert).\n\nWe first removed all the spans of text annotated with either of\nthe Out of Scope or Heading types. The removal of Out of Scope\nleaves the main body of a decision stripped of potential metadata\nor editorial content at the beginning of the document as well as\ndissents, or end notes at the end. The removal of the text spans\nannotated with the Heading type might appear counter-intuitive\nsince headings often provide a clear clue as to the content of the\nfollowing section (e.g., \u201cOutcome\u201d, \u201cAnalysis\u201d etc.). We remove\nthese (potentially valuable) headings because we want to focus\non the more interesting task of recognizing the sections purely\nby the semantics of their constitutive sentences. This task is more\nchallenging, and more closely emulates generalization to domains\nwhere headings are not used or not present in all cases, or are not\nreliable indicators.\n\nThe transformed documents are separated into several segments\nbased on the annotations of the three remaining types. Each seg-\nment is then split into sentences.2 A resulting document is a se-\nquence of sentences labeled with one of the Background, Analysis,\nor Outcome types. The highlighted (green) part of Table 1 provides\n\n2We used the processing pipeline from https:\/\/spacy.io\/ (large models). For the Czech\nlanguage we used https:\/\/github.com\/TakeLab\/spacy-udpipe with the Czech model\n(PDT) from https:\/\/universaldependencies.org\/. The output was further processed\nwith several regular expressions. A different method was used for the French dataset,\nwhich consists of a few very long sections, internally separated by a semicolon. After\nconsultation with an expert we decided to split the cases by the semicolon as well.\n\nhttps:\/\/spacy.io\/\nhttps:\/\/github.com\/TakeLab\/spacy-udpipe\nhttps:\/\/universaldependencies.org\/\n\n\nICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil Savelka and Westermann, et al.\n\nTable 1: Descriptive statistics of the created dataset. Each entry provides information about the country, the language of the\ndecisions (Lang), and the number of documents (Docs) in a specific context. The Sentence-Level Statistics subsection reports\nbasic descriptive statistics focused on sentences as well as the number of sentences labeled with each type (OoS - Out of Scope,\nHead - Heading, Int.S. - Introductory Summary, Back - Background, Anl - Analysis, Out - Outcome). The part highlighted in\ngreen contains the counts of sentences labeled with the types we focus on in this work.\n\nSentence-Level Statistics\nCountry Lang Docs Count Avg Min Max OoS Head Int.S. Back Anl Out Description\nCanada EN 100 12168 121.7 8 888 873 438 20 3319 7190 328 Random selection of cases retrieved from www.canlii.org from multiple provinces.\n\n7.2% 3.6% 0.2% 27.3% 59.1% 2.7% The selection is not limited to any specific topic or court.\nCzech R. CS 100 11283 112.8 10 701 945 1257 2 3379 5422 278 A random selection of cases from Constitutional Court (30), Supreme Court (40), and\n\n8.4% 11.1% 0.0% 29.9% 48.1% 2.5% Supreme Administrative Court (30). Temporal distribution was taken into account.\nFrance FR 100 5507 55.1 8 583 3811 220 0 485 631 360 A selection of cases decided by Cour de cassation between 2011 and 2019. A stratified\n\n69.2% 4.0% 0.0% 8.8% 11.4% 6.5% sampling based on the year of publication of the decision was used to select the cases.\nGermany DE 104 10724 103.1 12 806 406 333 38 2960 6697 290 A stratified sample from the federal jurisprudence database spanning all federal courts\n\n3.8% 3.1% 0.4% 27.6% 62.4% 2.7% (civil, criminal, labor, finance, patent, social, constitutional, and administrative).\nItaly IT 100 4534 45.3 10 207 417 1098 0 986 1903 130 The top 100 cases of the criminal courts stored between 2015 and 2020 mentioning\n\n9.2% 24.2% 0.0% 21.7% 42.0% 2.9% \u201cstalking\u201d and keyed to the Article 612 bis of the Criminal Code.\nPoland PL 101 9791 96.9 4 1232 796 303 0 2736 5820 136 A stratified sample from trial-level, appellate, administrative courts, the Supreme Court,\n\n8.1% 3.1% 0% 27.9% 59.4% 1.4% and the Constitutional tribunal. The cases mention \u201cdemocratic country ruled by law.\u201d\nU.S.A. I EN 102 24898 244.1 34 1121 574 1235 475 6042 16098 474 Federal district court decisions in employment law mentioning \u201cmotion for summary\n\n2.3% 5.0% 1.9% 24.3% 64.7% 1.9% judgment,\" \u201cemployee,\u201d and \u201cindependent contractor.\u201d\nU.S.A. II EN 100 10756 107.6 24 397 1766 650 639 3075 4402 224 Administrative decisions from the U.S. Department of Labor. Top 100 ordered in\n\n1.6% 6.0% 5.9% 28.6% 40.9% 2.1% reverse chronological rulings order, starting in October 2020, were selected.\nOverall 6 807 89661 105.6 4 1232 9588 5534 1174 22982 48163 2220\n\nTable 2: Raw agreement on a character level for the four\ndatasets with two human annotators. The agreement is com-\nputed as a percentage of characters where both the annota-\ntors agree on a specific type over all the characters annotated\nby that type by any of the annotators. (NM=Not Marked)\n\nOoS Head Int.S. Back Anl Out NM\n\nCanada 97.2 68.2 44.0 83.3 92.2 79.9 43.4\nCzech R. 80.3 54.6 0.0 92.6 94.5 46.9 10.0\nFrance 93.5 92.5 N\/A 43.0 72.2 99.1 1.0\nU.S.A. I 90.8 71.0 74.2 78.4 93.7 91.1 18.4\n\nOverall 91.8 70.4 72.1 82.1 92.6 77.3 3.8\n\nbasic descriptive statistics of the resulting dataset per the individual\ncontexts. Our final dataset for analysis consists of 807 cases split\ninto 74,539 annotated sentences.\n\n4 MODELS\nIn our experiments we use the Language-Agnostic Sentence Repre-\nsentations (LASER) model [5] to encode sentences from different\nlanguages into a shared semantic space. Each document becomes\na series of vectors which represent the semantic content of a sin-\ngle sentence. We use these vectors to train a bidirectional Gated\nRecurrent Unit (GRU) model [10] for predicting sentence labels.\n\nThe LASER model is a language-agnostic bidirectional LSTM\nencoder coupled with an auxiliary decoder and trained on parallel\ncorpora. The sentence embeddings are obtained by applying a max-\npooling operation over the output of the encoder. The resulting\nsentence representations (after concatenating both directions) are\n1024-dimensional. The released trained model,3 which we use in\nthis work, supports 93 languages (including the six in our dataset)\nbelonging to 30 different families and written in 28 different scripts.\nThe model was trained on 223 million parallel sentences. The joint\n\n3https:\/\/github.com\/facebookresearch\/LASER\n\nencoder itself has no information on the language or writing script\nof the tokenized text, while the tokenizer is language specific. It\nis even possible to mix multiple languages in one sentence. The\nfocus of the LASER model is to produce vector representations\nof sentences that are general with respect to two dimensions: the\ninput language and the NLP task [5]. An interesting property of\nsuch universal multi-lingual sentence embeddings is the increased\nfocus on the sentence semantics, as the syntax or other surface\nproperties are unlikely to be shared among languages.\n\nThe GRU neural network [10] is an architecture based on a\nrecurrent neural network (RNN) that is able to learn the mapping\nfrom a sequence of an arbitrary length to another sequence. GRUs\nare able to either score a pair of sequences or to generate a target\nsequence given a source sequence (this work). In a bidirectional\nGRU, two separate sequences are considered (one from right to\nleft and the other from left to right). Traditional RNNs work well\nfor shorter sequences but cannot be successfully applied to long\nsequences due to the well-known problem of vanishing gradients.\nLong Short-Term Memory (LSTM) networks [18] have been used\nas an effective solution to this problem (the forget gate, along with\nthe additive property of the cell state gradients). GRUs have been\nproposed as an alternative to LSTMs with a reduced number of\nparameters. In GRUs there is no explicit memory unit, and the\nforget gate and the update gate are combined. The performance of\nGRUs was shown to be superior to that of LSTMs in the scenario\nof long texts and small datasets [39], which is the situation in this\nwork. For these reasons, we chose to use GRUs over LSTMs.\n\nThe overall structure of the employed model is shown in Fig-\nure 1.4 Each case is transformed into a 1080 \u00d7 1024 matrix. The\nnumber 1080 represents the maximum length (in sentences) of any\ncase in our dataset. Shorter cases are padded to be of uniform length.\nThe vectors are passed to the model in batches of size 32. They first\ngo through a masking layer, which masks the sentences used for\n\n4The model was implemented using the Keras framework (https:\/\/keras.io\/).\n\nwww.canlii.org\nhttps:\/\/github.com\/facebookresearch\/LASER\nhttps:\/\/keras.io\/\n\n\nLex Rosetta: Transfer of Predictive Models Across Languages, Jurisdictions, and Legal Domains ICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil\n\nFigure 1: The structure of the sequential model used for pre-\ndiction. Each case\ud835\udc5a is split into \ud835\udc5b sentences, which are con-\nverted to language-independent LASER vector embeddings.\nThese are fed through a bidirectional recurrent GRU-model,\nwhich predicts one of the three labels per input sentence.\n\npadding. The data is then passed to a bidirectional GRU model\nwith 256 units and a dropout of 0.2. Finally, the model contains a\ntime-distributed dense output layer with softmax activation, which\noutputs the predicted label (i.e., Background, Analysis, Outcome) for\neach sentence. As a loss function, we use categorical cross-entropy.\nAs optimizer we use the Adam algorithm [19] with initial learning\nrate set to 4\ud835\udc52\u22123 (reduced to 4\ud835\udc52\u22124 once validation loss has stopped\ndecreasing, with a patience of 50 epochs). We train the model for up\nto 1000 epochs. We halt the training once the validation accuracy\nhas not increased in 80 epochs. For prediction, we use the best\nmodel as determined by validation accuracy.\n\n5 EXPERIMENTAL DESIGN\nWeperformed three experiments to test the hypotheses (Section 1.2).\nThe first experiment (H1) focused on model generalization across\ndifferent contexts (Out-Context). The second experiment (H2) as-\nsessed model robustness when trained on multiple pooled contexts\ndifferent from the one where the model was applied (Pooled Out-\nContext). Finally, the third experiment (H3) analyzed the effects\nof pooling the target context data with data from other contexts\n(Pooled with In-Context). The different pools of training data, and\nthe baselines we compare them against, are summarized in Table 3\nand described in Sections 5.1-5.3.\n\nSince our dataset is limited, we performed a 10-fold cross-vali-\ndation. The folds were kept consistent across all the experiments.\nThe experiments were conducted in the following manner:\n\n(1) Index \ud835\udc56 = 1 is set.\n(2) Pool of training context(s) is selected (see Sections 5.1-5.3).\n(3) A single test context is selected (see Sections 5.1-5.3).\n(4) Eight of the folds from the training context(s) are used as\n\ntraining data (index different from \ud835\udc56 and (\ud835\udc56 + 1) mod 10).\n(5) The folds with index (\ud835\udc56 + 1) mod 10 from the training con-\n\ntext(s) are designated as validation data.\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\nOut-Context Experiment (H1)\n\nTrain\n\nVal\n\nTest\nContext 1 Context n Target Ctx\n\n(i = 1)\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\nContext 1 Context n Target Ctx\n\nVal\n\nTrain\n\nTest\n\nTrain\n\n(i = 2)\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\nPooled Out-Context (H2)\n\nTrain\n\nVal\n\nTest\nContext 1 Context n Target Ctx\n\n(i = 1) (i = 1)\n\nOut-Context Experiment (H1)\n\nPooled With In-Context (H3)\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\n1\n\n2\n\n3\n\n...\n\n9\n\n10\n\nTrain\n\nVal\n\nTest\nContext 1 Context n Target Ctx\n\nFigure 2: A visualization of the three experimental setups.\nThe figure shows how training contexts are selected for H1,\nH2 and H3, and examples of how the folds are assigned to\nTrain, Val and Test.\n\nTable 3: Description of different training data selection and\nbaselines used for H1, H2 andH3. The random (H1), the best\nsingle Out-Context models (H2), and the In-Context models\n(H3) baselines are highlighted.\n\nName Trained on Hyp. Baseline\n\nRandom Target Context - -\nIn-Context Target Context - -\nOut-Context Non-Target Context H1 Random\nPooled Out-Context Pooled Non-Target H2 Best performing Out-\n\nContexts Context model per context\nPooled with In-Context Non-Target and Target H3 In-Context\n\nPooled Contexts\n\n(6) The \ud835\udc56 fold from the test context is designated as test data.\n(7) The models are trained and evaluated.\n(8) Index \ud835\udc56 = \ud835\udc56 + 1 is set.\n(9) If \ud835\udc56 \u2264 10 go to (4) else finish.\n\nNote that from here on we highlight the baselines using the colors\nas shown in Table 3 to improve clarity.\n\n5.1 Out-Context Experiment (H1)\nIn this experiment, we investigated the ability of the models to\ngeneralize beyond the training context. The pool of training data\nconsisted of a single context at a time (see Figure 2). As a baseline,\nwe used a random classifier that learns the distribution of the target\ndataset (train and validation folds) and generates predictions based\non that distribution. The baseline thus had a slight advantage in\nhaving access to the signal from the target dataset. The Out-Context\nmodels performing better than the random baseline would show\n\n\n\nICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil Savelka and Westermann, et al.\n\nthat the models are able to transfer (at least some) knowledge from\none context to another.\n\n5.2 Pooled Out-Context Experiment (H2)\nThe second experiment focused on the ability of the models to gain\nrobustness as they learn from more than one context, when applied\nto an unseen context. Therefore, the training pool for the exper-\niments consisted of data from all the contexts except the current\ntarget context (see Figure 2). As a baseline for each context, we used\nthe performance measurement taken from the best Out-Context\nmodel for that context (see Section 5.1). This was a very competitive\nbaseline since the best Out-Context model likely stems from the\ncontext that is the most similar to the target context. If the Pooled\nOut-Context model performed better than the best Out-Context\nmodel, this would indicate that pooling datasets increases the ro-\nbustness of the model, allowing it to perform better on previously\nunseen contexts.\n\n5.3 Pooled With In-Context Experiment (H3)\nThe third experiment focused on pooling the target context\u2019s data\nwith data from the other contexts. The training pool therefore\nincorporates all of the contexts, including the target one (see Fig-\nure 2). As a baseline, we used the In-Context models, trained on\nthe target context. Again, this is a very strong baseline since the\nIn-Context model should be able to learn the most accurate signal\nfrom the target context (including context specific peculiarities). If\nthe model trained on the pooled contexts (including target) is able\nto outperform the In-Context model, this would indicate that pool-\ning contexts is beneficial in terms of both robustness and absolute\nperformance on the individual contexts.\n\n6 EVALUATION METHOD\nTo evaluate the performance of the systems trained on data from\ndifferent contexts, we used Precision (\ud835\udc43 ), Recall (\ud835\udc45), and \ud835\udc391-measure.\nWe compute the evaluation metrics for each class (i.e., Background,\nAnalysis, Outcome) per fold. We then report per class \ud835\udc391-scores as\nwell as an overall \ud835\udc391-score (micro) including its standard deviation.\n\nFor statistical evaluation we used the Wilcoxon signed-rank\ntest [37] to perform pair comparisons between methods and the\nbaselines as suggested by [12]. We used the overall (micro) \ud835\udc391-score\nas the evaluation metric. The null-hypothesis states that the mean\nperformance of the two compared methods (i.e., an assessed model\nand a baseline) is equivalent.\n\nSince the number of samples is rather small (7 contexts in testing\nH1, and 8 contexts in testingH2 andH3), we are not able to reject the\nnull hypotheses at \ud835\udefc = 0.05 for any of H1\u2013H3. This is as expected\ngiven the size of the dataset, and it is not unacceptable given the\nexploratory nature of our work at this stage. We will be in a position\nto formally evaluate the hypotheses once we extend the dataset\n(see Section 10). Instead, we report raw p-values produced by the\ntesting as an evidence of the assessed methods\u2019 effectiveness. The\np-value is the probability of seeing the results at least as extreme\nas those we observed given the null hypothesis is correct (i.e., their\nmean performance is the same).\n\nFigure 3: A description of performance metrics reported in\nTable 4.\n\n7 RESULTS\nThe results of all three experiments are shown in Table 4. Each row\nreports a performance of a specific model across different contexts\n(columns). Each cell shows a bold number on the first row which\ncorrespond to the (micro) average F1-score over the three predicted\nclasses, with standard deviation across the 10 folds. The F1-scores\nof the three classes are reported in the second row of each cell\nordered as Background, Analysis, Outcome. A visual explanation of\ncell contents can be found in Figure 3.\n\n7.1 Out-Context Experiment (H1)\nThe performance of the models trained during the Out-Context\nexperiment is reported by the eight rows of Table 4 starting with\nCanada and ending with U.S.A. II. Here, the models are trained on\na single context (row), and then evaluated on all of the contexts\n(columns). It appears the models perform reasonably well under\nthe Out-Context condition. The application of the trained models\noutperforms the random baseline across the board (in 54 out of\n56 instances). This is further corroborated by the low p-values\nobtained for these methods when compared to the random baseline\n(reported in Table 4).\n\nSeveral interesting patterns emerge from the results. First, it\nappears that models trained on contexts with the same language\nor a language from the same family perform better. For example,\nthe models trained on Canada and the two U.S.A. contexts perform\nwell among each other. A similar observation applies to the models\ntrained on the Poland and the Czech R. contexts. Quite surprisingly\nthe models succeeded in identifying the Outcome sentences to some\nextent, despite a heavy under-representation of these sentences\nin our dataset. For example, a model trained on Canada predicts\nthe Outcome sentences of the both U.S.A. contexts with an average\nF1-score close to 0.7. At the same time the Outcome sentences\nare by far the most challenging ones. On multiple occasions the\nmodels completely fail to predict these (e.g., Canada\u2192Germany, or\nPoland\u2192France).\n\nOverall, the results demonstrate the ability of the models to\neffectively transfer the knowledge learned on one context to another.\nAt the same time, it is clear that the models trained on one context\nand applied to a different one do not perform as well as the In-Con-\ntext models.\n\n\n\nLex Rosetta: Transfer of Predictive Models Across Languages, Jurisdictions, and Legal Domains ICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil\n\nTable 4: Results of the Out-Context, Pooled Out-Context, and Pooled with In-Context experiments. Each row reports a perfor-\nmance of a model across contexts. A bold number in each cell reports the (micro) average F1-score over the predicted classes\nand the standard deviation across the 10 folds. The F1-scores of the three classes are reported below ordered as Background,\nAnalysis, Outcome. A visual explanation of cell contents can be found in Figure 3. The random (H1), the best single Out-Con-\ntext models (H2), and the In-Context (H3) baselines are highlighted. The p-values are color coded to match the baselines.\n\nCanada Czech R. France Germany Italy Poland U.S.A. I U.S.A. II Avg (-test) Avg (+test)\n\nRandom .54 \u00b1 .04 .49 \u00b1 .02 .33 \u00b1 .05 .56 \u00b1 .04 .50 \u00b1 .04 .55 \u00b1 .04 .58 \u00b1 .03 .51 \u00b1 .02 .51 \u00b1 .07\n(dist.) .31 .66 .06 .36 .59 .04 .32 .37 .25 .29 .68 .03 .31 .62 .04 .32 .67 .00 .26 .72 .02 .37 .61 .02 .32 .62 .10\n\nCanada .82 \u00b1 .09 .68 \u00b1 .08 .64 \u00b1 .09 .81 \u00b1 .06 .73 \u00b1 .08 .73 \u00b1 .07 .87 \u00b1 .05 .88 \u00b1 .03 .76 \u00b1 .09 .77 \u00b1 .08\n\ud835\udc5d = .016 .75 .87 .70 .53 .80 .39 .64 .66 .57 .71 .89 .00 .55 .82 .66 .50 .85 .00 .75 .92 .69 .87 .90 .70 .65 .83 .43 .66 .84 .46\n\nCzech R. .76 \u00b1 .09 .91 \u00b1 .04 .47 \u00b1 .08 .82 \u00b1 .08 .82 \u00b1 .05 .83 \u00b1 .07 .84 \u00b1 .06 .86 \u00b1 .05 .77 \u00b1 .13 .79 \u00b1 .13\n\ud835\udc5d = .016 .71 .80 .31 .90 .92 .64 .52 .36 .48 .75 .89 .01 .81 .84 .40 .73 .90 .00 .73 .90 .28 .86 .88 .41 .73 .80 .27 .75 .81 .32\n\nFrance .71 \u00b1 .07 .61 \u00b1 .06 .86 \u00b1 .08 .66 \u00b1 .10 .73 \u00b1 .08 .65 \u00b1 .07 .72 \u00b1 .07 .68 \u00b1 .08 .68 \u00b1 .04 .70 \u00b1 .07\n\ud835\udc5d = .016 .45 .83 .69 .37 .78 .45 .81 .83 .98 .37 .82 .00 .59 .81 .69 .33 .82 .00 .32 .86 .69 .48 .81 .73 .42 .82 .46 .47 .82 .53\n\nGermany .72 \u00b1 .10 .64 \u00b1 .09 .29 \u00b1 .12 .88 \u00b1 .11 .69 \u00b1 .09 .77 \u00b1 .09 .73 \u00b1 .10 .83 \u00b1 .07 .67 \u00b1 .16 .69 \u00b1 .17\n\ud835\udc5d = .031 .68 .76 .01 .42 .81 .01 .42 .32 .00 .82 .93 .66 .50 .84 .00 .54 .88 .54 .47 .85 .00 .82 .88 .00 .55 .76 .08 .58 .78 .15\n\nItaly .55 \u00b1 .12 .76 \u00b1 .09 .63 \u00b1 .08 .78 \u00b1 .09 .95 \u00b1 .02 .73 \u00b1 .10 .53 \u00b1 .13 .74 \u00b1 .08 .67 \u00b1 .10 .71 \u00b1 .13\n\ud835\udc5d = .047 .57 .55 .49 .74 .81 .12 .69 .63 .52 .73 .83 .00 .92 .96 .94 .66 .78 .00 .50 .54 .42 .74 .74 .63 .66 .70 .31 .69 .73 .39\n\nPoland .76 \u00b1 .08 .83 \u00b1 .05 .38 \u00b1 .11 .85 \u00b1 .08 .83 \u00b1 .05 .93 \u00b1 .05 .73 \u00b1 .08 .83 \u00b1 .07 .74 \u00b1 .15 .77 \u00b1 .16\n\ud835\udc5d = .016 .66 .84 .00 .82 .89 .01 .48 .52 .00 .73 .91 .44 .80 .91 .00 .89 .95 .88 .44 .86 .00 .82 .87 .00 .68 .83 .06 .71 .84 .17\n\nU.S.A. I .83 \u00b1 .06 .65 \u00b1 .08 .47 \u00b1 .14 .81 \u00b1 .07 .65 \u00b1 .15 .67 \u00b1 .09 .91 \u00b1 .03 .89 \u00b1 .03 .71 \u00b1 .13 .74 \u00b1 .14\n\ud835\udc5d = .016 .76 .87 .59 .45 .79 .49 .35 .61 .35 .71 .89 .00 .40 .80 .58 .38 .83 .00 .84 .94 .73 .87 .91 .68 .56 .81 .38 .60 .83 .43\n\nU.S.A. II .81 \u00b1 .08 .67 \u00b1 .06 .53 \u00b1 .15 .84 \u00b1 .10 .75 \u00b1 .10 .70 \u00b1 .08 .86 \u00b1 .05 .94 \u00b1 .02 .74 \u00b1 .11 .76 \u00b1 .12\n\ud835\udc5d = .016 .74 .86 .65 .49 .80 .31 .50 .63 .41 .76 .91 .00 .57 .84 .75 .43 .84 .00 .72 .92 .59 .93 .96 .82 .60 .83 .39 .64 .85 .44\n\nPooled .83 \u00b1 .06 .87 \u00b1 .03 .66 \u00b1 .08 .90 \u00b1 .04 .85 \u00b1 .04 .88 \u00b1 .05 .81 \u00b1 .10 .92 \u00b1 .03 .84 \u00b1 .08\n\ud835\udc5d = .148 .77 .86 .66 .87 .91 .03 .59 .67 .71 .87 .95 .01 .81 .89 .65 .83 .94 .01 .64 .88 .73 .91 .93 .65 .79 .88 .43\n\nPooled+ .88 \u00b1 .05 .94 \u00b1 .03 .82 \u00b1 .09 .96 \u00b1 .02 .94 \u00b1 .04 .94 \u00b1 .04 .92 \u00b1 .03 .96 \u00b1 .02 .92 \u00b1 .04\n\ud835\udc5d = .195 .83 .91 .77 .94 .95 .70 .76 .78 .96 .94 .98 .64 .91 .95 .90 .92 .97 .65 .86 .95 .84 .95 .96 .80 .89 .93 .78\n\n7.2 Pooled Out-Context Experiment (H2)\nThe performance of Pooled Out-Context models is reported in\nthe Pooled row of Table 4. The experiment concerns the resulting\nmodels\u2019 robustness, i.e., if a model trained on multiple contexts\nadapts well to unseen contexts. We are especially interested if such\na model adapts better than the models trained on single contexts.\n\nThe results suggest that training on multiple contexts leads to\nmodels that are robust and perform better than the models trained\non a single context. The multi-context models outperform the best\nsingle extra-context model baseline in 7 out of 8 cases. The \ud835\udc5d = 0.148\nneeds to be understood in terms of the small number of samples\n(contexts) and competitiveness of the baseline.\n\nInterestingly, the Pooled Out-Context models even appear to\nbe competitive with several In-Context models (Canada, Czech R.,\nGermany, U.S.A. II). The overall average F1-scores are often quite\nhigh (over 0.80 or 0.90). This is a surprising outcome considering\nthe fact that no data from the context on which a model is evaluated\nis used during training.\n\n7.3 Pooled With In-Context Experiment (H3)\nThe performance of the Pooled with In-Context model is reported\nin the Pooled+ row of Table 4. This experiment models a scenario\nwhere a sample of labeled data from the target context is available.\nThe question is whether combining In-Context data with data from\nother contexts leads to improved performance.\n\nThe results appear to suggest that pooling the target context\nwith data from other contexts does lead to improved performance.\nIn case of 3 out of the 8 contexts (Canada, Czech R., and U.S.A. I)\nthe improvement is clear and substantial across all the three classes\n(Background, Analysis, Outcome). For three additional contexts (Ger-\nmany, Poland, and U.S.A. II), the performance also improved in\nterms of overall (micro) F1-score, but took a slight hit for the chal-\nlenging Outcome class. With respect to the two remaining contexts\n(France, Italy) the overall performance of the pooled models is lower\nthan that of the In-Context models. As in the previous experiment,\nthe \ud835\udc5d = 0.195 needs to be understood in terms of the small number\nof samples (contexts) and high competitiveness of the In-Context\nbaseline.\n\n\n\nICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil Savelka and Westermann, et al.\n\nFigure 4: Average LASER embeddings for each case doc-\nument, projected to 2-dimensional space using Principal\nComponent Analysis.\n\n8 DISCUSSION\nIt appears that the multilingual sentence embeddings generated\nby the LASER model excel at capturing the semantics of a sen-\ntence. A model trained on a single context could in theory capture\nthe specific vocabulary used in that context. This would almost\ncertainly lead to poor generalization across different contexts. How-\never, the performance statistics we observed when transferring the\nOut-Context models to other contexts suggest that the sentence\nembeddings provide a representation of the sentences that enable\nthe model to learn aspects of the meaning of a sentence, rather than\nmere surface linguistic features.\n\nThe results clearly point to certain relationships where contexts\nwithin the same or related languages appear to work well together,\ne.g., {Canada, U.S.A. I, and U.S.A. II} or {Czech R. and Poland}. This\ncould indicate that the multi-lingual embeddings work better when\nthe language is the same or similar. It could also point to similarities\nin legal traditions, e.g. the use of somewhat similar sentences to\nindicate a transition from one type of section to the next. Note that\nwe removed headings, which means that explicit cues could not be\nrelied on by the models we trained on such transformed documents.\nFinally, the cause could also be topical (domain) similarity of the\ncontexts (e.g., both the U.S.A. contexts deal with employment law).\nAlso note that the above are just possible explanations. We did not\nperform feature importance analysis on the models.\n\nTo gain insight into this phenomenon, we visualized the relation-\nships among the contexts on a document level. We first calculated\nthe average sentence embedding for each document. This yielded\n1024-dimensional vectors for 807 documents representing their se-\nmantics. We arranged the resulting vectors in a matrix (1024 \u00d7 807)\nand performed a Principal Component Analysis (PCA) reducing\nthe dimensionality of the document vectors to 2. This operation\nenabled a convenient visualization shown in Figure 4.\n\nFigure 5: Distribution of labels across datasets. The X-axis\ncorresponds to unique cases,whereas theY-axis corresponds\nto the normalized length of the cases. The colors correspond\nto the different labels of the sentences (Background, Analy-\nsis, Outcome).\n\nOverall, the cases from the same contexts appear to cluster to-\ngether. This is expected as the documents written in the same\nlanguage, having similar topics, or sharing some similarities due\nto legal traditions, are likely to map to vectors that are closer. The\ndocuments from both the U.S.A. contexts occupy the same region\nin the embedding space. This is not surprising as they come from\nthe same jurisdiction, are written in the same language, and deal\nwith similar topics (employment law). The Canadian cases, which\nare also in English, occupy a nearby space. This could be linked to\nthe language as well as likely similarities in legal traditions. French,\nGerman, and Italian documents occupy the middle space; they are\ncloser to the English documents than those from the Czech R. and\nPoland. Interestingly, the Czech and Polish documents occupy al-\nmost the same space. The Polish context focuses on the rule of\nlaw while the Czech one is supposed to be more general. As the\nlatter deals with the decisions of the top-tier courts (one of them\nConstitutional), it is possible that the topics substantially overlap.\nMoreover, Poland and the Czech R. share similar legal traditions\nand languages from the same family (Slavic), so the close proximity\nof the documents in the embedding space might not be unexpected.\nFinally, German, French, and Canadian cases occupy wider areas\nthan documents from other contexts. This could be due to their\nlack of focus on specific legal domains.\n\nWe observed a peculiar phenomenon where the Out-Context\nmodels trained on the German and Polish contexts failed to detect\nOutcome sentences on the six remaining contexts and vice-versa.\nThe cause is readily apparent from the visualization shown in Fig-\nure 5. Each segment of the figure depicts a spatial distribution of\nsentences color coded with their labels across documents for a par-\nticular context. As can be seen, the cases typically follow a pattern\nof a contiguous Background segment, followed by a long Analysis\nsection. The several Outcome sentences are placed at the very end\nof the documents. In the Polish and German decisions, however,\nthe Outcome sentences come first. The GRU models we use rely on\nthe structure as well as semantics in making their predictions. As\nwe can see, a model trained exclusively on cases that begin with a\nBackground might therefore have difficulties correctly identifying\noutcome sections at the beginning, and vice-versa. However, as we\n\n\n\nLex Rosetta: Transfer of Predictive Models Across Languages, Jurisdictions, and Legal Domains ICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil\n\nwill see below, a model trained with data featuring both structures\ncan learn to correctly identify the correct structure based on the\nsemantics of the sentences.\n\nThe model trained on the French context appears to perform\nbetter on detectingOutcome sentences thanmodels trained on other\ncontexts. This is somewhat surprising as the French model\u2019s overall\nperformance is among the weakest (e.g. compare the Czech model\u2019s\naverage F1 = 0.77 to the F1 = 0.68 of the French model). Again,\nFigure 5 provides an insight into why this happens. The French\ncontext is the only one where the count of Outcome sentences is\ncomparable to those of the other two categories. For all the other\ncontexts, the Outcome sentences are heavily underrepresented. This\nreveals an interesting direction for future work where the use of\nre-sampling may yield models with better sensitivity for identifying\nOutcome sentences.\n\nIn two instances models trained on a single context under-per-\nformed the random baseline. The model trained on the Germany\ncontext achieved the average F1 = 0.29 when applied to the French\ncontext (Random \ud835\udc391 = 0.33). As the model trained on Polish data\nalso performed poorly on the French context (\ud835\udc391 = 0.38) the cause\nappears to be the inability of the two models to detect the Outcome\nsentences at the end (discussed above). As the Outcome sentences\nare heavily present in the French context, this problem manifests\nin a score lower than the random baseline. The second instance\nis the model trained on the Italy context applied to the U.S.A. I\ndata (F1 = 0.53 versus F1 = 0.58 on Random). Here, the cause\nappears to be different. Note that the Italian context likely has a very\nspecific notion of Outcome sentences (F1 = 0.94 on Italy\u2192Italy).\nIt appears that many Analysis sentences from the U.S.A. I context\nwere labeled as Outcome by the model. Summary judgments often\naddress multiple legal issues with their own conclusions which\ncould have triggered the model to label such sentences as Outcome.\n\nAn important finding is the performance of the Pooled Out-\nContext model (H2) shown in the Pooled row of Table 4. The ex-\nperiment simulates training of a model on several contexts, and\nthen applying it to an unseen context. The Pooled Out-Context\nmodels, having no access to the data from a target context, reliably\noutperform the best single Out-Context models. They appear to\nbe competitive with several In-Context models. These results are\nachieved with a fairly small dataset of 807 cases. We expect that\nexpanding the dataset would lead to further improved performance.\n\nThe Pooled with In-Context experiment (H3) models the situ-\nation where data from a target context is available in addition to\nlabeled data from other contexts. Our experiments indicate that\nthe use of data from other contexts (if available) in addition to data\nfrom the target context is preferable to the use of the data from the\ntarget context only. This is evidenced by the improved performance\nof the models trained on the pooled contexts over the single In-Con-\ntext models. The models have an interesting property of being able\nto identify the Outcome sentences with effectiveness comparable\nto (or higher than) the models trained on the same context only.\nThis holds for all the contexts, except Poland where the Outcome\nperformance is a bit lower (0.65 vs. 0.88). This indicates that the\nmodel is able to learn the two possible modes of theOutcome section\nplacement. It successfully distinguishes cases where the section is\nat the beginning from the cases where the Outcome sentences are\nfound toward the end.\n\nThe inclusion of the In-Context data in the pooled data leads\nto a remarkable improvement over only using the pooled Out-\nContext data. The magnitude of the improvement highlights the\nimportance of including such data in the training. We envision that\nthe models trained on different contexts used in combination with\nhigh-speed similarity annotation frameworks [35, 36] could enable\nhighly cost efficient annotation in situations where resources are\nscarce. Perhaps, adapting a model to an entirely new context could\nbe as simple as starting with a model trained on other contexts, and\nspending a few hours correcting the misconceptions of the model\nto teach it the particularities of the new context.\n\n9 CONCLUSIONS\nWe analyzed the use of multi-lingual sentence embeddings in se-\nquence labeling models to enable transfer across languages, juris-\ndictions, legal systems (common and civil law), and domains. We\ncreated a new type schema for functional segmentation of adjudi-\ncatory decisions and used it to annotate legal cases across eight\ndifferent contexts. We found that models generalize beyond the\ncontexts they were trained on and that training the models on\nmultiple contexts increases their robustness and improves the over-\nall performance when evaluating on previously unseen contexts.\nWe also found that pooling the training data of a model with data\nfrom additional contexts enhances its performance on the target\ncontext. The results are promising in enabling re-use of annotated\ndata across contexts and creating generalizable and robust models.\nWe release the newly created dataset (807 documents with 89,661\nannotated sentences), including the annotation schema and the\ncode used in our experiments, to the public.\n\nThis work suggests a promising path for the future of inter-\nnational collaboration in the field of AI & Law. While previous\nannotation efforts have typically been limited to a single context,\nthe experiments presented here suggest that researchers can work\ntogether by annotating cases from many different contexts at the\nsame time. Such a combined effort could aid researchers in creating\nmodels that perform well on the data from the context they care\nabout, while at the same time helping other groups train even better\nmodels for other contexts. We encourage these research directions\nand hope to form such collaborations under the Lex Rosetta project.\n\n10 FUTUREWORK\nThe application of multi-lingual sentence embeddings to functional\nsegmentation of case law across different contexts yielded promis-\ning results. At the same time, the work is subject to limitations\nand leaves much room for improvement. Hence, we suggest several\ndirections for future work:\n\n\u2022 Extension of the datasets from different contexts used in this\nwork beyond ~100 documents per context.\n\n\u2022 Annotation of data from contexts beyond the eight used here\n(multi-lingual models support close to 100 languages).\n\n\u2022 Analysis of automatic detection of Introductory Summary,\nHeadings, and Out of Scope.\n\n\u2022 Identification and investigation of other tasks applicable\nacross different contexts.\n\n\u2022 Evaluation of the application of other multilingual models\n(e.g., those mentioned in Section 2).\n\n\n\nICAIL\u201921, June 21\u201325, 2021, S\u00e3o Paulo, Brazil Savelka and Westermann, et al.\n\n\u2022 Exploring other transfer learning strategies beyond simple\ndata pooling, such as the framework proposed in [26].\n\n\u2022 Using multi-lingual models for annotation tasks with high-\nspeed annotation framework, such as [35, 36].\n\n\u2022 Performing the transfer across contexts with related (but\ndifferent) tasks, such as in [28].\n\n\u2022 Further exploring the differences in the distribution of the\nmultilingual embeddings for purposes of comparing and\nanalyzing domains, languages, and legal traditions.\n\nACKNOWLEDGMENTS\nHannes Westermann, Karim Benyeklef, and Kevin D. Ashley would\nlike to thank the Cyberjustice Laboratory at Universit\u00e9 de Montr\u00e9al,\nthe LexUM Chair on Legal Information and the Autonomy through\nCyberjustice Technologies (ACT) project for their support of this\nresearch. Kevin D. Ashley also thanks the Canadian Legal Infor-\nmation Institute for providing the corpus of legal cases. Matthias\nGrabmair thanks the SINC GmbH for supporting this research.\nJakub Hara\u0161ta and Tereza Novotn\u00e1 acknowledge the support of\nthe ERDF project \u201cInternal grant agency of Masaryk University\u201d\n(No. CZ.02.2.69\/0.0\/0.0\/19_073\/0016943).\n\nREFERENCES\n[1] Tommaso Agnoloni, Lorenzo Bacci, Enrico Francesconi, P Spinosa, Daniela Tis-\n\ncornia, Simonetta Montemagni, and Giulia Venturi. 2007. Building an ontological\nsupport for multilingual legislative drafting. Frontiers in Artificial Intelligence\nand Applications 165 (2007), 9.\n\n[2] Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively Multilingual\nNeural Machine Translation. In NAACL-HLT, Vol. 1 (Long and Short Papers).\n3874\u20133884.\n\n[3] Gianmaria Ajani, Guido Boella, Luigi Di Caro, Livio Robaldo, Llio Humphreys,\nSabrina Praduroux, Piercarlo Rossi, and Andrea Violato. 2016. The European\nLegal Taxonomy Syllabus: A multi-lingual, multi-level ontology framework to\nuntangle the web of European legal terminology. Applied Ontology 11, 4 (2016).\n\n[4] Sai Saket Aluru, Binny Mathew, Punyajoy Saha, and Animesh Mukherjee. 2020.\nDeep learning models for multilingual hate speech detection. arXiv preprint\narXiv:2004.06465 (2020).\n\n[5] Mikel Artetxe and Holger Schwenk. 2019. Massively multilingual sentence\nembeddings for zero-shot cross-lingual transfer and beyond. Transactions of the\nAssociation for Computational Linguistics 7 (2019), 597\u2013610.\n\n[6] Paheli Bhattacharya, Shounak Paul, Kripabandhu Ghosh, Saptarshi Ghosh, and\nAdam Wyner. 2019. Identification of rhetorical roles of sentences in Indian legal\njudgments. In JURIX 2019, Vol. 322. IOS Press, 3.\n\n[7] Guido Boella, Luigi Di Caro, Michele Graziadei, Loredana Cupi, Carlo Emilio\nSalaroglio, Llio Humphreys, Hristo Konstantinov, Kornel Marko, Livio Robaldo,\nClaudio Ruffini, et al. 2015. Linking legal open data: breaking the accessibility and\nlanguage barrier in european legislation and case law. In ICAIL 2015. 171\u2013175.\n\n[8] Paul Boniol, George Panagopoulos, Christos Xypolopoulos, Rajaa El Hamdani,\nDavid Restrepo Amariles, and Michalis Vazirgiannis. 2020. Performance in the\nCourtroom: Automated Processing and Visualization of Appeal Court Decisions\nin France. In Proceedings of the Natural Legal Language Processing Workshop 2020.\n\n[9] Karl Branting, Brandy Weiss, Bradford Brown, Craig Pfeifer, A Chakraborty, Lisa\nFerro, M Pfaff, and A Yeh. 2019. Semi-supervised methods for explainable legal\nprediction. In ICAIL 2019. 22\u201331.\n\n[10] Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase\nRepresentations using RNN Encoder-Decoder for Statistical Machine Translation.\nIn EMNLP 2014.\n\n[11] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning\nat Scale. In ACL 2020. 8440\u20138451.\n\n[12] Janez Dem\u0161ar. 2006. Statistical comparisons of classifiers over multiple data sets.\nJournal of Machine learning research 7, Jan (2006), 1\u201330.\n\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL 2019, Volume 1 (Long and Short Papers). 4171\u20134186.\n\n[14] Luca Dini, Wim Peters, Doris Liebwald, Erich Schweighofer, Laurens Mommers,\nand Wim Voermans. 2005. Cross-lingual legal information retrieval using a\n\nWordNet architecture. In ICAIL 2005. 163\u2013167.\n[15] Atefeh Farzindar and Guy Lapalme. 2004. LetSum, an Automatic Text Summa-\n\nrization system in Law field. JURIX 2004.\n[16] Jorge Gonz\u00e1lez-Conejero, Pompeu Casanovas, and Emma Teodoro. 2018. Business\n\nRequirements for Legal Knowledge Graph: the LYNX Platform.. In TERECOM@\nJURIX 2018. 31\u201338.\n\n[17] Jakub Hara\u0161ta, Jarom\u00edr \u0160avelka, Franti\u0161ek Kasl, and Jakub M\u00ed\u0161ek. 2019. Automatic\nSegmentation of Czech Court Decisions into Multi-Paragraph Parts. Jusletter IT\n4, M (2019).\n\n[18] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-termmemory. Neural\ncomputation 9, 8 (1997), 1735\u20131780.\n\n[19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n\n[20] Philipp Koehn, Alexandra Birch, and Ralf Steinberger. 2009. 462 Machine Trans-\nlation Systems for Europe. In Proceedings of the Twelfth Machine Translation\nSummit. Association for Machine Translation in the Americas, 65\u201372.\n\n[21] Guokun Lai, Barlas Oguz, Yiming Yang, and Veselin Stoyanov. 2019. Bridg-\ning the domain gap in cross-lingual document classification. arXiv preprint\narXiv:1909.07009 (2019).\n\n[22] Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk.\n2020. MLQA: Evaluating Cross-lingual Extractive Question Answering. In ACL\n2020. 7315\u20137330.\n\n[23] D.N. MacCormick, R.S. Summers, and A.L. Goodhart. 2016. Interpreting Precedents:\nA Comparative Study. Taylor & Francis.\n\n[24] Rohan Nanda, Llio Humphreys, Lorenzo Grossio, and Adebayo Kolawole John.\n2020. Multilingual Legal Information Retrieval System for Mapping Recitals and\nNormative Provisions. In Proceedings of Jurix 2020. IOS Press, 123\u2013132.\n\n[25] Alina Petrova, John Armour, and Thomas Lukasiewicz. 2020. Extracting Out-\ncomes from Appellate Decisions in US State Courts. In JURIX 2020. 133.\n\n[26] Jarom\u00edr \u0160avelka and Kevin D Ashley. 2015. Transfer of predictive models for\nclassification of statutory texts in multi-jurisdictional settings. In ICAIL 2015.\n216\u2013220.\n\n[27] Jaromir Savelka and Kevin D Ashley. 2018. Segmenting US Court Decisions into\nFunctional and Issue Specific Parts.. In JURIX 2018. 111\u2013120.\n\n[28] Jarom\u0131r \u0160avelka, Hannes Westermann, and Karim Benyekhlef. 2020. Cross-\nDomain Generalization and Knowledge Transfer in Transformers Trained on\nLegal Data. In ASAIL@ JURIX 2020.\n\n[29] P\u00e1rai Sheridan, Martin Braschlert, and Peter Schauble. 1997. Cross-language\ninformation retrieval in a Multilingual Legal Domain. In International Conference\non Theory and Practice of Digital Libraries. Springer, 253\u2013268.\n\n[30] Ralf Steinberger, Mohamed Ebrahim, Alexandros Poulis, Manual Carrasco-\nBenitez, Patrick Schluter, Marek Przybyszewski, and Signe Gilbro. 2014. An\noverview of the European Union\u2019s highly multilingual parallel corpora. Lan-\nguage Resources and Evaluation 48, 4 (2014), 679\u2013707.\n\n[31] Kyoko Sugisaki, Martin Volk, Rodrigo Polanco, Wolfgang Alschner, and Dmitriy\nSkougarevskiy. 2016. Building a Corpus of Multi-lingual and Multi-format Inter-\nnational Investment Agreements. In JURIX 2016.\n\n[32] Linyuan Tang and Kyo Kageura. 2019. VerifyingMeaning Equivalence in Bilingual\nInternational Treaties. In JURIX 2019. 103\u2013112.\n\n[33] Vern R Walker, Krishnan Pillaipakkamnatt, Alexandra M Davidson, Marysa\nLinares, and Domenick J Pesce. 2019. Automatic Classification of Rhetorical\nRoles for Sentences: Comparing Rule-Based Scripts with Machine Learning.. In\nASAIL@ ICAIL 2019.\n\n[34] Hannes Westermann, Jarom\u00edr \u0160avelka, and Karim Benyekhlef. 2021. Paragraph\nSimilarity Scoring and Fine-Tuned BERT for Legal Information Retrieval and\nEntailment. In New Frontiers in Artificial Intelligence (Lecture Notes in Computer\nScience). Springer International Publishing.\n\n[35] HannesWestermann, Jarom\u00edr \u0160avelka, Vern RWalker, Kevin D Ashley, and Karim\nBenyekhlef. 2019. Computer-Assisted Creation of Boolean Search Rules for Text\nClassification in the Legal Domain. In JURIX 2019, Vol. 322. IOS Press, 123.\n\n[36] HannesWestermann, Jarom\u00edr \u0160avelka, Vern RWalker, Kevin D Ashley, and Karim\nBenyekhlef. 2020. Sentence Embeddings and High-Speed Similarity Search for\nFast Computer Assisted Annotation of Legal Documents. In JURIX 2020, Vol. 334.\nIOS Press, 164.\n\n[37] Frank Wilcoxon. 1992. Individual comparisons by ranking methods. In Break-\nthroughs in statistics. Springer, 196\u2013202.\n\n[38] Huihui Xu, Jarom\u00edr \u0160avelka, and Kevin D Ashley. 2020. Using Argument Mining\nfor Legal Text Summarization. In JURIX 2020, Vol. 334. IOS Press.\n\n[39] Shudong Yang, Xueying Yu, and Ying Zhou. 2020. LSTM andGRUNeural Network\nPerformance Comparison Study: Taking Yelp Review Dataset as an Example. In\nIWECAI 2020. IEEE, 98\u2013101.\n\n[40] Vladimir Zhebel, Denis Zubarev, and Ilya Sochenkov. 2020. Different Approaches\nin Cross-Language Similar Documents Retrieval in the Legal Domain. In Interna-\ntional Conference on Speech and Computer. Springer, 679\u2013686.\n\n[41] Linwu Zhong, Ziyi Zhong, Zinian Zhao, Siyuan Wang, Kevin D Ashley, and\nMatthias Grabmair. 2019. Automatic summarization of legal decisions using\niterative masking of predictive sentences. In ICAIL 2019. 163\u2013172.\n\n\n\tAbstract\n\t1 Introduction\n\t1.1 Functional Segmentation\n\t1.2 Hypotheses\n\t1.3 Contributions\n\n\t2 Related Work\n\t3 Dataset\n\t4 Models\n\t5 Experimental Design\n\t5.1 Out-Context Experiment (H1)\n\t5.2 Pooled Out-Context Experiment (H2)\n\t5.3 Pooled With In-Context Experiment (H3)\n\n\t6 Evaluation Method\n\t7 Results\n\t7.1 Out-Context Experiment (H1)\n\t7.2 Pooled Out-Context Experiment (H2)\n\t7.3 Pooled With In-Context Experiment (H3)\n\n\t8 Discussion\n\t9 Conclusions\n\t10 Future Work\n\tAcknowledgments\n\tReferences\n\n","7":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n07\n87\n\n0v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n5 \nD\n\nec\n 2\n\n02\n1 Cross-Domain Generalization and\n\nKnowledge Transfer in Transformers\n\nTrained on Legal Data\n\nJarom\u0131\u0301r S\u030cAVELKA a,1, Hannes WESTERMANN b, and Karim BENYEKHLEF b\n\na School of Computer Science, Carnegie Mellon University\nb Cyberjustice Laboratory, Faculte\u0301 de droit, Universite\u0301 de Montre\u0301al\n\nAbstract. We analyze the ability of pre-trained language models to transfer knowl-\n\nedge among datasets annotated with different type systems and to generalize be-\n\nyond the domain and dataset they were trained on. We create a meta task, over mul-\n\ntiple datasets focused on the prediction of rhetorical roles. Prediction of the rhetor-\n\nical role a sentence plays in a case decision is an important and often studied task\n\nin AI & Law. Typically, it requires the annotation of a large number of sentences to\n\ntrain a model, which can be time-consuming and expensive. Further, the applica-\n\ntion of the models is restrained to the same dataset it was trained on. We fine-tune\n\nlanguage models and evaluate their performance across datasets, to investigate the\n\nmodels\u2019 ability to generalize across domains. Our results suggest that the approach\n\ncould be helpful in overcoming the cold-start problem in active or interactvie learn-\n\ning, and shows the ability of the models to generalize across datasets and domains.\n\nKeywords. Transfer learning, transformers, cross-domain generalization, case-law,\n\nrhetorical roles, document classification\n\n1. Introduction\n\nIn this paper we examine the ability of pre-trained language models to generalize beyond\n\nthe domain and dataset they were trained on and to transfer knowledge between datasets\n\nin the legal domain. We show that datasets created with different type systems, and in\n\ndifferent domains and jurisdictions can be utilized towards a common goal. As high-\n\nquality legal data sets are scarce, the efficient use of those that are available is of utmost\n\nimportance in AI & Law research. We explore if and how different datasets and type\n\nsystems can be re-cast in such a way that a language model fine-tuned on one (source\n\ndomain) could be successfully utilized on the other (target domain). We also explore if\n\nand how a language model could benefit from the availability of data from other datasets.\n\nWe address these questions by evaluating performance of language models fine-tuned\n\non three data sets of adjudicatory decisions coming from different countries and legal\n\ndomains. The three data sets are annotated in terms of rhetorical roles sentences play in a\n\ndecision, each of them using its own type system. We re-cast the tasks to a common type\n\nsystem, train models on the individual datasets and their combinations, and evaluate the\n\nmodels\u2019 performance.\n\n1E-mail: jsavelka@andrew.cmu.edu\n\nhttp:\/\/arxiv.org\/abs\/2112.07870v1\n\n\n2. Background\n\nWritten legal cases typically follow a certain pattern in their reasoning. For example,\n\nlegal cases often contain the following sections:\n\n1. Claims of the plaintiff and responses\/counter-claims of the defendant\n\n2. A description of factual circumstances of the case as seen by the parties and as\n\ndetermined by the court\n\n3. Legal rules applicable to the factual circumstances\n\n4. Application of the rules to the factual circumstances\n\n5. Court\u2019s conclusions and outcomes of the case\n\nThe sections play different roles in a decision, and could carry different meaning depen-\n\ndent on the context in which they are read. A judge, a law student, a party to a dispute, or\n\na legal professional could all read the different sections with different focus depending\n\non what they need to learn. It is an indispensable skill for any professional lawyer to\n\nquickly identify and understand the different sections of a case.\n\nDue to its prominence the task has attracted abundant research in AI & Law (see e.g.\n\n[1,26]. Many researchers have worked on automating the identification of the different\n\nsections, using sophisticated ML models, on a sentence, paragraph or other level. The\n\nautomatic segmentation of cases could serve as a reading aid for actors in the legal world,\n\nby automatically visualizing the different sections. It could also serve as a way to improve\n\nlegal search results, as users of legal databases could search in specific sections (such as\n\nthe application of a rule) to get more relevant results. Further, it serves as an important\n\nprerequisite for future research in AI & Law. Researchers could leverage the information\n\nabout the sections to understand their relevance for, e.g., prediction of the outcome of a\n\ncase or mining of reasoning patterns or stereotypical factual occurrences.\n\nTypically, a research in prediction of rhetorical roles includes dataset creation and\n\ntraining and evaluation of different ML models on that dataset. The dataset typically\n\nfocuses on one or several narrowly domains. The model is then tested against a test set,\n\ntypically sampled randomly from the dataset. This shows the predictive capability on test\n\ndata from the same distribution as the training data.\n\nIt can be difficult to evaluate how well a model would perform if applied to other\n\ndomains, and if it is able to generalize beyond the domains captured in the dataset. For\n\nexample, it is possible that a model has only learned a specific vocabulary characteris-\n\ntic of a single domain or jurisdiction to describe factual circumstances, rather than the\n\ngeneral idea of what a sentence describing the circumstances looks like.\n\nEvaluating the models on multiple datasets, however, is not straightforward. Firstly,\n\npublicly available datasets are scarce in AI & Law. Further, the datasets that are available\n\ntypically use custom type systems, making cross-dataset evaluation challenging. In this\n\npaper, we identify an example meta type system, that generalizes across three datasets.\n\nWe use the meta type system to train models on data from different domains and juris-\n\ndictions and assess performance of the models across the domains.\n\nWe compare the ability of different types of ML models to generalize from one do-\n\nmain to another. First, as a baseline we use a Support Vector Machines classifier which\n\nlearns the correlation between word and\/or n-gram occurrences and a certain label. Then,\n\nwe evaluate the more recently developed BERT model (bidirectional encoder represen-\n\ntation from transformers). This model has been pre-trained on massive corpora of text,\n\n\n\nto learn a language model. It can then be further fine-tuned on a down-stream task on\n\na specialized dataset, achieving strong results with little training data by leveraging the\n\npreviously learned language representation.\n\n3. Hypotheses\n\nBy conducting the experiments described in Section 5, we investigate the following hy-\n\npotheses:\n\n\u2022 H1 - The ML models are able to generalize the knowledge learned in one domain\n\nand apply it successfully in another domain.\n\n\u2022 H2 - The BERT model is able to leverage its understanding of language to ab-\n\nstract beyond the specific domain vocabulary, thereby generalizing better across\n\nthe domains than the SVM model.\n\n\u2022 H3 - Training the models on pooled data from multiple datasets improves perfor-\n\nmance and results in more robust models.\n\nShowing that the models generalize well among different domains could have impor-\n\ntant implications for the real-world applicability of the methods. Further, they could be\n\nemployed in active or interactive learning settings, where annotators are presented with\n\npredictions by the model and confirm\/correct these to create accurate models. This could\n\ndrastically lower the number of expensive annotations that need to be performed to start\n\nthe annotation of novel datasets.\n\n4. Related Work\n\nThe identification of rhethorical rules of sentences in legal cases has been investigated by\n\nseveral researchers. [22] used Conditional Random Field models with custom features to\n\npredict the rhethorical role of sentences in three distinct domains. [26] trained a number\n\nof machine learning models to predict the rhethorical role of sentences in decision from\n\nthe U.S. Board of Veteran Appeals and compared them to rule-based approaches. [32]\n\ncreated an interface to easily create boolean search rules for sentence classification. [1]\n\ncreated a dataset of decisions from the Indian Supreme Court decisions from 5 domains,\n\nand trained Hierarchical BiLSTM CRF models, achieving a macro F-1 score of up to\n\n0.82. These papers trained and evaluated the models on the same datasets. The contribu-\n\ntion in this paper is that we investigate the capability of models trained on one dataset\n\nto generalize to other datasets. Therefore, we re-cast three datasets (including the pub-\n\nlicly available datasets from [26] and [1]) into a meta type system. Then we investigate\n\nthe ability and respective performance of SVM and RoBERTa models trained on a sin-\n\ngle or multiple datasets to generalize to the other datasets. To our knowledge, training\n\nmodels for the classification of legal texts on one dataset and evaluating the performance\n\non other datasets has not been extensively investigated previously, and is thus a novel\n\ncontribution.\n\nThere are numerous examples of successful applications of BERT-based models on\n\nlegal texts. In [3] BERT is evaluated on classification of claim acceptance given judges\u2019\n\narguments. A task of retrieving related case-law similar to a case decision a user provides\n\n\n\nis tackled in [19]. The authors demonstrate the effectiveness of using BERT for this\n\ntask while focusing on mitigating the constraint on document length imposed by BERT.\n\nIn [2] BERT is evaluated as one of the approaches to predict court decision outcome\n\ngiven the facts of a case. BERT has been successfully used for classification of legal\n\nareas of Supreme Court judgments. [9] The authors of [16] combine BERT with simple\n\nsimilarity measure to tackle the challenging task of case law entailment. BERT was also\n\nused in learning-to-rank settings for retrieval of legal news [20] and case-law sentences\n\ninterpreting statutory concepts [21]. In [30], the researchers investigate the additional\n\nfine-tuning of language models on related tasks to improve performance in the analysis\n\nof legal entailment.\n\nWe show that models pre-trained on one dataset can to some extent perform predic-\n\ntions on other datasets. This could be used to bootstrap new datasets in other domains.\n\nThis follows a steady line of work in AI & Law on making annotation more effective.\n\nWestermann et al. [32] proposed and assessed a method for building strong, explainable\n\nclassifiers in the form of Boolean search rules. Employing an intuitive interface, the user\n\ndevelops Boolean rules for matching instead of annotating the individual sentences. In\n\n[31] a method for using pre-trained language models to identify semantically similar sen-\n\ntences suitable for annotation is proposed. S\u030cavelka and Ashley [24] evaluated the effec-\n\ntiveness of an approach where a user labels the documents by confirming (or correcting)\n\nthe prediction of a ML algorithm (interactive approach). The application of active learn-\n\ning has further been explored in the context of classification of statutory provisions [28]\n\nand eDiscovery [4,5,7].\n\n5. Experimental Design\n\nIn order to evaluate the ability of models to generalize beyond a single domain, we em-\n\nploy an experimental design consisting of several steps. First, we identify three datasets\n\ncontaining a categorization of sentences by rhetorical role (Section 5.1). Then, we iden-\n\ntify a meta type system that we can transform all the type systems into to create a task\n\nshared among the datasets (Section 5.2). We then fine-tune and evaluate a pre-trained\n\nlanguage model and a Support Vector Machine model (Section 5.4) on different combi-\n\nnations of these datasets (Sections 5.4 and 5.5).\n\n5.1. Data\n\nIn this work we utilize three datasets. The first one comes from [26]. The authors ana-\n\nlyzed 50 fact-finding decisions issued by the U.S. Board of Veterans\u2019 Appeals (\u201cBVA\u201d)\n\nfrom 2013 through 2017, all arbitrarily selected cases dealing with claims by veterans\n\nfor service-related post-traumatic stress disorder (PTSD). For each of the 50 BVA deci-\n\nsions in the PTSD dataset, the researchers extracted all sentences addressing the factual\n\nissues related to the claim for PTSD, or for a closely-related psychiatric disorder. These\n\nwere tagged with the rhetorical roles [27] the sentences play in the decision. These were\n\nFinding, Reasoning, Evidence, Legal Rule, and Citation. 2\n\nThe second dataset also focuses on rhetorical roles of sentences. Bhattacharya et\n\nal. [1] analyzed 50 opinions of the Supreme Court of India. The cases were sampled from\n\n2Dataset available at https:\/\/github.com\/LLTLab\/VetClaims-JSON\n\nhttps:\/\/github.com\/LLTLab\/VetClaims-JSON\n\n\nLabel BVA CB ISC Total\n\nFacts 2,420 (39%) 4,182 (42%) 2,219 (19%) 8,821 (32%)\n\nNon-Facts 3,733 (61%) 5,783 (58%) 9,380 (81%) 18,896 (68%)\n\nTotal 6,153 9,965 11,599 27,717\n\nTable 1. F1 scores for models trained on specific dataset training pools (rows) predicting on testing datasets\n\n(columns), compared between SVM and RoBERTa. Grey cells indicate that the training data includes the target\n\ndataset.\n\nfive different domains in proportion to their frequencies (criminal, land and property,\n\nconstitutional, labor and industrial, and intellectual property). The decisions were split\n\ninto 9,380 sentences and manually classified into one of the seven categories according\n\nto the rhetorical roles they play in a decision. These were Facts, Ruling (lower court),\n\nArgument, Ratio, Statute, Precedent, Ruling (present court).3\n\nWe also created a brand-new data set by scraping the case briefs from a publicly\n\navailable Case Brief Summary database.4 The case briefs were categorized in terms of\n\nthe areas of regulation, such as administrative law, business law, or criminal law (11 cat-\n\negories in total). In total, we were able to obtain 715 unique case briefs. The case briefs\n\nare structured into a number of sections with headings. We extract the sections based on\n\nan extensive battery of regular expressions to segment the retrieved briefs into individual\n\nsections. While there were over 100 unique section heading names we were able to iden-\n\ntify six main types to which we could map many of the different variations (e.g., all of\n\nLegal Issue, Issues, and Issue map to a single category). We applied a specialized legal\n\ncase sentence boundary detection system [23] to segment the sections into sentences.\n\nThis results in the dataset comprising 9,965 sentences, each with one of the six labels\n\ncorresponding to the section of the brief where the sentence occurred. These were Facts,\n\nIssue, Conclusion, Procedural History, Reasoning, and Rule.\n\n5.2. Task\n\nThe datasets described in 5.1 use different annotation type systems and stem from differ-\n\nent domains and decision makers. However, parts of the type systems overlap, making\n\nit possible to map certain types to a new meta type system that allows the training of\n\nmodels and evaluation of the trained models between datasets.\n\nIn order to map the source type systems into a single type system, we identify the\n\nlabel of the source datasets that\n\nThe meta type system establishes a binary classification whether a sentence states\n\nfactual circumstances of the case. We do not specify an explicit definition for such a\n\nsentence. Instead, we identify the best fitting types in the BVA, Case Briefs, and Indian\n\nSupreme Court datasets. BVA dataset contains sentences labeled as Evidence (2,420 out\n\nof 6,153). An Evidence sentence is defined as a sentence that primarily states the content\n\nof the testimony of a witness, states the content of documents introduced into evidence,\n\nor describes other evidence.5 The Case Briefs dataset does not come with a definition\n\nof the individual types. However, there is a large number of sentences coming from\n\na section commonly labeled as Facts (4,182 out of 9,965). The Indian Supreme Court\n\n3Dataset available at github.com\/Law-AI\/semantic-segmentation\n4http:\/\/www.casebriefsummary.com\/ , site unavailable as of 2020-11-29\n5https:\/\/github.com\/LLTLab\/VetClaims-JSON\n\ngithub.com\/Law-AI\/semantic-segmentation\nhttp:\/\/www.casebriefsummary.com\/\nhttps:\/\/github.com\/LLTLab\/VetClaims-JSON\n\n\ndataset contains sentences labeled as Facts (2,219 out of 11,599). A Facts sentence refers\n\nto the chronology of events that led to filing the case, and how the case evolved over time\n\nin the legal system (e.g., First Information Report at a police station, filing an appeal\n\nto the Magistrate). The task is then to predict if the sentence is of the focused type or\n\nnot. While the source type systems are different, and the definitions of the corresponding\n\ntypes are not the same, for the purposes of the training and evaluation we assume that the\n\nnew type system describes the same task when applied across the three datasets (i.e., we\n\nassume that we have transformed three related tasks into a unified one).\n\n5.3. Dataset splits\n\nWe randomly split all the data sets into the training, validation, and test sets using ratios\n\nof 50-25-25. The splitting for all four datasets is performed at the level of documents, i.e.,\n\nall the sentences from a single document are in the same fold. We did not take the size\n\nof a document into account, meaning that the number of sentences could vary slightly\n\nbetween datasets. The training split of the datasets is used for training the models, the\n\nvalidation split is used for model selection and hyperparameter optimization, while the\n\ntest set is used for final evaluation.\n\n5.4. Models\n\nIn this work, we use the RoBERTa (a robustly optimized BERT pretraining approach)\n\ndescribed in [11] as the starting point for our experiments.6 Out of the available models\n\nwe chose to work with the smaller roberta.base model that has 125 million parameters.\n\nThis choice was motivated by the ability to iterate the experiments faster when com-\n\npared to working with roberta.large with 355 million parameters. RoBERTa is using the\n\nsame architecture as BERT. However, the authors of [11] conducted a replication study\n\nof BERT pre-training and found that BERT was significantly undertrained. They used\n\nthe insights thus gained to propose a better pre-training procedure. Their modifications\n\ninclude longer training with bigger batches and more data, removal of the next sentence\n\nprediction objective, training on longer sequences on average (still limited to 512 tokens),\n\nand dynamic changing of the masking pattern applied to the training data. [11]\n\nAs baseline, we use a Support Vector Machine (SVM) classifier. SVM constructs a\n\nhyper-plane in a high dimensional space, which is used to separate the classes from each\n\nother. As an implementation of SVM we use the scikit-learn\u2019s Support Vector Classifica-\n\ntion module.7 As features we use the bag of words of (1-3)-grams weighted by TF-IDF.\n\n5.5. Experiments\n\nWe train the models on all the different possible pools of training data. The possible pools\n\nare BVA, CB, ISC, BVA+CB, BVA+ISC, CB+ISC, and BVA+CB+ISC. Both RoBERTa\n\nand SVM are trained on each of these pools separately, and the results are presented for\n\nprediction on the test split of each dataset (Section 5.5).\n\nIn all the experiments, we fine-tune the base RoBERTa model for 10 epochs on the\n\ntraining splits of the selected datasets. We use the batch size of 8 which is the maximum\n\n6github.com\/pytorch\/fairseq\/tree\/master\/examples\/roberta\n7scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html\n\ngithub.com\/pytorch\/fairseq\/tree\/master\/examples\/roberta\nscikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html\n\n\nallowed by our hardware setup (1080Ti with 11GB) given we set the length of a sequence\n\nto 512 (maximum). As optimizer we use Adam with initial learning rate set to 4e\u22125.\n\nWe store a model\u2019s checkpoint after the end of each training epoch. The checkpoints\n\nare evaluated on the corresponding validation set. The model with the highest F1 on\n\nthe validation set is then selected as the one to make predictions on the test sets. The\n\nperformance on the test sets is what we report in the results section.\n\nThe SVM is optimized via gridsearch over the few most important hyperparameters\n\n(C, class weight, number of iterations). The various hyperparameter combinations are\n\nevaluated against the validation dataset. The model with the highest F1 on the validation\n\nset is then selected as the one to make predictions on the test sets. The performance on\n\nthe test sets is what we report in the results section.\n\nFor evaluation we report the F1-measure (F1), i.e., the traditional information re-\n\ntrieval measures, to evaluate performance of the trained models. Since both the tasks are\n\nbinary the application of the measure is straightforward.\n\nP =\n\n|S|\n\n\u2211\ni=1\n\nT P\n\nT P+FP\nR =\n\n|S|\n\n\u2211\ni=1\n\nT P\n\nT P+FN\nF1 =\n\n2PR\n\nP+R\n\nIn the formulas, T P stands for true positives, FP for false positives, and FN for false\n\nnegatives.\n\n6. Results\n\n6.1. H1 - The ML models are able to generalize the knowledge learned in one domain\n\nand apply it successfully in another domain.\n\nThe results displayed in Table 2 confirm the hypothesis to some extent. Despite being\n\ntrained and evaluated on two different datasets, the performance of the models on the\n\ntarget datasets is decent. The RoBERTa model performs stronger than the SVM model\n\nacross the board, which is expected as RoBERTa is much more complex model (com-\n\npared to SVM) pre-trained for language understanding. Overall, the performance ap-\n\npears promising to mitigate the cold-start problem in active or interactive learning. The\n\nRoBERTa model trained on the CB dataset achieves the highest average performance\n\nacross the datasets with F1 = 0.75 on average, underperforming the dataset dedicated\n\nmodels by a rather small margin (difference of 0.1 for BVA, and 0.02 for ISC). This\n\nmight be due to the size and thematic breadth of this dataset. The models trained on\n\nBVA and ISC also achieve strong score (0.70 on BVA and 0.64 on ISC on averages for\n\nRoBERTa). Interestingly, the SVM trained on the BVA dataset performs the strongest,\n\nwith an average performance of .68 across all the datasets. This seems to be inflated due\n\nto the high performance on the BVA dataset, however. The SVM further performs much\n\nworse when trained on ISC, which is likely due to the ISC dataset being much more in-\n\nbalanced than the other target datasets. Curiously, the SVM model even performs better\n\non the ISC dataset when trained with BVA or CB rather than ISC itself.\n\n\n\nBVA CB ISC Avg\n\nSVM RoBERTa SVM RoBERTa SVM RoBERTa SVM RoBERTa\n\nBVA .92 .94 .67 .71 .44 .44 .68 .70\n\nCB .60 .84 .78 .83 .50 .57 .63 .75\n\nISC .11 .67 .19 .65 .41 .59 .24 .64\n\nTable 2. F1 scores for models trained on specific dataset training pools (rows) predicting on testing datasets\n\n(columns), compared between SVM and RoBERTa. Grey cells indicate that the training data includes the target\n\ndataset.\n\n6.2. H2 - The BERT model is able to leverage its understanding of language to abstract\n\nbeyond the specific domain vocabulary, thereby generalizing better across the\n\ndomains than the SVM model.\n\nThe RoBERTa model appears to transfer the knowledge learned on any one dataset to\n\nprediction on the other two much better than the baseline SVM model (Table 2). For\n\nexample, RoBERTa trained on CB performs at F1 = 0.84 on BVA. This is a sizeable\n\nperformance hit when compared to RoBERTa trained on BVA (F1 = 0.94). However, the\n\nSVM model goes from F1 = 0.92 to F1 = 0.60. While it is customary for BERT models to\n\nperform better than traditional ML models, the sizeable difference of .24 in this instance\n\nmight indicate that there is a qualitatively different understanding going on, as RoBERTa\n\ncan understand the semantic meaning rather than the specific vocabulary used. This is\n\nvery promising for the possibility of creating robust models that can generalize between\n\ndomains.\n\n6.3. H3 - Training the models on pooled data from multiple datasets improves\n\nperformance and results in more robust models.\n\nThe first part of this hypothesis is not confirmed by our experiments. As can be seen\n\nin Table 3 (as compared to Table 2), performance does not clearly improve for models\n\ntrained on multiple datasets. However, in isolated instances the performance does seem\n\nto slighlty improve when trained on additional datasets. For example, training the model\n\non CB and ISC to predict ISC seems to slightly improve the performance of RoBERTa\n\nas compared to the model only trained on ISC (.59 \u2192 .60), while significantly improving\nthe performance of the SVM (.41 \u2192 .53). It is possible that larger datasets could further\nimprove the performance.\n\nAs for the part of the hypothesis concerning robustness, the results suggest that\n\nRoBERTa is much better at handling data coming from different datasets. It appears that\n\nwhen the models are trained on other datasets in addition to the target dataset, the per-\n\nformance matches the model trained only on the target dataset. This is not the case for\n\nthe SVM baseline (e.g., consider the model trained on CB+ISC data applied to CB). This\n\nis very important as it allows training of the model on multiple domains without any\n\ndegradation in performance on those tasks. The result is a model that performs well on\n\nall three datasets. Such a model would likely generalize better to other domains (not con-\n\nsidered here) as compared to the model trained exclusively on one of the three datasets.\n\nThis robustness can be illustrated by the fact that combining data from the two non-target\n\ndatasets seems to yield higher performance on the target datasets than training just on\n\neither of the non-target datasets. For example, training the RoBERTa model on both CB\n\n\n\nBVA CB ISC\n\nSVM RoBERTa SVM RoBERTa SVM RoBERTa\n\nBVA+CB .91 .93 .78 .84 .41 .55\n\nBVA+ISC .91 .93 .46 .75 .46 .59\n\nCB+ISC .55 .85 .71 .82 .53 .60\n\nBVA+CB+ISC .90 .93 .68 .82 .50 .59\n\nTable 3. F1-scores for models trained on specific dataset training pools (rows) predicting on testing datasets\n\n(rows), compared between SVM and RoBERTa.\n\nand ISC yields higher performance when predicting BVA (.85) than using either CB (.84)\n\nor ISC (.67) on their own.\n\n7. Discussion\n\nThe results reveal several interesting properties of the evaluated models. First of all, they\n\nshow that it is indeed possible to train a model on one domain, and use it for predic-\n\ntion on another domain with decent performance. This holds especially for the powerful\n\nRoBERTa model. It should be noted that the models in 2 have never seen any sample\n\nfrom the target dataset, except when the source dataset and the target dataset are the\n\nsame, and still achieve strong results. This is promising for multiple reasons. It shows\n\na clear path towards using a model trained on one dataset to bootstrap a new dataset\n\nfor active or interactive learning purposes. It also shows that the tasks defined in terms\n\nof their respective type systems are related, despite being created in different contexts\n\nand jurisdictions. The relatedness enables ML models to be successfully applied across\n\ndomains.\n\nFurther, several interesting observations can be made when comparing the per-\n\nformance of the SVM and RoBERTa model. There are several instances where the\n\nRoBERTa model significantly outperforms the SVM model. The difference in results\n\ncould potentially be explained by the RoBERTa model being able to abstract beyond the\n\nvocabulary used, to grasp the meaning and semantic type of a sentence, by leveraging\n\nthe language model learned during the pre-training on general language data. The SVM\n\nmodel, on the other hand, only has access to the language of the specific dataset used for\n\ntraining. Of course, further study would be required to verify this, but the difference in\n\nresults is staggering, up to 0.5 in F1 scores on some datasets.\n\nFinally, the RoBERTa models trained on multiple datasets seem to retain high per-\n\nformance on the specific datasets they are trained on, while also improving performance\n\non datasets they were not trained on. This is another promising result showing that the\n\nmodels can learn patterns even from datasets created in different contexts and jurisdic-\n\ntions, and use the additional data to create robust and strong decision rules.\n\n8. Future Work\n\nWe observed that it is possible to train a model on one dataset, and use it for predic-\n\ntion on another one with decent performance. This could be utilized for bootstrapping\n\na strong active or interactive learning model and, hence, solving the cold-start problem.\n\n\n\nFor future, we plan to validate this assumption and assess the benefit of the technique to\n\nthe down-stream task (i.e., the annotation supported with active learning).\n\nIn this work, we use three datasets supporting related tasks. We performed a simple\n\ntransformation on those tasks to unify their label spaces. There is a plenty of space to\n\nexperiment with different transformations, i.e., tasks. Also, there is a potential to explore\n\nother existing datasets looking for those that support other types of related tasks (e.g.,\n\ndecision outcome prediction). It might be particularly interesting to look into the options\n\nto work with documents in different languages.\n\nFinally, we used the base version of RoBERTa for our experiments. It is worth noting\n\nthat this model, nor its large sibling are considered to be the most up-to-date state-of-\n\nthe-art as of the time this paper is being published. Because of the promising results it is\n\nwarranted to analyze how much improvement could one get by utilizing bigger and more\n\npowerful models.\n\n9. Conclusion\n\nIn this paper, we have provided an example of how datasets created in different con-\n\ntexts can be re-cast to support the same task, and utilized towards a common goal. We\n\nshowed an advantage in performance of pre-trained language models when applied to\n\ndomains different from the one they were trained on, compared to SVM models. The\n\npre-trained language models seem to be able to understand the underlying idea behind a\n\ntask to a larger extent than SVM models. Further, training the models on several domains\n\nimproved robustness and to some extent performance. Overall, these results suggest that\n\npre-trained language models have several desirable properties when training on multiple\n\ndatasets, making them an ideal tool for efficient utilization of the existing AI & Law\n\ndatasets to support future research in novel tasks.\n\nReferences\n\n[1] Bhattacharya, P., S. Paul, K. Ghosh, S. Ghosh, and A. Wyner. \u201cIdentification of Rhetorical Roles of\n\nSentences in Indian Legal Judgments.\u201d arXiv preprint arXiv:1911.05405 (2019).\n\n[2] Chalkidis, Ilias, Ion Androutsopoulos, and Nikolaos Aletras. \u201cNeural legal judgment prediction in en-\n\nglish.\u201d arXiv preprint arXiv:1906.02059 (2019).\n\n[3] Condevaux, Charles, et al. \u201cWeakly Supervised One-Shot Classification Using Recurrent Neural Net-\n\nworks with Attention: Application to Claim Acceptance Detection.\u201d JURIX. 2019.\n\n[4] Cormack, G., and M. Grossman. \u201cScalability of continuous active learning for reliable high-recall text\n\nclassification.\u201d In Proc. 25th ACM Int\u2019l Conf. on Info. & Knowledge Management, pp. 1039-1048. 2016.\n\n[5] Cormack, G., and M. Grossman. \u201cAutonomy and reliability of continuous active learning for technology-\n\nassisted review.\u201d arXiv preprint arXiv:1504.06868 (2015).\n\n[6] Devlin, Jacob, et al. \u201cBert: Pre-training of deep bidirectional transformers for language understanding.\u201d\n\narXiv preprint arXiv:1810.04805 (2018).\n\n[7] Hogan, C., R. Bauer, and D. Brassil. \u201cHuman-aided computer cognition for e-discovery.\u201d In Proc. 12th\n\nInt\u2019l Conf. on Artificial Intelligence and Law, pp. 194-201. 2009.\n\n[8] Howard, Jeremy, and Sebastian Ruder. \u201cUniversal language model fine-tuning for text classification.\u201d\n\narXiv preprint arXiv:1801.06146 (2018).\n\n[9] Howe, Jerrold Soh Tsin, Lim How Khang, and Ian Ernst Chai. \u201cLegal area classification: a comparative\n\nstudy of text classifiers on Singapore supreme court judgments.\u201d arXiv:1904.06470 (2019).\n\n[10] Lan, Zhenzhong, et al. \u201cAlbert: A lite bert for self-supervised learning of language representations.\u201d\n\narXiv preprint arXiv:1909.11942 (2019).\n\nhttp:\/\/arxiv.org\/abs\/1911.05405\nhttp:\/\/arxiv.org\/abs\/1906.02059\nhttp:\/\/arxiv.org\/abs\/1810.04805\nhttp:\/\/arxiv.org\/abs\/1801.06146\nhttp:\/\/arxiv.org\/abs\/1904.06470\nhttp:\/\/arxiv.org\/abs\/1909.11942\n\n\n[11] Liu, Yinhan, et al. \u201cRoberta: A robustly optimized bert pretraining approach.\u201d arXiv preprint\n\narXiv:1907.11692 (2019).\n\n[12] Mikolov, Tomas, et al. \u201cDistributed representations of words and phrases and their compositionality.\u201d\n\nAdvances in neural information processing systems. 2013.\n\n[13] Mikolov, Tomas, et al. \u201cEfficient estimation of word representations in vector space.\u201d arXiv preprint\n\narXiv:1301.3781 (2013).\n\n[14] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. \u201cGlove: Global vectors for word\n\nrepresentation.\u201d EMNLP. 2014.\n\n[15] Peters, Matthew E., et al. \u201cSemi-supervised sequence tagging with bidirectional language models.\u201d arXiv\n\npreprint arXiv:1705.00108 (2017).\n\n[16] Rabelo, Juliano, Mi-Young Kim, and Randy Goebel. \u201cCombining similarity and transformer methods\n\nfor case law entailment.\u201d ICAIL. 2019.\n\n[17] Radford, Alec, et al. \u201cImproving language understanding by generative pre-training.\u201d (2018): 12.\n\n[18] Raffel, Colin, et al. \u201cExploring the limits of transfer learning with a unified text-to-text transformer.\u201d\n\narXiv preprint arXiv:1910.10683 (2019).\n\n[19] Rossi, Julien, and Evangelos Kanoulas. \u201cLegal Search in Case Law and Statute Law.\u201d JURIX. 2019.\n\n[20] Sanchez, Luis, et al. \u201cEasing Legal News Monitoring with Learning to Rank and BERT.\u201d European\n\nConference on Information Retrieval. Springer, Cham, 2020.\n\n[21] Savelka, Jaromir. Discovering sentences for argumentation about the meaning of statutory terms. Diss.\n\nUniversity of Pittsburgh, 2020.\n\n[22] Savelka, J., and Kevin D. Ashley. \u201cUsing CRF to detect different functional types of content in decisions\n\nof united states courts with example application to sentence boundary detection.\u201d ASAIL 2017.\n\n[23] Savelka, J., Walker, V. R., Grabmair, M., & Ashley, K. D. (2017). Sentence boundary detection in adju-\n\ndicatory decisions in the united states. Traitement automatique des langues, 58, 21.\n\n[24] Savelka, J., G. Trivedi, and K. Ashley. \u201cApplying an interactive machine learning approach to statutory\n\nanalysis.\u201d In Proc. 28th Ann. Conf. on Legal Knowledge & Info. Systems (JURIX\u201915). IOS Press. 2015.\n\n[25] Vaswani, Ashish, et al. \u201cAttention is all you need.\u201d Advances in neural information processing systems.\n\n2017.\n\n[26] Walker, Vern R., et al. \u201cAutomatic Classification of Rhetorical Roles for Sentences: Comparing Rule-\n\nBased Scripts with Machine Learning.\u201d Proceedings of ASAIL 2019 (2019).\n\n[27] Walker, Vern R., et al. \u201cSemantic types for computational legal reasoning: propositional connectives and\n\nsentence roles in the veterans\u2019 claims dataset.\u201d Proceedings of ICAIL \u201917. ACM, 2017.\n\n[28] Waltl, B., J. Muhr, I. Glaser, G. Bonczek, E. Scepankova, and F. Matthes. \u201cClassifying Legal Norms\n\nwith Active Machine Learning.\u201d In JURIX, pp. 11-20. 2017.\n\n[29] Wang, Wei, et al. \u201cStructbert: Incorporating language structures into pre-training for deep language\n\nunderstanding.\u201d arXiv preprint arXiv:1908.04577 (2019).\n\n[30] Westermann, H, J. Savelka and K. Benyekhlef. \u201cParagraph Similarity Scoring and Fine-Tuned BERT\n\nfor Legal Information Retrieval and Entailment.\u201d COLIEE. 2020.\n\n[31] Westermann, H., J. Savelka, V. Walker, K. Ashley, and K. Benyekhlef. \u201cSentence Embeddings and High-\n\nspeed Similarity Search for Fast Computer Assisted Annotation of Legal Documents.\u201d In JURIX. 2020.\n\n[32] Westermann, H., J. Savelka, V. Walker, K. Ashley, and K. Benyekhlef. \u201cComputer-Assisted Creation of\n\nBoolean Search Rules for Text Classification in the Legal Domain.\u201d In JURIX, pp. 123-132. 2019.\n\nhttp:\/\/arxiv.org\/abs\/1907.11692\nhttp:\/\/arxiv.org\/abs\/1301.3781\nhttp:\/\/arxiv.org\/abs\/1705.00108\nhttp:\/\/arxiv.org\/abs\/1910.10683\nhttp:\/\/arxiv.org\/abs\/1908.04577\n\n","8":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Evidence Collection for Fake News Detection\n\nMrinal Rawat\nUpGrad Education Pvt. Ltd., Mumbai, India.\n\nLiverpool John Moores University,\nLiverpool, United Kingdom.\n\nrawatmrinal06@gmail.com\n\nDiptesh Kanojia\nCentre for Translation Studies,\n\nUniversity of Surrey,\nSurrey, United Kingdom.\n\nd.kanojia@surrey.ac.uk\n\nAbstract\n\nFake news, misinformation, and unverifi-\nable facts on social media platforms propa-\ngate disharmony and affect society, especially\nwhen dealing with an epidemic like COVID-\n19. The task of Fake News Detection aims to\ntackle the effects of such misinformation by\nclassifying news items as fake or real. In this\npaper, we propose a novel approach that im-\nproves over the current automatic fake news\ndetection approaches by automatically gather-\ning evidence for each claim. Our approach\nextracts supporting evidence from the web ar-\nticles and then selects appropriate text to be\ntreated as evidence sets. We use a pre-trained\nsummarizer on these evidence sets and then\nuse the extracted summary as supporting evi-\ndence to aid the classification task. Our exper-\niments, using both machine learning and deep\nlearning-based methods, help perform an ex-\ntensive evaluation of our approach. The results\nshow that our approach outperforms the state-\nof-the-art methods in fake news detection to\nachieve an F1-score of 99.25 over the dataset\nprovided for the CONSTRAINT-2021 Shared\nTask. We also release the augmented dataset,\nour code and models 1 for any further research.\n\n1 Introduction\n\nThe ability to consume readily available informa-\ntion from the internet is alarming for both indi-\nviduals and organizations. The quality of content\non social media platforms has been significantly\naffected due to the spread of fake news, misinfor-\nmation and unverifiable facts. The current tally\nof internet users stands at 4.66 billion2 (Kemp,\n2015); and many of these users generate, post\nand consume content without any regulation, in\n\n1https:\/\/github.com\/rawat-mrinal06\/\nfake_news\n\n2Internet Live Stats (as on 15-07-2021)\n\na large number of countries3. Due to the unre-\nstricted nature of online platforms, there is a sig-\nnificant increase in the amount of misinformation\non social media (Allen et al., 2020), especially in\ndeveloping nations (Badrinathan, 2020; Wasser-\nman and Madrid-Morales, 2019). Studies show\nthat events such as the presidential election of the\nUnited States in 2016 were affected due to moder-\nated fake news campaigns (Tavernise, 2016). Shu\net al.(2017) (Shu et al., 2017) propose that fake\nnews is intentionally written, verifiably false, and\nis created in a way that makes it look authentic.\nManual efforts by other online platforms such as\nPoynter4, FactCheck5, AltNews6 etc. to detect fake\nnews, requires a lot of human effort and can prove\nto be cumbersome. Such manual efforts can be\ntime-consuming, challenging, and at times, can\nalso be ineffective as fake news can spread faster\nthan verified claims over social media platforms.\n\nAutomatic Fake News Detection is a task that\naims to mitigate the problem of misinformation\nwith the help of evidence supported by various\nsources. Most of the approaches in this recently\ndevised task aim to use the classical machine\nlearning-based methods or the recent deep\nlearning-based methods to help classify news\nitems as fake or as real. Initially proposed methods\nfor the task applied machine learning-based\ntechniques but cited insufficient data as a major\nconcern (Vlachos and Riedel, 2014). Recent\ndeep learning and ensemble approaches (Malon,\n2018; Roy et al., 2018) were proposed on the\nFEVER (Thorne et al., 2018a) and LIAR (Wang,\n2017) datasets, and have been shown to perform\nvery well. Studies have proposed a combination\nof evidence detection with textual entailment\n\n3Internet Censorship in Countries\n4Poynter: Online\n5FactCheck: Online\n6AltNews: Online\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n06\n50\n\n7v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n3 \nD\n\nec\n 2\n\n02\n1\n\nhttps:\/\/github.com\/rawat-mrinal06\/fake_news\nhttps:\/\/github.com\/rawat-mrinal06\/fake_news\nhttps:\/\/www.internetlivestats.com\/\nhttps:\/\/en.wikipedia.org\/wiki\/Internet_censorship_and_surveillance_by_country\nhttps:\/\/www.poynter.org\/\nhttps:\/\/www.factchecker.in\/\nhttps:\/\/www.altnews.in\/\n\n\nconcerning the claim (Vijjali et al., 2020). FEVER\nShared Tasks (Thorne et al., 2018b, 2019) have\nhelped the automatic fact verification task gather\nattention towards the problem and helped generate\napproaches to mitigate the issues with previously\nproposed solutions. This study shows that our\nnovel approach improvises over state-of-the-art\napproaches and helps detect fake news related to\nCOVID-19. Our approach performs web-search\nfor evidence collection and uses BERT-score\nsimilarity to match the unverified claim with the\ntop-k searches. Further, we propose the use of\nsummarization to mitigate problems with the\nevidence collection. We summarize the top-n\nselected lines from these articles and use them as\nevidence to support or reject the news item claim.\nOur experiments perform an extensive evaluation\nof the approach over the datasets released as a part\nof the CONSTRAINT-2021 Shared Task (Patwa\net al., 2020) shared task.\n\nOur summarized contributions with this pa-\nper are:\n\n\u2022 We propose a novel approach to help automate\nthe evidence collection for any fake news de-\ntection dataset.\n\n\u2022 Additionally, we incorporate a summarization\ncomponent that helps outperform the state-of-\nthe-art approaches for automatic fact verifica-\ntion on the CONSTRAINT-2021 dataset.\n\n2 Related Work\n\nAutomatic detection and classification of fake news,\nespecially in epidemic situations like COVID-19,\nis a significant issue for society. Most of the re-\ncent works have identified that fake news is written\nintentionally and factually false (Shu et al., 2017).\nSeveral datasets have been released for the AI com-\nmunity in the field of fake news detection, such\nas LIAR (Wang, 2017), Fake News Challenge-1,\nand FEVER (Thorne et al., 2018a). Some recent\ntechniques extract the evidence from Wikipedia to\nclassify a claim as SUPPORTED, REFUTED or\nNOTENOUGHINFO (Thorne et al., 2018a). They\nformulate the problem as a three-step process (i)\nfirst the top-k documents are identified based on the\nTF-IDF based approaches (ii) then top-k sentences\nare identified from the documents, and (iii) finally\nthe textual entailment based approaches (Parikh\net al., 2016) are used to classify the claim. Team\n\nPapelo (Malon, 2018) used the Transformer-based\napproach for the textual entailment and selected\nthe evidence-based on tf-idf and entities present\nin the title. Hanselowski et al. (2018) selects the\ndocuments and sentence using the entity mentions\nand recognizes textual entailment using Enhanced\nSequential Inference Model (ESIM) (Chen et al.,\n2017). Despite the several attempts, fake news de-\ntection is a challenging problem and countering\nfake news is a typical issue that requires continu-\nous studies. Recently, some researchers released\nthe datasets related to COVID-19 fake news detec-\ntion. Shahi and Nandini (2020) (Shahi and Nandini,\n2020) proposed the first multi-lingual cross-domain\ndataset for COVID-19 that consists of 5182 fact-\nchecked news articles from Jan-2020 to May-2020.\nThey collected data from 92 different websites and\nmanually classified them into 23 classes. Kar et\nal. (Kar et al., 2020) also released a multi-indic-\nlingual dataset besides English to detect the fake\nnews in social media tweets. They obtained 480\ntweets in Bengali and 460 tweets in Hindi. In addi-\ntion to tweets, they also included several features\nrelated to tweets such as retweet count, favourite\ncount, total URL in description, URL, friend list,\nfollowers, etc. Recently, a very relevant dataset was\nreleased by Patwa et al. (Patwa et al., 2020) which\nconsists of 10,700 tweets or claim collected from\nvarious sources such as Twitter, PolitiFact, Snopes,\nBoomlive. They experimented with various ma-\nchine learning techniques like Decision Trees, Lo-\ngistic Regression, SVM, Gradient Boosting DT and\nachieved the F1-score of 93.32. Most of the pre-\nvious work on COVID-19 dataset proposed an en-\nsemble approach of various models such as BERT,\nRoBERTa, XLNet, etc. (Shifath et al., 2021; Raha\net al., 2021; Shushkevich and Cardiff, 2021). Chen\net al. (Chen et al., 2021) trained the model with\nadditional words such as covid-19, coronavirus,\npandemic, indiafightscorona since the BERT tok-\nenizer will split these words into separate tokens.\nSome works leveraged the fine-tuned models like\nCOVID-Twitter-BERT (CT-BERT) (Mu\u0308ller et al.,\n2020) and demonstrated a boost in performance\n(Li et al., 2021; Glazkova et al., 2020; Wani et al.,\n2021). The fake news detection methods described\nabove mainly uses the claim for the classification.\nOur method focuses on extracting and summarizing\nthe evidence from the external source and uses it to\nclassify the claim on the COVID-19 fake news de-\ntection dataset (Li et al., 2021; Patwa et al., 2020).\n\n\n\nSplit Real Fake Total\nTraining 3360 3060 6420\nValidation 1120 1020 2140\nTest 1120 1020 2140\nTotal 5600 5100 10700\n\nTable 1: Dataset Statistics\n\n3 Dataset\n\nFor our work, we use the pre-released COVID-19\nfake news dataset as a part of the CONSTRAINT-\n2021 shared task (Patwa et al., 2020). This gold-\nstandard manually annotated dataset comprises so-\ncial media posts and articles which are related to\nCOVID-19. Each post or tweet contains content in\nthe English language and is classified in either of\nthe two categories- (1) Real: where tweets or arti-\ncles which are factually correct and verified from\nauthentic sources, for example, \u201cWearing mask can\nprotect you from the virus. (Twitter)\u201d; or (2) Fake:\nwhere tweets or posts related to COVID-19 which\nare factually incorrect and verified as false, for ex-\nample, \u201cIf you take Crocin thrice a day you are\nsafe. (Facebook)\u201d.\n\nThe authors collect fake news from two different\nsources- social media platforms and public fact-\nchecking platforms. The social media posts in-\nclude text from Facebook posts, Instagram posts,\nand Twitter posts, whereas the fact-checking web-\nsites such as PolitiFact, Snopes, and Boomlive are\nused to collect fact-checked news items. To further\ncollect real news, they sample tweets from official\ngovernment channels, news channels, and medical\ninstitutes. Overall, a total of 14 such sources were\nused to prepare this dataset.\n\nThe dataset comprises 10700 manually anno-\ntated samples and is split into (60%) train, (20%)\nvalidation and (20%) test sets. We provide the\nexact numbers for each split\/class in Table 1 for\nclarity. The dataset is class-balanced as it contains\n52.3% samples of real posts and 47.7% samples\nof fake posts. As an analysis on it, we obtained a\nword-cloud illustration for both real and fake sam-\nples and observed a high lexical overlap between\nboth the classes, where words like \u2018coronavirus\u2019,\n\u2018covid19\u2019, \u2018people\u2019, \u2018cases\u2019, \u2018number\u2019, \u2018test\u2019, etc.\nare repeatedly used in both the sets. We do not\nshow the word cloud due to space constraints. We\ncreate and present a wordcloud for the dataset in\nFigure 1.\n\n(a) Worcloud of Real Posts (b) Worcloud of Fake Posts\n\nFigure 1: Wordcloud of real\/fake posts in our dataset.\n\n4 Our Approach\n\nIn this section, we provide details of our novel ap-\nproach to augment the dataset with evidence from\nweb search and the use of this evidence to comple-\nment the task of fact verification. The algorithm for\nour approach can be seen in Algorithm 1. As dis-\ncussed above, we collect this evidence and prune\nto top-k related news items based on semantic sim-\nilarity via BERTScore (Devlin et al., 2019). We\nalso select top-n lines from each article for further\nbuilding an evidence repository, as detailed below\nin further subsections.\n\n4.1 Evidence Collection\n\nIn the original dataset of COVID-19, evidence is\nnot released along with the claim. We hypothe-\nsize that evidence is equally relevant to classify the\nclaim as proposed by Thorne et al. (2018a). As\nper our approach, given a claim or post text, we\nfirst select K relevant articles using a BERT-based\nsentence similarity score as detailed here; and can\nbe seen as an architecture component in Figure 2.\n\n4.1.1 Article Retrieval\nFor each claim c, we search the claim as a query\nusing a publicly available search API. The response\nreturned by this API consists of (heading,\ntext) pairs. We use the spacy (Honnibal et al.,\n2020) library to get the similarity score of response\ntext with respect to the input claim. Based on this\nsimilarity score, we select top K results that have\nthe similarity score greater than 0.77. While se-\nlecting documents, we prune for webpages in other\nlanguages and pages which are direct links to PDF\nor other such non-text files. As an immediate next\nstep, we scrape the selected web pages to obtain\nthe matching N sentences concerning the claim as\ndetailed here.\n\n7Selected with empirical evaluation and manual analysis\nafter trying 0.5, 0.6, 0.7, 0.8 using the \u2019en nli roberta base\u2019\nmodel for document similarity\n\n\n\nFigure 2: Full architecture of our proposed approach.\n\nAlgorithm 1: Algorithm to collect the evi-\ndence from the input claim\nInput: Claim c, Blocked URLs u\nOutput: Evidence e\n\n1 Function ArticleRet(c, k = 3):\n2 results\u2190 GoogleSearch(c);\n\nfiltered results\u2190 \u2205; foreach\nri \u2208 results do\n\n3 if ri \/\u2208 u then\n4 si \u2190 Similarity(c, ri);\n\n\/\/ Document Similarity\n\nusing spacy library\n\n5 if si > 0.7 then\n6 filtered results\u2190\n\n(ri, si)\n\n7 filtered results\u2190\nSort(filtered results)[:k];\nreturn filtered results;\n\n8 articles\u2190 ArticleRet(c);\n9 e\u2190 \u2205; \/\/ Evidences\n\n10 foreach ai \u2208 articles do\n11 d\u2190WebsiteData(ai.url);\n\nsents\u2190 d[\u2032< h >\u2032] + d[\u2032< p >\u2032];\n\/\/ Extract <p> and <h> tags\n\nfrom html\n\n12 foreach si \u2208 sents do\n13 sim\u2190 Similarity(c, si); if\n\nsim > 0.5 then\n14 e\u2190 (si, sim);\n\n15 e\u2190 Sort(e)[:3];\n16 return e;\n\n4.1.2 Sentence(s) Retrieval\n\nIn the previous step, we extract the relevant arti-\ncle URLs U = (u1, u2, u3). We employ a similar\nmethod to find the sentences within each article.\nFor every url u, we first scrape the webpage and\nextract the text from < h > and < p > tags. Fur-\nther, we use the same similarity score to select\nthe top N sentences with respect to the claim. We\nobtain a similarity threshold of 0.5 after perform-\ning a similar empirical evaluation as mentioned in\nthe footnote. Eventually, we concatenate the se-\nlected sentences from these articles, which act as\nour evidence for the claim. We would like to note\nthat increasing the threshold significantly higher\nreturned an empty set in some case and hence we\nchoose a relatively lower threshold (0.5).\n\nAn example of evidence collected via our ap-\nproach is shown in Table 2 where the column titled\n\u201cEvidence\u201d shows the output after these steps.\n\n4.2 Dataset Preprocessing\n\nTo map claims with evidence, we pre-process both\nthe dataset and the evidence collected from exter-\nnal sources. Following are the details of the pre-\nprocessing steps: (1) URL Mapping: We observe\nthat some posts contain URLs in a masked form,\ne.g., https:\/\/t.co\/z5kk XpqkYb. Our approach ex-\ntracts these URLs using a regular expression-based\nmatch and maps them to the original URL using\nthe python \u2018requests\u2019 library. Any additional in-\nformation from the URL is removed, and only ap-\npropriate URLs remain in the text. For example,\nhttps:\/\/t.co\/z5kkXpqkYb \u2212\u2192 https:\/\/www.cdc.gov\/;\n(2) Special symbols: We removed extra white-\nspaces, special symbols and brackets like ,\u0302 (, ),\n\n\n\nClaim Evidence Summarization-1 (S1) Summarization-2 (S2)\n\nThere is no evidence that children have\ndied because of a COVID-19 vaccine.\nNo vaccine currently in development has\nbeen approved for widespread public use.\nhttps:\/\/t.co\/9ecvMR8SAf\n\nCurrently there is no coronavirus vaccine that has been\napproved for the American public. And there is no\nevidence that children have died because they received\none of the COVID-19 vaccines being developed.\nPolitiFact found no evidence that anyone has died from\ncomplications related to a trial COVID-19 vaccination.\nThere is no evidence that children have died because\nof a COVID-19 vaccine.\n\nThere is no evidence that children have died\nbecause they received a COVID-19 vaccine.\nNo evidence that anyone has died from\ncomplications related to a trial COVID-19.\n\nThere is no evidence that children have\ndied because they received one of the\nCOVID-19 vaccines being developed.\nPolitiFact found no evidence that anyone\nhas died from complications related to a\ntrial COVID-19 vaccination.\n\nTable 2: Illustrative example of our approach pipeline shown as Claim \u2212\u2192 Evidence \u2212\u2192 Summarization-1 \u2212\u2192\nSummarization-2, where Summarization-2 is obtained after fine-tuning T5 language model, and used as evidence\ninput for classification\n\n{,}; (3) Hashtags, Emojis and Mentions: Addi-\ntionally, we remove hashtags and replace it with\nthe token \u201cHASHTAG:\u201d.\n\nFor example, #COVID-19 becomes\nHASHTAG:COVID-19. Similarly, we also\nreplace mentions \u201c@\u201d with \u201cMENTION:\u201d token.\nAt the end, we convert emojis to their text form\nusing the \u2018demoji\u2019 library8; (4) Lowercasing:\nEventually, we lowercase the claim and the\nevidence text to obtain the input data used for the\nnext Summarization step.\n\n4.3 Summarization of Evidence\n\nOur pre-processed evidence for claims in many\ncases were multiple paragraphs resulting in perfor-\nmance degradation. Therefore, we propose the\naddition of a summarization component to our\npipeline which utilizes state-of-the-art Text-to-Text\nTransfer Transformer (T5) language model (Raffel\net al., 2019) for the inherent summarization task9.\nDue to the nature of the usual summarization task\ninput, a large body of text (full documents), we\nbelieve that our comparatively short paragraphs\nwould be better summarized. This language model\nis fine-tuned for the task of summarization helps\nus obtain a summarized text for each piece of evi-\ndence resulting in what we call Summarization-1\nor S1. An output obtained is shown in Table 2.\n\n4.3.1 Fine-tuning T5 on FEVER Dataset\nAs an additional experimental step, we further fine-\ntune the Text-to-Text Transfer Transformer (T5)\nmodel using the original FEVER dataset (Thorne\net al., 2018a). The original T5 summarization\nmodel is trained on the CNN\/Daily Mail (Hermann\net al., 2015) data where the input is the news ar-\nticle text, and the objective is to highlight sum-\nmarized text as the output. The T5 is an encoder-\n\n8GitHub: Demoji\n9This language model can perform the summarization task\n\nwith the help of a prefix \u201csummarize\u201d to the input text pro-\nvided.\n\ndecoder model pre-trained on a multi-task mix-\nture of unsupervised and supervised tasks and for\nwhich each task is converted into a text-to-text for-\nmat. This allows for the use of the same model,\nloss function, hyperparameters, etc. across our di-\nverse set of tasks. T5 works well on a variety of\ntasks out-of-the-box by prepending a different pre-\nfix to the input corresponding to each task, e.g.,\nfor the task of translation\u2212\u2192 translate English to\nGerman: <English Sentence>, for the task of\nsummarization\u2212\u2192 summarize: <English Text>.\n\nFor our experiments, the aim is to summarize the\npre-processed evidence while including the claim.\nThus, we hypothesize that fine-tuning on an aux-\niliary dataset will improve the quality of the gen-\nerated summary. For fine-tuning, we use the same\nhyperparameters as described in their paper to gen-\nerate another model. We perform another iteration\nof the summarization step using this fine-tuned\nmodel to generate a parallel set of evidence and la-\nbel the output as Summarization-2 or S2 as shown\nin Table 2. Further, we provide the details of as\nthe classification task, which uses either S1 or\nS2 as evidence input to classify the claims as real\nor fake (Figure 2).\n\n5 Experiment Setup\n\nIn this section, we discuss the experiment setup\nin detail. We perform the task of fake news de-\ntection as a binary classification task in a super-\nvised setting. We choose to perform our experi-\nments with both conventional machine learning-\nand deep learning- based classifiers. From the ma-\nchine learning-based approaches, we choose Logis-\ntic Regression (LR) and Support Vector Machines\n(SVM) with the GridSearch implementation for\nbest results over multiple hyperparameters (val-\nues of c, different kernels, etc.) We also utilize\nLSTMs with various contextual language models\nfrom the deep learning methods. From the deep\nlearning-based approaches, we use a simple LSTM\n\nhttps:\/\/github.com\/bsolomon1124\/demoji \n\n\nPrevious Approaches Our Approach w\/ various Classification methods\n\nChen et al. (2021) Li et al. (2021)\nLogistic Regression SVM LSTM\n- S1 S2 - S1 S2 - S1 S2\n\nP 0.9902 0.986 0.9531 0.9565 0.9701 0.9641 0.9671 0.9764 0.9589 0.9598 0.9612\nR 0.9901 0.985 0.9531 0.9564 0.9700 0.9639 0.9668 0.9761 0.9584 0.9596 0.9612\nF 0.9901 0.985 0.9531 0.9565 0.9700 0.9639 0.9668 0.9761 0.9584 0.9596 0.9612\n\nTable 3: Results obtained after the fake news classification task where the values for previous approaches are from\nthe latest shared task results and the results for each iteration of our approach are shown [P (Precision), R (Recall),\nand F (F-Score)]. (-) \u2212\u2192 No Evidence, S1 \u2212\u2192 Summarization-1 as Evidence, S2 \u2212\u2192 Summarization-2 as Evidence.\n\nOur Approach w\/ various Deep Learning Classification methods\nBERTbase RoBERTabase XLNetbase\n\n- S1 S2 - S1 S2 - S1 S2\nP 0.9612 0.9916 0.9917 0.9918 0.9929 0.9922 0.9920 0.9934 0.9947\nR 0.9864 0.9888 0.9897 0.9897 0.9911 0.9916 0.9892 0.9911 0.9925\nF 0.9858 0.9888 0.9893 0.9893 0.9908 0.9908 0.9892 0.9910 0.9925\n\nTable 4: Results obtained after the fake news classification task where the results for each iteration of our approach\nwith various deep learning classification methods are shown [P (Precision), R (Recall), and F (F-Score)]. (-)\u2212\u2192 No\nEvidence, S1 \u2212\u2192 Summarization-1 as Evidence, S2 \u2212\u2192 Summarization-2 as Evidence.\n\nimplementation with pre-trained GloVE10 vectors,\nBERTbase, RoBERTabase, and XLNETbase -based\nclassifiers. Our LSTM implementation uses Adam\noptimizer with a learning rate of 0.001, and 256 as\nthe batch size. For classifiers based on BERTbase,\nRoBERTabase, and XLNETbase, we use the Hug-\ngingFace implementations with a batch size of 32,\nL2 regularization and cross-entropy loss. The regu-\nlarization parameter \u03bb was set to 0.1. Each classifi-\ncation method is iterated (1) without evidence (-),\n(2) with augmented summarized evidences from\nS1, (3) and then with S2, thus giving us three sets\nof results for each method; as shown in Table 4.\n\nAs an input to the classifier, we use the claim as-\nis from the dataset as described above. We have a\ndataset D = (xn, yn)Nn=1 comprising of N training\nsamples. Here xn = (cn, en), where cn represents\nthe claim, and en represents evidence gathered us-\ning our approach. X \u2208 X is defined on input\nspace, and Y \u2208 Y = {0, 1} are the correspond-\ning labels. Thus, given a claim c and evidence e,\nthe aim of this task is to train a classifier such that\nthe claim c is predicted as fake news or not, i.e\nF\u03b8 : X \u2192 Y \u2208 {0, 1}.\n\nF (c, e; \u03b8) =\n\n{\n1, if c is the fake news\n0, otherwise\n\n(1)\n\nwhere F (c, e) is the function our model aims to\nlearn over each iteration or epoch.\n\n10https:\/\/nlp.stanford.edu\/projects\/glove\/\n\n6 Results and Discussion\n\nThe results for our classification task are shown\nin Table 4. Using our approach, we are able to\nmarginally outperform (+0.24, F-Score) the pre-\nvious state-of-the-art (SoTA) approaches for the\ntask of fake news detection as shown in the last\ncolumn (XLNetbase, S2). Even the RoBERTbase\nmodel is able to outperform the SoTA approaches\nby a small margin. We present the values of our top\ntwo best models in boldface in Table 4. Although\nthe improvement margin is small, we would like to\nnote that the previous SoTA approaches are already\nperforming at almost a 0.99 F-score. We executed\nour model run multiple times to ensure that our\nimprovement margin is indeed truly obtained. We\nalso observe that RoBERTAbase, and XLNetbase\noutperform the SoTA approaches (Chen et. al. \/ Li\net. al.) even with S1 summarization component.\nClassical machine learning-based approaches are\nalso shown to perform very well for this task as\nthe scores of 0.96 can be considered to be a good\nperformance for any classification method.\n\nHowever, this is not the only key takeaway from\nthese results. We observe that by using our novel\napproach, a consistent improvement is seen in the\ntask results. The efficacy of our approach can be\nseen from Table 4, as either S1 or S2 consistently\noutperforms all the base models (-) [no evidence]\nin the table. Moreover, using our approach, we\nare able to gather key evidence for such a dataset\n\n\n\nText XLNet LR SVM\nWe always appreciate questions about the quality of our data. If you see a\nnumber that doesn\u2019t look right please file an issue at and we will investigate.\n[SEP] SOURCES: github.com\n\n7 X X\n\nThe number of daily tests has been increasing in a steep climb. Average daily\ntests during the past three weeks also strongly depict the progress made in\nenhancement of #COVID19 tests across the country. [SEP] SOURCES:\ntwitter.com\/MoHFW INDI\n\n7 X X\n\nBill Gates said thousands of people will die with the COVID-19 vaccine\n[SEP] SOURCES:\n\n7 7 7\n\nTable 5: Qualitative error analysis of some output cases both in terms of successes and failures of our approach.\n\nwhere, to begin with, only claims were present\nwith manually annotated labels. Table 5 illustrates\nthe success and failure cases from XLNet, Logis-\ntic Regression and Support Vector Machines. We\nobserve that first two cases were incorrectly pre-\ndicted by the XLNet but were predicted correctly\nby LR and SVM. Last case was predicted incor-\nrectly by all of the models. We believe that the\nabsence of source in the text could be a potential\nreason for this failure. Our approach can gather\nthe evidence using a fully automated method with\nsummarization component(s) in the pipeline. The\nimportance of this component can be gathered from\nmanual observations of examples in the augmented\ndataset. We observe that summarized evidences\nshorten the length of the evidences, which helps\nthe Transformer architecture-based classifiers like\nBERTbase, RoBERTabase, XLNetbase perform bet-\nter. These pre-trained models have a token length\nlimitation of 512 tokens which is easily able to cap-\nture our summarized evidence. We also manually\nobserve that the summarization component helps\nreduce redundancy in the generated sentences and\nremoves duplicates. Hence, improving the quality\nof evidence used as additional input helps reduce\nthe training time. The performance of our models\nwith the fine-tuned summarization component (S2)\nseems to perform better than S1, and the model\nwithout any evidence, as can be seen in Table 4.\n\nWe acknowledge that the CONSTRAINT dataset\nis saturated in terms of possible improvements.\nHowever, with this paper, our aim is to show the\nefficacy of our summarization technique which can\nhelp the evidence detection for news. We chose this\ndataset at an early stage of our work, and our exper-\niments do show that improvements can, in fact, still\nbe shown on this dataset. Our best-performing sys-\ntem surpasses the state-of-the-art by 0.23% points.\n\n7 Conclusion and Future Work\n\nIn this paper, we present an automated method to\ncollect evidence for the fake news detection task.\nWe use our novel approach to augment the dataset,\nreleased in the CONSTRAINT-2021 Shared Task,\nwith evidence sets collected from the web. Our\nmethod helps process these evidence sets, clean\nthem and use them to generate summarized evi-\ndence based on two different methodologies. We\nuse either of the summarized evidence as an addi-\ntional input to the fake news classification task and\nperform an evaluation of our approach. We discuss\nthe results of the classification task and conclude\nthat our approach helps outperform the previous\nSoTA approaches by a small margin, however, help-\ning generate evidence for a crucial dataset. We\nshow that a summarization module can help collect\nevidence more effectively. We augment this dataset\nwith the summarized evidence and release it along\nwith the code and generated models for further re-\nsearch. We would also like to conclude that our\nmethod is generalizable; since it uses pre-trained\nmetrics (BERTScore) and models (T5), it can be\nused to gather evidence for other datasets. The\noverall pipeline is also not very time-consuming (2\nseconds per sample) once fine-tuned models are in-\ncluded in it. We hope our method and the resources\nare helpful to the NLP community.\n\nIn future, we would like to use our method to\ngather evidence for other fact detection\/verification\ndatasets as well. Our initial aim is to reproduce\nthis study with other datasets and ensure that our\nmethod performs well in a real-world scenario. We\nwould also like to apply this method and gather fur-\nther evidence for existing fake news datasets, and\nperform our experiments to evaluate this approach\nover multiple exisiting datasets, including existing\nmultilingual datasets.\n\n\n\nReferences\nJennifer Allen, Baird Howland, Markus Mobius, David\n\nRothschild, and Duncan J Watts. 2020. Evaluating\nthe fake news problem at the scale of the information\necosystem. Science Advances, 6(14):eaay3539.\n\nSumitra Badrinathan. 2020. Educative interventions to\ncombat misinformation: Evidence from a field ex-\nperiment in india. American Political Science Re-\nview, pages 1\u201317.\n\nBen Chen, Bin Chen, Dehong Gao, Qijin Chen,\nChengfu Huo, Xiaonan Meng, Weijun Ren, and\nYang Zhou. 2021. Transformer-based language\nmodel fine-tuning methods for covid-19 fake news\ndetection. In Combating Online Hostile Posts in\nRegional Languages during Emergency Situation,\npages 83\u201392, Cham. Springer International Publish-\ning.\n\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM\nfor natural language inference. In Proceedings of\nthe 55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers),\npages 1657\u20131668, Vancouver, Canada. Association\nfor Computational Linguistics.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n\nAnna Glazkova, Maksim Glazkov, and Timofey Tri-\nfonov. 2020. g2tmn at constraint@aaai2021:\nExploiting CT-BERT and ensembling learning\nfor COVID-19 fake news detection. CoRR,\nabs\/2012.11967.\n\nAndreas Hanselowski, Hao Zhang, Zile Li, Daniil\nSorokin, Benjamin Schiller, Claudia Schulz, and\nIryna Gurevych. 2018. UKP-athene: Multi-sentence\ntextual entailment for claim verification. In Pro-\nceedings of the First Workshop on Fact Extraction\nand VERification (FEVER), pages 103\u2013108, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\n\nKarl Moritz Hermann, Toma\u0301s Kocisky\u0301, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS, pages 1693\u20131701.\n\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy:\nIndustrial-strength Natural Language Processing in\nPython.\n\nDebanjana Kar, Mohit Bhardwaj, Suranjana Samanta,\nand Amar Prakash Azad. 2020. No rumours please!\n\nA multi-indic-lingual approach for COVID fake-\ntweet detection. CoRR, abs\/2010.06906.\n\nSimon Kemp. 2015. Global digital & social media\nstats: 2015. Social Media Today.\n\nXiangyang Li, Yu Xia, Xiang Long, Zheng Li, and\nSujian Li. 2021. Exploring text-transformers in\naaai 2021 shared task: Covid-19 fake news detec-\ntion in english. In Combating Online Hostile Posts\nin Regional Languages during Emergency Situation,\npages 106\u2013115, Cham. Springer International Pub-\nlishing.\n\nChristopher Malon. 2018. Team papelo: Trans-\nformer networks at FEVER. In Proceedings of the\nFirst Workshop on Fact Extraction and VERification\n(FEVER), pages 109\u2013113, Brussels, Belgium. Asso-\nciation for Computational Linguistics.\n\nMartin Mu\u0308ller, Marcel Salathe\u0301, and Per Egil Kummer-\nvold. 2020. Covid-twitter-bert: A natural language\nprocessing model to analyse COVID-19 content on\ntwitter. CoRR, abs\/2005.07503.\n\nAnkur P. Parikh, Oscar Ta\u0308ckstro\u0308m, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable atten-\ntion model for natural language inference. CoRR,\nabs\/1606.01933.\n\nParth Patwa, Shivam Sharma, Srinivas PYKL, Vineeth\nGuptha, Gitanjali Kumari, Md Shad Akhtar, Asif Ek-\nbal, Amitava Das, and Tanmoy Chakraborty. 2020.\nFighting an infodemic: Covid-19 fake news dataset.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs\/1910.10683.\n\nTathagata Raha, Vijayasaradhi Indurthi, Aayush Upad-\nhyaya, Jeevesh Kataria, Pramud Bommakanti,\nVikram Keswani, and Vasudeva Varma. 2021. Iden-\ntifying COVID-19 fake news in social media. CoRR,\nabs\/2101.11954.\n\nArjun Roy, Kingshuk Basak, Asif Ekbal, and Pushpak\nBhattacharyya. 2018. A deep ensemble framework\nfor fake news detection and classification. CoRR,\nabs\/1811.04670.\n\nGautam Kishore Shahi and Durgesh Nandini. 2020.\nFakecovid - A multilingual cross-domain fact\ncheck news dataset for COVID-19. CoRR,\nabs\/2006.11343.\n\nS. M. Sadiq-Ur-Rahman Shifath, Mohammad Faiyaz\nKhan, and Md. Saiful Islam. 2021. A transformer\nbased approach for fighting COVID-19 fake news.\nCoRR, abs\/2101.12027.\n\nKai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and\nHuan Liu. 2017. Fake news detection on socialwang\nmedia: A data mining perspective.\n\nhttps:\/\/doi.org\/10.18653\/v1\/P17-1152\nhttps:\/\/doi.org\/10.18653\/v1\/P17-1152\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1423\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1423\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1423\nhttp:\/\/arxiv.org\/abs\/2012.11967\nhttp:\/\/arxiv.org\/abs\/2012.11967\nhttp:\/\/arxiv.org\/abs\/2012.11967\nhttps:\/\/doi.org\/10.18653\/v1\/W18-5516\nhttps:\/\/doi.org\/10.18653\/v1\/W18-5516\nhttp:\/\/papers.nips.cc\/paper\/5945-teaching-machines-to-read-and-comprehend\nhttp:\/\/papers.nips.cc\/paper\/5945-teaching-machines-to-read-and-comprehend\nhttps:\/\/doi.org\/10.5281\/zenodo.1212303\nhttps:\/\/doi.org\/10.5281\/zenodo.1212303\nhttps:\/\/doi.org\/10.5281\/zenodo.1212303\nhttp:\/\/arxiv.org\/abs\/2010.06906\nhttp:\/\/arxiv.org\/abs\/2010.06906\nhttp:\/\/arxiv.org\/abs\/2010.06906\nhttps:\/\/doi.org\/10.18653\/v1\/W18-5517\nhttps:\/\/doi.org\/10.18653\/v1\/W18-5517\nhttp:\/\/arxiv.org\/abs\/2005.07503\nhttp:\/\/arxiv.org\/abs\/2005.07503\nhttp:\/\/arxiv.org\/abs\/2005.07503\nhttp:\/\/arxiv.org\/abs\/1606.01933\nhttp:\/\/arxiv.org\/abs\/1606.01933\nhttp:\/\/arxiv.org\/abs\/2011.03327\nhttp:\/\/arxiv.org\/abs\/1910.10683\nhttp:\/\/arxiv.org\/abs\/1910.10683\nhttp:\/\/arxiv.org\/abs\/1910.10683\nhttp:\/\/arxiv.org\/abs\/2101.11954\nhttp:\/\/arxiv.org\/abs\/2101.11954\nhttp:\/\/arxiv.org\/abs\/1811.04670\nhttp:\/\/arxiv.org\/abs\/1811.04670\nhttp:\/\/arxiv.org\/abs\/2006.11343\nhttp:\/\/arxiv.org\/abs\/2006.11343\nhttp:\/\/arxiv.org\/abs\/2101.12027\nhttp:\/\/arxiv.org\/abs\/2101.12027\nhttp:\/\/arxiv.org\/abs\/1708.01967\nhttp:\/\/arxiv.org\/abs\/1708.01967\n\n\nElena Shushkevich and John Cardiff. 2021. TUDublin\nteam at Constraint@AAAI2021 \u2013 COVID19\nFake News Detection. arXiv e-prints, page\narXiv:2101.05701.\n\nSabrina Tavernise. 2016. As fake news spreads lies,\nmore readers shrug at the truth. The New York Times,\n6.\n\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018a.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809\u2013819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\n\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal.\n2018b. The fact extraction and verification (fever)\nshared task.\n\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2019.\nThe FEVER2.0 shared task. In Proceedings of the\nSecond Workshop on Fact Extraction and VERifica-\ntion (FEVER), pages 1\u20136, Hong Kong, China. Asso-\nciation for Computational Linguistics.\n\nRutvik Vijjali, Prathyush Potluri, Siddharth Kumar,\nand Sundeep Teki. 2020. Two stage transformer\nmodel for COVID-19 fake news detection and fact\nchecking. In Proceedings of the 3rd NLP4IF\nWorkshop on NLP for Internet Freedom: Censor-\nship, Disinformation, and Propaganda, pages 1\u201310,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics (ICCL).\n\nAndreas Vlachos and Sebastian Riedel. 2014. Fact\nchecking: Task definition and dataset construction.\nIn Proceedings of the ACL 2014 Workshop on Lan-\nguage Technologies and Computational Social Sci-\nence, pages 18\u201322, Baltimore, MD, USA. Associa-\ntion for Computational Linguistics.\n\nWilliam Yang Wang. 2017. \u201cliar, liar pants on fire\u201d: A\nnew benchmark dataset for fake news detection. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 422\u2013426, Vancouver, Canada.\nAssociation for Computational Linguistics.\n\nApurva Wani, Isha Joshi, Snehal Khandve, Vedangi\nWagh, and Raviraj Joshi. 2021. Evaluating deep\nlearning approaches for covid19 fake news detection.\nCoRR, abs\/2101.04012.\n\nHerman Wasserman and Dani Madrid-Morales. 2019.\nAn exploratory study of \u201cfake news\u201d and media trust\nin kenya, nigeria and south africa. African Journal-\nism Studies, 40(1):107\u2013123.\n\nhttp:\/\/arxiv.org\/abs\/2101.05701\nhttp:\/\/arxiv.org\/abs\/2101.05701\nhttp:\/\/arxiv.org\/abs\/2101.05701\nhttps:\/\/doi.org\/10.18653\/v1\/N18-1074\nhttps:\/\/doi.org\/10.18653\/v1\/N18-1074\nhttp:\/\/arxiv.org\/abs\/1811.10971\nhttp:\/\/arxiv.org\/abs\/1811.10971\nhttps:\/\/doi.org\/10.18653\/v1\/D19-6601\nhttps:\/\/www.aclweb.org\/anthology\/2020.nlp4if-1.1\nhttps:\/\/www.aclweb.org\/anthology\/2020.nlp4if-1.1\nhttps:\/\/www.aclweb.org\/anthology\/2020.nlp4if-1.1\nhttps:\/\/doi.org\/10.3115\/v1\/W14-2508\nhttps:\/\/doi.org\/10.3115\/v1\/W14-2508\nhttps:\/\/doi.org\/10.18653\/v1\/P17-2067\nhttps:\/\/doi.org\/10.18653\/v1\/P17-2067\nhttp:\/\/arxiv.org\/abs\/2101.04012\nhttp:\/\/arxiv.org\/abs\/2101.04012\n\n","9":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 2019\n\nComputer-Assisted Creation of Boolean\nSearch Rules for Text Classification in the\n\nLegal Domain\n\nHannes WESTERMANN, a,1, Jarom\u0131\u0301r S\u030cAVELKA b, Vern R. WALKER c,\nKevin D. ASHLEY b and Karim BENYEKHLEF a\n\na Cyberjustice Laboratory, Faculte\u0301 de droit, Universite\u0301 de Montre\u0301al\nb ISP, School of Computing and Information, University of Pittsburgh\n\nc LLT Lab, Maurice A. Deane School of Law, Hofstra University\n\nAbstract. In this paper, we present a method of building strong, explainable classi-\nfiers in the form of Boolean search rules. We developed an interactive environment\ncalled CASE (Computer Assisted Semantic Exploration) which exploits word co-\noccurrence to guide human annotators in selection of relevant search terms. The\nsystem seamlessly facilitates iterative evaluation and improvement of the classifi-\ncation rules. The process enables the human annotators to leverage the benefits of\nstatistical information while incorporating their expert intuition into the creation of\nsuch rules. We evaluate classifiers created with our CASE system on 4 datasets, and\ncompare the results to machine learning methods, including SKOPE rules, Ran-\ndom forest, Support Vector Machine, and fastText classifiers. The results drive the\ndiscussion on trade-offs between superior compactness, simplicity, and intuitive-\nness of the Boolean search rules versus the better performance of state-of-the-art\nmachine learning models for text classification.\n\nKeywords. Artificial Intelligence & Law, Text Classification, Semantic exploration,\nBoolean search, Natural language processing, Explainable artificial intelligence\n\n1. Introduction\n\nReading, interpreting, and understanding legal texts is one of the most important skills\nof legal professionals. Lawyers, judges, students, and researchers alike spend a lot of\ntime and effort learning, honing, and using the skill in reading statutory law, a legal\ncase, a contract, or a journal article, while interpreting the document and applying the\nknowledge to solve a new problem. Such analysis is done on several levels\u2014sometimes,\nan individual sentence carries the much needed information, while other times the reader\nhas to study whole sections or the entire document to understand the important point.\nFurther, the reader may have to understand different features, such as the facts of a legal\ncase or the relevance of a sentence.\n\nThe ability to categorize the texts or their pieces into certain types (e.g., court rea-\nsoning, legal rule, facts) is an integral part of the analysis. It is therefore no coincidence\n\n1Corresponding Author: Hannes Westermann, E-mail: hannes.westermann@umontreal.ca\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n05\n80\n\n7v\n1 \n\n [\ncs\n\n.L\nG\n\n] \n 1\n\n0 \nD\n\nec\n 2\n\n02\n1\n\n\n\nDecember 2019\n\nthat text classification is one of the big focus areas in the field of artificial intelligence\nand law (AI & Law). Automating the classification tasks has often become feasible due\nto advances in machine learning (ML) and natural language processing (NLP) methods.\nTypically, the research is conducted by manually labelling hundreds or thousands of doc-\numents. Once the annotation is completed, the researchers use the data to build ML mod-\nels that are able to learn patterns in the annotated data and apply these to classify new\nunseen texts.\n\nAn automated approach has a host of advantages when compared to the tedious work\nof classifying the whole corpus without the help of a computer. However, there are some\ndrawbacks as well. Firstly, it still takes a lot of effort to manually label the subset of\ndocuments required for training an ML classifier. Secondly, it can be difficult to explain\nthe decisions of sophisticated models. This may sometimes lead to skepticism as to the\nsuitability of such models to be used in practice. Possible over-fitting is yet another risk\nwhich needs to be taken into account. Sometimes the models work well on the annotated\ndata, but fail to generalize to unseen documents.\n\nIn this paper, we present a method addressing these issues. We built a tool that al-\nlows annotators to create Boolean rules in a computer-assisted fashion. These rules could\npotentially be used for classification in domains with little available data, by incorporat-\ning human intuition into the process. Further, the rules created are more explainable than\nmost machine learning models, while still performing reasonably well.\n\n2. Prior work\n\nAccording to Antonie and Zaiane [1] \u201ca good text classifier [. . . ] efficiently categorizes\nlarge sets of text documents in a reasonable time frame and with an acceptable accuracy,\nand [. . . ] provides classification rules that are human readable for possible fine-tuning.\u201d\nOne approach to text classification is to let a human expert define a set of logical rules\nbased on his domain-specific knowledge of how to classify documents under a given\nset of categories [2]. Generating rules based on human expertise is time-consuming,\nexpensive, and sometimes not feasible. However, the great advantage of such rules is that\nthey often provide intuitive and meaningful explanation (justification) of the resulting\nclassification.\n\nAlternatively, one can apply various methods for inducting text classification rules\nautomatically including such methods as decision trees or associative rule mining [2].\nThe latter employs an iterative search of a database to discover the most frequent sets\nof k items (k-itemsets) that are associated with the documents sharing a particular clas-\nsification; a logical rule based on a k-itemset should support the classification with a\nconfidence above a certain threshold. The potentially very large number of rules are then\npruned using various techniques [1]. A disadvantage of such automatically learned rules\nis that they may not correspond to expert intuitions about texts in the domain.\n\nVarious hybrids of manual and automated methods are possible. For example, Yao,\net al. [3] evaluated a medical clinical text classification method that employed rules to\nidentify trigger phrases such as disease names and alternatives. They used the trigger\nphrases to predict classes that had very few examples. For the remaining classes they\ntrained a knowledge-guided convolutional neural network (CNN) with word embeddings\nand medical feature embeddings.\n\n\n\nDecember 2019\n\nIn Walker et al. [4], the authors investigated the task of automatically classifying,\nwithin adjudicatory decisions in the United States, those sentences that state whether the\nconditions of applicable legal rules have been satisfied or not (\u201cFinding Sentences\u201d), by\nanalyzing a small sample of classified sentences (N = 530) to manually develop rule-\nbased scripts, using semantic attribution theory. The methodology and results suggested\nthat some access-to-justice use cases can be adequately addressed at much lower cost\nthan previously believed. Our work extends that effort by developing a platform for effi-\nciently improving the classification rules in the iterative fashion.\n\n3. Boolean Search Rules\n\nWe propose and evaluate a novel hybrid combination of manual and automated con-\nstruction of text classification rules. Our CASE system helps annotators select relevant\nterms, create Boolean text classification rules, and evaluate and improve them in an it-\nerative manner. Depending on the use case, the resulting rules may prove very useful\u2014\nespecially where explanatory power and compactness are important.\n\n\u201cBoolean search rules\u201d are an appealing method for classifying documents because\nsuch rules are familiar to anyone who works with legal information retrieval systems.\nThey make it possible to search for single words (such as \u201cveteran\u201d), which would return\nall cases containing the word. Further, it is possible to logically combine several rules,\nusing the OR, AND, and NOT operators. OR returns texts with either of the two words\nwhile AND requires both of them to be present. NOT excludes texts containing a partic-\nular word. In our case, we are using the FTS5 search engine integrated into the SQLite\nDatabase [5] to process our queries. This allows us to build complex queries, combining\ndifferent logical operators, that are executed very rapidly.\n\n3.1. Existing methodologies to create Boolean rules\n\nThere have been previous attempts of using Boolean search rules in AI & Law. How-\never, without the methods presented in this paper the process can be long and laborious.\nA recent attempt at creating such search rules was made by Walker et al. [4]. The re-\nsearchers tested whether distinctive phrasing in legal decisions enables the development\nof automatic classifiers on the basis of a small sample of labeled decisions, with ade-\nquate results for some important use cases. Certain words, such as \u201cfinds\u201d, were found to\nclosely correspond to a sentence having the rhetorical role of a finding of fact. Two such\nrules were tested, leading to an F1 score of 0.512 in identifying such sentences. Testing\nnew hypotheses, observing the results, and comparing the results of new classification\nrules against the old ones, was a time-consuming and laborious process. In this paper,\nwe introduce a tool that makes such a process more efficient.\n\n4. Methodology\n\nIn this paper, we test the hypothesis that Boolean search rules created by humans with the\nassistance of a computerized tool can prove useful in building text classifiers in the legal\ndomain. To test the hypothesis we created such rules on four datasets of case texts, and\ncompared the results to those obtained by using ML methods. The process is described\nin this section.\n\n\n\nDecember 2019\n\n4.1. Datasets\n\nWe selected four existing datasets created within the AI & Law community to evaluate\nour methodology. These are presented below.\n\n4.1.1. Veterans Claims Dataset (sentence roles)\n\nWalker et al. [4] analyzed 50 fact-finding decisions issued by the U.S. Board of Veterans\u2019\nAppeals (\u201cBVA\u201d) from 2013 through 2017, all arbitrarily selected cases dealing with\nclaims by veterans for service-related post-traumatic stress disorder (PTSD). For each of\nthe 50 BVA decisions in the PTSD dataset, the researchers extracted all sentences ad-\ndressing the factual issues related to the claim for PTSD, or for a closely-related psychi-\natric disorder. These were tagged with the rhetorical roles [6] the sentences play in the\ndecision. We conducted our experiments on this set of sentences.\n\n4.1.2. Court Decisions Segmentation Dataset (functional parts)\n\nS\u030cavelka and Ashley [7] examined the possibility of automatically segmenting court opin-\nions into high-level functional parts (i.e., Introduction (I), Background (B), Analysis (A),\nFootnotes (F)) and issue specific parts (i.e., Conclusions(C)). They assembled 316 court\ndecisions from Court Listener and Google Scholar, 143 in the area of cyber crime and 173\ninvolving trade secrets. These were annotated, after which Conditional Random Fields\n(CRF) models were trained to recognize the boundaries between the sections. We used\nthe cases in the area of cyber crime for our tests. It should be noted that we do not attempt\nto detect the boundaries, but instead try to classify the annotated text sections.\n\n4.1.3. The Trade Secrets Factors Dataset (factor prediction)\n\nFalakmasir and Ashley [8] assembled a corpus of 172 trade secret misappropriation cases\nemployed in the HYPO, CATO, SMILE+IBP and VJAP programs. Legal experts had\nlabeled the cases by the applicable factors, stereotypical patterns of fact that strengthen or\nweaken a claim. There are 26 trade secret misappropriation factors. For our experiments,\nwe used the existence of security measures in a case (Factor 6), to deal with a binary\nclassification task.\n\n4.1.4. The Statutory Interpretation Dataset (interpretative value of sentences\n\nS\u030cavelka et al. [9] studied methods for retrieving useful sentences from court opinions\nthat elaborate on the meaning of a vague statutory term. To support their experiments\nthey queried the database of sentences from case law that mentioned three terms from\ndifferent provisions of the U.S. Code. They manually classified the sentences in terms of\nfour categories with respect to their usefulness for the interpretation of the corresponding\nstatutory term. Here we work with the sentences mentioning \u2018common business purpose\u2019\n(149 high value, 88 certain value, 369 potential value, 274 no value). In [9] the goal was\nto rank the sentences with respect to their usefulness; here, we classify them into the four\nvalue categories.\n\n\n\nDecember 2019\n\nFigure 1. The Computer Assisted Semantic Exploration (CASE) interface.\n\n4.2. Dataset Split\n\nThe four datasets are described in section 4.1. For our experiments we split each dataset\ninto three parts: training (20%), validation (10%), and testing (70%). The unusual split\n(small training set) was used to evaluate the performance of the search queries in situa-\ntions where very little data is available. This is often the case in the problems of interest\nin the field of AI & Law. Using the identical dataset splits we created classifiers with the\nCASE tool and ML methods, as described below.\n\n4.3. CASE - Computer Assisted Semantic Exploration\n\nWe developed a tool for Computer Assisted Semantic Exploration (CASE). CASE facil-\nitates seamless creation of Boolean classification rules. Figure 1 shows a screenshot of\nthe interface. The tool supports users in interactively creating Boolean rules using several\nstatistical methods. At (1), CASE displays the possible classes for annotation. By click-\ning on a class, the user selects texts in that specific class, and is then shown information\nabout the word distribution inside that selection under (2). This list is sortable, and shows\nseveral headers, containing metrics useful for the selection of significant words.\n\nOnce a user has found a word that is a strong indicator of a specific class, he can\ncreate a query in (3), using logical operators such as AND, OR, and NOT. For example, a\nquery to identify the class Analysis could be \u201ccannot OR conclusion.\u201d The query can then\nbe run, and is immediately evaluated, with the results being presented in (4). Here, the\nuser also has the possibility of selecting documents that are misidentified, for example,\nin order to exclude certain words. The user can thus work on creating queries able to\nidentify classes with high precision and recall in an iterative fashion.\n\nOnce the user is content with a query, he can save it and create additional queries.\nIdeally, in conjunction, the queries will identify documents with high precision and re-\ncall. The filters are constantly evaluated against the training data (5), and validation data\n(6) to prevent over-fitting.\n\n\n\nDecember 2019\n\nThe queries used for the current paper were created by the co-authors of this pa-\nper. Using the statistics provided by CASE as well as our previous intuitions about the\ndatasets, we used the tool to add and modify rules until it was difficult to introduce new\nrules without lowering the validation score. CASE was very helpful in identifying words\nsignificant for a class and using these to create the rules.\n\n4.4. Machine Learning\n\nWe trained four different types of ML models as benchmarks. We used SKOPE-rules\n[10] to simulate the situation where the model is forced to construct similar Boolean\nrules as a human using CASE. The difference is that the rules are learned automatically.\nWe trained random forest classifier, support vector machine (SVM), and fastText [11]\nmodels on more sophisticated features. Their predictions are used to investigate how\nmuch performance one has to sacrifice in order to benefit from the explanatory power\nof CASE (computer assisted) and SKOPE-rules (computer generated). We have used the\nsame training sets as those in the CASE experiments to train the models. The validation\nsets were used to optimize the models\u2019 hyperparamters. The same test sets were used for\nthe evaluation.\n\nSKOPE-rules is a Python ML module the aim of which is learning logical, inter-\npretable rules. [10] A decision rule is a logical expression of the form \u201cIF conditions\nTHEN response.\u201d The problem of generating such rules has been widely considered in\nML, see e.g., RuleFit [12], Slipper [13], LRI [14], MLRules [15]. SKOPE-Rules extracts\nrules from an ensemble of trees. A weighted combination of these rules is then built by\nsolving an L1-regularized optimization problem over the weights as described in [16]. To\nforce the model to construct the rules that are comparable to those created using CASE,\nwe have used unigram, bigram, and trigram word occurrences as features. The classifi-\ncation model is then a set of rules (possibly overlapping, i.e., OR), where each rule is a\nconjunction (i.e., AND) of matching or filtering (NOT) on words and phrases. For each\ndata set we have trained a number of binary models, one for each class.\n\nA random forest is an ensemble classifier that fits a number of decision trees on sub-\nsamples of the data set. It uses averaging to improve the predictive accuracy and control\nover-fitting. As an implementation of random forest we used the scikit-learn\u2019s Random\nForest Classifier module [17]. As features we use TF-IDF weights of (1-4)-grams of\nlowercase tokens with their POS tags.\n\nAn SVM classifier constructs a hyper-plane in a high dimensional space, which is\nused to separate the classes from each other. As an implementation of SVM we used the\nscikit-learn\u2019s Support Vector Classification module [18]. We used the same features as\nwith the random forest models to train a number of binary classifiers.\n\nFastText is a linear classifier that uses ngram features that are embedded and aver-\naged to form the hidden variable. We worked with the Python wrapper [19] for the orig-\ninal library released by Facebook [20]. As for other classifiers we trained a number of\nbinary classification models using grid search to optimize hyperparameters.\n\n4.4.1. Evaluation\n\nThe evaluation of all the models is performed on the test sets (70% of the respective\ndatasets). Note that all the methods were trained on the identical training sets and fine-\ntuned on the identical validation sets. The performance is measured in terms of precision\n\n\n\nDecember 2019\n\nCASE SKOPE RF SVM Fasttext\nP R F1 P R F1 P R F1 P R F1 P R F1\n\nVetClaims\n-sentence .84 .25 .38 .80 .33 .47 .90 .61 .72 .99 .44 .61 .87 .61 .72\n-finding .71 .38 .50 .63 .40 .49 .77 .26 .39 .83 .57 .67 .68 .59 .63\n-evidence .82 .74 .78 .71 .81 .76 .88 .88 .88 .90 .92 .91 .87 .92 .90\n-rule .71 .48 .57 .60 .65 .63 .95 .55 .70 .90 .78 .83 .87 .79 .82\n-citation .96 .99 .97 .86 .86 .86 .99 .96 .97 .98 .98 .98 .99 .97 .98\n-reasoning .62 .14 .22 .50 .23 .31 .65 .06 .12 .75 .27 .39 .43 .39 .41\n-overall .78 .50 .57 .68 .55 .59 .86 .55 .63 .89 .66 .73 .79 .71 .74\n-overall-w .80 .61 .67 .74 .71 .71 .88 .68 .73 .96 .83 .87 .83 .80 .81\n\nSection segmentation\n-intro .90 .75 .81 .83 1.0 .91 1.0 .98 .99 1.0 .99 .99 .98 .95 .97\n-backg. .76 .80 .78 .62 .96 .75 .97 .83 .90 .99 .85 .91 .96 .85 .90\n-analysis .87 .83 .85 .88 .88 .88 .98 .88 .93 .93 .97 .95 .90 .98 .94\n-overall .84 .79 .81 .78 .95 .85 .98 .90 .94 .97 .94 .95 .95 .93 .94\n-overall-w .84 .79 .82 .78 .95 .85 .98 .89 .94 .97 .94 .95 .95 .93 .94\n\nTrade secrets\n-security .65 .61 .63 .53 .97 .69 .59 .69 .64 .50 1.0 .67 .57 .49 .53\n\nStatutory interpretation\n-high .72 .45 .55 .66 .39 .49 .91 .10 .17 .96 .22 .36 .61 .46 .52\n-certain .18 .18 .18 .26 .23 .24 .67 .13 .22 .60 .10 .17 .40 .13 .20\n-potential .69 .36 .47 .49 .98 .65 .69 .54 .60 .71 .64 .67 .63 .68 .65\n-no .89 .65 .75 .74 .71 .73 .90 .77 .83 .90 .78 .83 .92 .79 .85\n-overall .62 .41 .49 .54 .58 .53 .79 .39 .45 .79 .44 .51 .64 .52 .55\n-overall-w .70 .44 .54 .57 .72 .61 .79 .50 .56 .80 .56 .62 .69 .62 .64\n\nTable 1. P, R and F1 for the different classifiers applied on datasets described in Section 4.1\n.\n\n(P), recall (R), and F1-measure (F1). All the classifiers are evaluated in the one-vs-rest\nsettings where each label within each of the four datasets has its own classifier. We mea-\nsure aggregate results as well. \u201cOverall\u201d averages the scores for the different classes over\nthe total number of classes. \u201cOverall-w\u201d uses a weighted average, where each class is\ngiven a weight according to how often it appears in the test dataset. For each dataset, the\nhighest overall F1-score is written in bold.\n\n5. Results\n\nThe results are presented in Table 1. In general, the rules created by CASE performed\nsimilarly to the computer generated SKOPE rules. However, they seem to have a slightly\nhigher precision, with a lower recall. This can have utility for certain use cases. As ex-\npected the more complex RF, SVM, and fastText models perform better than the hu-\nman generated rules. We discuss the trade-offs between explainability and performance\nbelow.\n\nVeteran Claims Dataset. Compared to the work in [4], the CASE tool gave us\nsignificant flexibility and speed improvements in creating and optimizing the Boolean\nsearch rules. For \u201cfinding sentences,\u201d for example, we confirmed the usefulness of the\n\u201cfinds\u201d and \u201cpreponderance\u201d search terms [4], while adding others such as \u201celements\u201d\nand \u201cwarranted\u201d.\n\n\n\nDecember 2019\n\nCASE (f1: .85) SKOPE-rules (f1: .88) Random Forest (f1: .93)\n(boolean rules) (boolean rules) (important features)\ncannot OR (cir AND NOT headquarters in is not, that, be, cir, cases, can,\napparent OR AND is) OR is, provides, held, it, statute, in,\nprohibit OR (2d AND NOT february 17 intended, record, see also, there,\ndefinition AND is not) OR 9th cir, subsection, 7th cir, of\n\n(NOT appeal AND NOT cir such, have, may be, congress,\nAND it is) OR when, issue, 3d at, evidence\n(NOT cir AND is not AND that, if, thus the, as, where,\nNOT of 18) is to, here the, definition of\n\nTable 2. Comparison between created rules for identifying the analysis section in the segmentation dataset.\n\nCourt Decisions Segmentation Dataset. The CASE rules achieved higher preci-\nsion, but lower recall than the SKOPE rules. The created Boolean search rules are quite\nsimple. For identifying the analysis section, for example, the following query was quite\nsuccessful: \u201ccannot OR apparent OR definition OR prohibition.\u201d\n\nTrade Secrets Factors Dataset. This dataset was the most difficult to deal with.\nThere were few cases, and they were long and complex. For training, only 33 cases\nwere available. However, the rules achieved the highest precision among the classifiers.\nWe relied heavily on human intuition, such as the term \u201cnon-disclosure\u201d implying the\nexistence of security measures. Building the rules also helped us identify an error in the\nannotation of a case.\n\nStatutory Interpretation Dataset. This dataset was also very hard to deal with, due\nto its being unbalanced and the fact that the value of a sentence for statutory interpretation\nis hard to link to individual terms. Again, we can see the pattern of the CASE rules\nhaving higher precision than SKOPE rules, but lower recall.\n\n5.1. Explainability of Rules or Features\n\nTable 2 shows a comparison of rules created using the different systems for classifica-\ntion of the \u201canalysis part\u201d of the court decisions segmentation dataset. For the CASE\nand SKOPE rules, a document triggering any of the listed queries will result in the doc-\nument being labeled as \u201canalysis.\u201d For the random forest algorithm, we present the most\nimportant features, as selected by the algorithm. Overall, the CASE rules are much less\ncomplex, while still showing performance that is not much inferior. Further, the CASE\nrules seem to contain more legally relevant terms, such as \u201cprohibit\u201d and \u201cdefinition.\u201d\nThese properties make the rules easier to explain.\n\n6. Discussion\n\nWe have shown that Boolean search rules can be created efficiently with a system such as\nCASE. In most areas, the performance was weaker than their ML counterparts. However,\nthe CASE rules have advantages that might make their use desirable in some use cases. In\nthis section, we discuss some of the advantages and disadvantages of using such Boolean\nsearch rules for classification in legal domains.\n\n\n\nDecember 2019\n\n6.1. Advantages of using Boolean rules\n\nOne advantage of using Boolean rules, developed with the assistance of the CASE plat-\nform, is that those rules can incorporate human intuitions. Thus, the user can rapidly\nformulate and evaluate hypotheses of which terms might prove useful in search rules, as-\nsisted by the statistical measures provided by CASE. In doing so, users are able to select\nwords that they know have legal significance in relation to the classifier.\n\nIn incorporating this intuition, the user has significantly more control over the cre-\nated model than with ML systems. With ML it is difficult to direct the training of the sys-\ntem, beyond feature selection and hyperparameter optimization. In CASE, on the other\nhand, the human is always in control of the system. Evaluation occurs continuously, and\nthe human has complete control over how the model develops and how new search terms\naffect the precision and recall of the search rules. This allows users to fit the rules more\nexactly to their requirements and use case. Further, the creation process allows the anno-\ntator to develop an intuition for the particularities of the dataset in an exploratory fashion.\nIn the trade secrets dataset, the system helped us to discover an error in classification,\nshowcasing this advantage.\n\nThe incorporation of human intuition, together with the level of control a human is\ngiven over the creation of the search rules, can potentially allow the user to create search\nrules that are less prone to overfit and therefore generalize better. The user can choose\nto use only phrases that are independent of the specific context of the dataset, thereby\ncreating rules that generalize to other datasets. Since the users decide whether to use a\nterm, even very small datasets could support the creation of the rules with high precision.\n\nA big issue in the practical use of ML in the legal field is the difficulty of explain-\ning the created models. This might cause legal professionals not to trust the algorithms.\nUsing Boolean search rules might alleviate this issue. Firstly, the human who makes the\ndecisions in creating models, is fully aware of why a particular word was chosen and\nused in a certain way. Further, the structure of the created rules, using AND, NOT, and\nOR, should be easier to grasp than complex ML models. They can thus offer a basis for\nbetter explaining why a particular document was chosen, and why not. As can be seen in\nTable 2, the CASE rules are both simpler and more legally relevant than the ML models.\n\n6.2. Limitations\n\nAs can be seen from the results presented in Section 5, ML models often performed\nbetter than the human-created rules. This is an interesting result in itself, as it shows the\npower of well-optimized ML methods even on small datasets. If performance is the most\nimportant metric, using ML methods could thus often be preferable. We discuss methods\nto combine advantages from ML and CASE below (Section 7).\n\n7. Future work\n\nThis paper is an initial step in exploring the use of computer-assisted creation of Boolean\nsearch rules for text classification. There are many avenues for further research. One is to\nexpand the CASE system. For example, the system could include n-grams beyond simple\nwords. Restructuring the classifiers as multi-label classifiers, and running the best clas-\n\n\n\nDecember 2019\n\nsifiers first, would improve performance. The system should also be expanded to work\nbetter with long documents, such as the trade secrets cases. Another avenue is combining\nthe CASE platform with ML methods in a hybrid approach to harness the advantages of\nboth. For example, CASE could be used to preselect documents from a massive corpus,\nafter which a ML algorithm could be trained on only those documents. Another approach\nwould be to run a ML model on annotated data, and use CASE subsequently to analyze\nthe results and exclude false positives.\n\n8. Conclusions\n\nIn this paper we have proposed and evaluated CASE, a novel approach for computer-\nassisted text classification using Boolean matching rules. We have shown that in a num-\nber of use cases the rules perform surprisingly well using little annotated data while\noffering superior explanatory power when compared to ML methods.\n\nReferences\n\n[1] Antonie, M-L., and Osmar R. Zaiane. \u201cText document categorization by term association.\u201d 2002 IEEE\nInternational Conference on Data Mining, 2002. Proceedings. IEEE, 2002.\n\n[2] Korde, Vandana, and C. Namrata Mahender. \u201cText classification and classifiers: A survey.\u201d International\nJournal of Artificial Intelligence & Applications 3.2 (2012): 85.\n\n[3] Yao, L., C. Mao, and Y. Luo. \u201cClinical text classification with rule-based features and knowledge-guided\nconvolutional neural networks.\u201d BMC medical informatics and decision making 19.3 (2019): 71.\n\n[4] Walker, Vern R., et al. \u201cAutomatic Classification of Rhetorical Roles for Sentences: Comparing Rule-\nBased Scripts with Machine Learning.\u201d Proceedings of ASAIL 2019 (2019).\n\n[5] Hipp, D. R., Kennedy, D., Mistachkin, J., SQLite FTS5 Extension (2019, September 20) SQLite https:\n\/\/sqlite.org\/fts5.html\n\n[6] Walker, Vern R., et al. \u201cSemantic types for computational legal reasoning: propositional connectives and\nsentence roles in the veterans\u2019 claims dataset.\u201d Proceedings of ICAIL \u201917. ACM, 2017.\n\n[7] Savelka, J., and Kevin D. Ashley. \u201cUsing CRF to detect different functional types of content in decisions\nof united states courts with example application to sentence boundary detection.\u201d ASAIL 2017.\n\n[8] Falakmasir, Mohammad Hassan, and Kevin D. Ashley. \u201dUtilizing Vector Space Models for Identifying\nLegal Factors from Text.\u201d JURIX. 2017.\n\n[9] Savelka, Jaromir, Huihui Xu, and Kevin D. Ashley. \u201cImproving Sentence Retrieval from Case Law for\nStatutory Interpretation.\u201d Proceedings of the ICAIL \u201919. ACM, 2019.\n\n[10] SKOPE-Rules. (2019, September 11). github.com\/scikit-learn-contrib\/skope-rules\n[11] Joulin, Armand, et al. \u201dBag of tricks for efficient text classification.\u201d preprint arXiv:1607.01759 (2016).\n[12] Friedman, Jerome H., and Bogdan E. Popescu. \u201cPredictive learning via rule ensembles.\u201d The Annals of\n\nApplied Statistics 2.3 (2008): 916-954.\n[13] Cohen, W. W., and Y. Singer. \u201cA simple, fast, and effective rule learner.\u201d AAAI\/IAAI 99 (1999): 335-342.\n[14] Weiss, Sholom M., and Nitin Indurkhya. \u201cLightweight rule induction.\u201d (2000).\n[15] Dembczyn\u0301ski, Krzysztof, Wojciech Kot\u0142owski, and Roman S\u0142owin\u0301ski. \u201cMaximum likelihood rule en-\n\nsembles.\u201d Proceedings of the 25th international conference on Machine learning. ACM, 2008.\n[16] Friedman, J., and Bogdan E. Popescu. Gradient directed regularization for linear regression and classi-\n\nfication. Technical Report, Statistics Department, Stanford University, 2003.\n[17] Random Forest Classifier. (2019, September 13). scikit-learn. scikit-learn.org\/stable\/\n\nmodules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\n[18] Support Vector Classification. (2019, September 13). scikit-learn. scikit-learn.org\/stable\/\nmodules\/generated\/sklearn.svm.SVC.html\n\n[19] FastText. (2019, September 11). Github repository. Retrieved from github.com\/\nfacebookresearch\/fastText\/tree\/master\/python\n\n[20] FastText. (2019, September 11). https:\/\/fasttext.cc\/\n\nhttps:\/\/sqlite.org\/fts5.html\nhttps:\/\/sqlite.org\/fts5.html\ngithub.com\/scikit-learn-contrib\/skope-rules\nhttp:\/\/arxiv.org\/abs\/1607.01759\nscikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nscikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nscikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\nscikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\ngithub.com\/facebookresearch\/fastText\/tree\/master\/python\ngithub.com\/facebookresearch\/fastText\/tree\/master\/python\nhttps:\/\/fasttext.cc\/\n\n","10":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCLASSIC: Continual and Contrastive Learning of Aspect\nSentiment Classification Tasks\n\nZixuan Ke1, Bing Liu1, Hu Xu2 and Lei Shu3\u2217\n1Department of Computer Science, University of Illinois at Chicago\n\n2Facebook AI Research\n3Amazon AWS AI\n\n1{zke4,liub}@uic.edu\n2huxu@fb.com\n\n3shulindt@gmail.com\n\nAbstract\nThis paper studies continual learning (CL) of\na sequence of aspect sentiment classification\n(ASC) tasks in a particular CL setting called\ndomain incremental learning (DIL). Each task\nis from a different domain or product. The\nDIL setting is particularly suited to ASC be-\ncause in testing the system needs not know the\ntask\/domain to which the test data belongs. To\nour knowledge, this setting has not been stud-\nied before for ASC. This paper proposes a\nnovel model called CLASSIC. The key novelty\nis a contrastive continual learning method that\nenables both knowledge transfer across tasks\nand knowledge distillation from old tasks to\nthe new task, which eliminates the need for\ntask ids in testing. Experimental results show\nthe high effectiveness of CLASSIC.1\n\n1 Introduction\n\nContinual learning (CL) learns a sequence of tasks\nincrementally. After learning a task, its training\ndata is often discarded (Chen and Liu, 2018). The\nCL setting is useful when the data privacy is a con-\ncern, i.e., the data owners do not want their data\nused by others (Ke et al., 2020b; Qin et al., 2020;\nKe et al., 2021). In such cases, if we want to lever-\nage the knowledge learned in the past to improve\nthe new task learning, CL is appropriate as it shares\nonly the learned model, but not the data. In our\ncase, a task is a separate aspect sentiment classifi-\ncation (ASC) problem of a product or domain (e.g.,\ncamera or phone) (Liu, 2012). ASC is stated as\nfollows: Given an aspect term (e.g., sound quality\nin a phone review) and a sentence containing the\naspect (e.g., \"The sound quality is poor\"), ASC\nclassifies whether the sentence expresses a positive,\nnegative, or neutral opinion about the aspect.\n\nThere are three CL settings (van de Ven\nand Tolias, 2019): Class Incremental Learning\n\n\u2217 Work was done prior to joining Amazon.\n1https:\/\/github.com\/ZixuanKe\/\n\nPyContinual\n\n(CIL), Task Incremental Learning (TIL), and Do-\nmain Incremental Learning (DIL). In CIL, the tasks\ncontain non-overlapping classes. Only one model\nis built for all classes seen so far. In testing, no task\ninformation is provided. This setting is not suitable\nfor ASC as ASC tasks have the same three classes.\nTIL builds one model for each task in a shared net-\nwork. In testing, the system needs the task (e.g.,\nphone domain) that each test instance (e.g., \"The\nsound quality is great\") belongs to and uses only\nthe model for the task to classify the instance. Re-\nquiring the task information (e.g., phone domain)\nis a limitation. Ideally, the user should not have to\nprovide this information for a test sentence. That\nis the DIL setting, i.e., all tasks sharing the same\nfixed classes (e.g., positive, negative, and neutral).\nIn testing, no task information is required.\n\nThis work uses the DIL setting to learn a se-\nquence of ASC tasks in a neural network. The key\nobjective is to transfer knowledge across tasks to\nimprove classification compared to learning each\ntask separately. An important goal of any CL is to\novercome catastrophic forgetting (CF) (McCloskey\nand Cohen, 1989), which means that in learning a\nnew task, the system may change the parameters\nlearned for previous tasks and cause their perfor-\nmance to degrade. We solve the CF problem as\nwell; otherwise we cannot achieve improved accu-\nracy. However, sharing the classification head for\nall tasks in DIL makes cross-task interfere\/update\ninevitable. Without task information provided in\ntesting makes DIL even more challenging.\n\nPrevious research has shown that one of the most\neffective approaches for ASC (Xu et al., 2019; Sun\net al., 2019) is to fine-tune the BERT (Devlin et al.,\n2019) using the training data. However, our experi-\nments show that this works poorly for DIL because\nthe fine-tuned BERT on a task captures highly task\nspecific features that are hard to use by other tasks.\n\nIn this paper, we propose a novel model called\nCLASSIC (Continual and contrastive Learning for\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n02\n71\n\n4v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 5\n\n D\nec\n\n 2\n02\n\n1\n\nhttps:\/\/github.com\/ZixuanKe\/PyContinual\nhttps:\/\/github.com\/ZixuanKe\/PyContinual\n\n\nASpect SentIment Classification) in the DIL set-\nting. Instead of fine-tuning BERT for each task,\nwhich causes serious CF, CLASSIC uses the idea\nof Adapter-BERT in (Houlsby et al., 2019) to avoid\nchanging BERT parameters and yet achieve equally\ngood results as BERT fine-tuning. A novel contra-\ntive continual learning method is proposed (1) to\ntransfer the shareable knowledge across tasks to\nimprove the accuracy of all tasks, and (2) to distill\nthe knowledge (both shareable and not shareable)\nfrom previous tasks to the model of the new task\nso that the new\/last task model can perform all\ntasks, which eliminates the need for task informa-\ntion (e.g., task id) in testing. Existing contrastive\nlearning (Chen et al., 2020) cannot do these.\n\nTask masks are also learned and used to pro-\ntect task-specific knowledge to avoid forgetting\n(CF). Extensive experiments have been conducted\nto show the effectiveness of CLASSIC.\n\nIn summary, this paper makes the following con-\ntributions: (1) It proposes the problem of domain\ncontinual learning for ASC, which has not been at-\ntempted before. (2) It proposes a new model called\nCLASSIC that uses adapters to incorporate the pre-\ntrained BERT into the ASC continual learning, a\nnovel contrastive continual learning method for\nknowledge transfer and distillation, and task masks\nto isolate task-specific knowledge to avoid CF.\n\n2 Related Work\n\nSeveral researchers have studied lifelong or contin-\nual learning for sentiment analysis. Early works\nare done under Lifelong Learning (LL) (Silver\net al., 2013; Ruvolo and Eaton, 2013; Chen and Liu,\n2014). Two Naive Bayes (NB) approaches were\nproposed to improve the new task learning (Chen\net al., 2015; Wang et al., 2019). Xia et al. (2017)\nproposed a voting based approach. All these sys-\ntems work on document sentiment classification\n(DSC). Shu et al. (2017) used LL for aspect extrac-\ntion. These works do not use neural networks, and\nhave no CF problem.\n\nL2PG (Qin et al., 2020) uses a neural network\nbut improves only the new task learning for DSC.\nWang et al. (2018) worked on ASC, but since they\nimprove only the new task learning, they did not\ndeal with CF. Each task uses a separate network.\n\nExisting CL systems SRK (Lv et al., 2019) and\nKAN (Ke et al., 2020b) are for DSC in the TIL\nsetting, not for ASC. B-CL (Ke et al., 2021) is\nthe first CL system for ASC. It also uses the idea\n\nof Adapter-BERT in (Houlsby et al., 2019) and is\nbased on Capsule Network. More importantly, B-\nCL works in the TIL setting. The proposed CLAS-\nSIC system is based on contrastive learning and\nworks in the DIL setting for ASC, which is a more\nrealistic setting for practical applications.\n\nGeneral Continual Learning (CL): CL has been\nstudied extensively in machine learning (Chen and\nLiu, 2018; Parisi et al., 2019). Existing work\nmainly focuses on dealing with CF. There are sev-\neral main approaches. (1) Regularization-based\napproaches such as those in (Kirkpatrick et al.,\n2016; Lee et al., 2017) add a regularization in the\nloss to consolidate previous knowledge when learn-\ning a new task. (2) Parameter isolation-based\napproaches such as those in (Serr\u00e0 et al., 2018;\nKe et al., 2020a; Abati et al., 2020) make differ-\nent subsets of the model parameters dedicated to\ndifferent tasks and identify and mask them out\nduring the training of the new task. (3) Replay-\nbased approaches such as those in (Rebuffi et al.,\n2017; Lopez-Paz and Ranzato, 2017; Chaudhry\net al., 2019) retain an exemplar set of old task train-\ning data to help train the new task. The methods\nin (Shin et al., 2017; Kamra et al., 2017; Rostami\net al., 2019; He and Jaeger, 2018) build data gener-\nators for previous tasks so that in learning the new\ntask, the generated data for previous tasks can help\navoid CF.\n\nThese methods are for overcoming CF in the CIL\nor TIL setting of CL. Limited work has been done\non knowledge transfer, which is our goal. There\nis little work in the DIL setting except the replay\nmethod DER++ (Buzzega et al., 2020), which saves\nsome past data. CLASSIC saves no past data.\n\nContrastive learning (Chen et al., 2020; He et al.,\n2020) is the base of our contrastive continual learn-\ning method. However, there is a major difference.\nExisting contrastive learning uses various transfor-\nmations (e.g., rotation and cropping) of the existing\ndata (e.g., images) to generate different views of\nthe data. However, we use the hidden space in-\nformation from the previous task models to create\nviews for explicit knowledge transfer and distilla-\ntion. Existing contrastive learning cannot do that.\n\n3 Proposed CLASSIC Method\n\nState-of-the-art ASC systems all use BERT (De-\nvlin et al., 2019) or other language models as the\nbase. The proposed technique CLASSIC adopts\nthe BERT-based ASC formulation in (Xu et al.,\n\n\n\nFigure 1: CLASSIC adopts Adapter-BERT (Houlsby\net al., 2019) and its adapters (yellow boxes) in a trans-\nformer (Vaswani et al., 2017) layer (above (CSC)). An\nadapter is a 2-layer fully connected network with a skip-\nconnection. It is added twice to each Transformer layer.\nOnly the adapters and layer norm (green boxes) layers\nare trainable. The other modules (grey boxes) of BERT\nare frozen. (CSC): CSC loss is computed based on the\ncurrent task model (details in Sec. 3.4). (CED): CED\nloss is computed based on all previous tasks from 1 to\nt \u2212 1 (details in Sec. 3.2). (CKS): CKS loss is com-\nputed based on previous and current tasks and a task-\nbased self-attention. Details are given in Sec. 3.3.\n\n2019), where the aspect term (e.g., sound quality)\nand review sentence (e.g., \"The sound quality is\ngreat\") are concatenated via [SEP]. The sentiment\npolarity is predicted on top of the [CLS] token.\nAs indicated earlier, although BERT can achieve\nstate-of-the-art performance on a single task, its\narchitecture and fine-tuning are unsuitable for CL\n(see Sec. 1) and perform very poorly (Sec. 4.4). We\nfound that the BERT adapter idea in (Houlsby et al.,\n2019) is a better fit for CL.\n\nBERT Adapter. The idea was given in Adapter-\nBERT (Houlsby et al., 2019), which inserts two 2-\nlayer fully-connected networks (adapters) in each\ntransformer layer of BERT (Figure 1(CSC)). Dur-\ning training for the end-task, only the adapters and\nnormalization layers are updated. All the other\nBERT parameters are frozen. This is good for CL\nas fine-tuning the BERT causes serious forgetting.\nAdapter-BERT achieves similar accuracy to the\nfine-tuned BERT (Houlsby et al., 2019).\n\n3.1 Overview of CLASSIC\n\nThe architecture of CLASSIC is given in Figure 1,\nwhich works in the DIL setting for ASC. It uses\nAdapter-BERT to avoid fine-tuning BERT. CLAS-\nSIC takes two inputs in training: (1) hidden states\nh(t) from the feed-forward layer of a transformer\nlayer of BERT and (2) task id t (no task id is needed\n\nin testing, see Sec. 3.2.3). The outputs are hidden\nstates with features for task t to build a classifier.\n\nCLASSIC uses three sub-systems to achieve\nits objectives (see Sec. 1): (1) contrastive ensem-\nble distillation (CED) for mitigating CF by dis-\ntilling the knowledge of previous tasks to the cur-\nrent task model; (2) contrastive knowledge sharing\n(CKS) to encourage knowledge transfer; and (3)\ncontrastive supervised learning on the current task\nmodel (CSC) to improve the current task model\naccuracy. We call this framework contrastive con-\ntinual learning, inspired by contrastive learning.\n\nContrastive learning uses multiple views of the\nexisting data for representation learning to group\nsimilar data together and push dissimilar data far\naway, which makes it easier to learn a more accu-\nrate classifier. It uses various transformations of the\nexisting data to create useful views. Given a mini-\nbatch of N training examples, if we create another\nview for each example, the batch will have 2N ex-\namples. We assume that i and j are two views of\nthe training example. If we use i as the anchor,\n(i, j) is called a positive pair. All other pairs (i, k)\nfor k 6= i are negative pairs. The contrastive loss\nfor this positive pair is (Chen et al., 2020),\n\nLi,j = \u2212 log\nexp((hi \u00b7 hj)\/\u03c4)\u22112N\n\nk=1 1k 6=j exp((hi \u00b7 hk)\/\u03c4)\n, (1)\n\nwhere the dot product hi \u00b7 hj is regarded as a sim-\nilarity function in the hidden space and \u03c4 is tem-\nperature. The final loss for the batch is calculated\nacross all positive pairs. Eq. 1 is for unsupervised\ncontrastive learning. It can also be used for su-\npervised contrastive learning, where any two in-\nstances\/views from the same class form a positive\npair, and any instance of a class and any instance\nfrom other classes form a negative pair.\n\n3.2 Overcoming Forgetting via Contrastive\nEnsemable Distillation (CED)\n\nThe CED objective is to deal with CF. We first\nintroduce task masks that CED relies on to preserve\nthe previous task knowledge\/models to be distilled\nto the new task model to avoid CF.\n\n3.2.1 Task Masks (TMs)\nGiven the input hidden states h(t) from the feed-\nforward layer of a transformer layer, the adapter\nmaps them into input k(t)l via a fully-connected\nnetwork, where l is the l-th layer of the adapter. A\nTM (a \u201csoft\u201d binary mask) m(t)l is trained for each\n\n\n\nFigure 2: Illustration of task masking: a (learnable)\ntask mask is applied after the activation function to se-\nlectively activate a neuron (or feature). The four rows\nof each task corresponds to the two fully-connected lay-\ners and their corresponding task masks. In the neurons\nbefore training, those with 0\u2019s are the neurons to be pro-\ntected (masked) and those neurons without a number\nare free neurons (not used). In the neurons after train-\ning, those with 1\u2019s show neurons that are important for\nthe current task, which are used as masks for the fu-\nture. Those neurons with more than one color indicate\nthat they are shared by more than one task. Those 0\nneurons without a color are not used by any task.\n\ntask t at each layer l in the adapter during training\ntask t\u2019s classifier, indicating the neurons that are\nimportant for the task in the layer. Here we borrow\nthe hard attention idea in (Serr\u00e0 et al., 2018) and\nleverage the task id embedding to train the TMs.\n\nFor a task id t, its embedding e(t)l consists of dif-\nferentiable parameters that can be learned together\nwith other parts of the network and it is trained for\neach layer in the adapter. To generate the TM m(t)l\nfrom e(t)l , Sigmoid is used as a pseudo-gate and\na positive scaling hyper-parameter s is applied to\nhelp training. The m(t)l is computed as follows:\n\nm\n(t)\nl = \u03c3(se\n\n(t)\nl ). (2)\n\nNote that the neurons in m(t)l may overlap with\n\nthose in otherm\n(iprev)\nl s from previous tasks showing\n\nsome shared knowledge. Given the output of each\nlayer in the adapter, k(t)l , we element-wise multiply\nk\n(t)\nl \u2297 m\n\n(t)\nl . The masked output of the last layer\n\nk(t) is fed to the next layer of the BERT with a skip-\nconnection (see Figure 2) After learning task t, the\nfinal m(t)l is saved and added to the set {m\n\n(t)\nl }.\n\n3.2.2 Training Task Masks (TMs)\nFor each previous task iprev \u2208 Tprev, its TM m\n\n(iprev)\nl\n\nindicates which neurons are used by that task and\nneed to be protected. In learning task t, m\n\n(iprev)\nl is\n\nused to set the gradient g(t)l on all used neurons of\nthe layer l to 0. Before modifying the gradient, we\nfirst accumulate all used neurons by all previous\ntasks TMs. Since m\n\n(iprev)\nl is binary, we use max-\n\npooling to achieve the accumulation:\n\nm\n(tac)\nl = MaxPool({m\n\n(iprev)\nl }). (3)\n\nThe term m(tac)l is applied to the gradient:\n\ng\n\u2032(t)\nl = g\n\n(t)\nl \u2297 (1\u2212m\n\n(tac)\nl ). (4)\n\nThose gradients corresponding to the 1 entries in\nm\n\n(tac)\nl are set to 0 while the others remain un-\n\nchanged. In this way, neurons in an old task are\nprotected. Note that we expand (copy) the vector\nm\n\n(tac)\nl to match the dimensions of g\n\n(t)\nl .\n\nThough the idea is intuitive, e(t)l is not easy to\ntrain. To make the learning of e(t)l easier and more\nstable, an annealing strategy is applied (Serr\u00e0 et al.,\n2018). That is, s is annealed during training, in-\nducing a gradient flow and set s = smax during\ntesting. Eq. 2 approximates a unit step function as\nthe mask, with m(t)l \u2192 {0, 1} when s \u2192 \u221e. A\ntraining epoch starts with all neurons being equally\nactive, which are progressively polarized within the\nepoch. Specifically, s is annealed as follows:\n\ns =\n1\n\nsmax\n+ (smax \u2212\n\n1\n\nsmax\n)\nb\u2212 1\nB \u2212 1\n\n, (5)\n\nwhere b is the batch index andB is the total number\nof batches in an epoch.\n\nIllustration. In Figure 2, after learning Task\n1, we obtain its useful neurons marked in orange\nwith a \u201c1\u201d in each neuron, which serves as a mask\nin learning future tasks. In learning Task 2, those\nuseful neurons for Task 1 are masked (with \u201c0\u201d in\nthose orange neurons on the left). The process\nalso learns the useful neurons for Task 2 marked in\ngreen with \u201c1\u201ds. When Task 3 arrives, all neurons\nfor Tasks 1 and 2 are masked, i.e., its TM entries are\nset to 0 (orange and green before training). After\ntraining Task 3, we see that Task 3 and Task 2\nhave a shared neuron that is important to both. The\nshared neuron is marked in both red and green.\n\n3.2.3 Contrastive Ensemble Distillation\n(CED)\n\nThe TMs mechanism isolates different parameters\nfor different tasks. This seems to be perfect for\n\n\n\novercoming forgetting since the previous task pa-\nrameters are fixed and cannot be updated by future\ntasks. However, since DIL setting does not have\ntask id in testing, we cannot directly take the ad-\nvantage of the TMs. To address this issue, we\npropose the CED objective to help distill all pre-\nvious knowledge to the current task model so that\nwe can simply use the last model as the final model\nwithout requiring the task id in testing.\n\nRepresentation of Previous Tasks. Recall that\nwe know which neurons\/units are for which task i\nby reading {m(i)l }. For each previous task i of the\ncurrent task t, we can compute its masked output of\nAdapter-BERT h(i)m (the layer before the classifica-\ntion head) by applying m(i)l to the Adapter-BERT.\n\nEnsemble Distillation Loss. We distill the\nknowledge of the ensemble of previous tasks into\nthe single current task model. As we have a shared\nclassification head for all tasks in DIL, which is ex-\nposed to forgetting, the distillation should be based\non the output of the classification head. Specifi-\ncally, given a previous task\u2019s Adapter-BERT out-\nput h(i)m , we compute the output of the classifica-\ntion head using h(i)m , which gives us the logit (un-\nnormalized prediction) value z(i)m . We then distill\nthe knowledge using z(i)m and the current task clas-\nsification head output z(t)m based on contrastive loss,\ninspired by (Tian et al., 2020a),\n\nL(i)CED =\n2N\u2211\nn=1\n\n\u2212 log\nexp((z\n\n(i)\nm:2n\u22121 \u00b7 z\n\n(t)\nm:2n)\/\u03c4)\u22112N\n\nj=1\n1n 6=j exp((z\n\n(i)\nm:n \u00b7 z\n\n(t)\nm:j)\/\u03c4)\n\n,\n\n(6)\n\nwhere N is the batch size and \u03c4 > 0 is an ad-\njustable temperature parameter controlling the sep-\naration of classes. The index n is the anchor and\nthe notation z(i)m:n refers to the n-th sample in z\n\n(i)\nm .\n\nz\n(i)\nm:2n\u22121 and z\n\n(t)\nm:2n are the logits of previous and\n\ncurrent task models for the same input sample, a\npositive pair in contrastive learning. All the other\npossible pairs are negative pairs. Note that for each\nanchor i, there is 1 positive pair and 2N \u2212 2 nega-\ntive pairs. The denominator has a total of 2N \u2212 1\nterms (both the positives and negatives). Note that\nthe previous task models are fixed and thus can\nserve as teacher networks. As we have i \u2265 1 previ-\nous tasks, hence i \u2265 1 teacher networks but only\none current task student network. We adopt the con-\ntrastive framework by defining multiple pair-wise\ncontrastive losses between z(i)m and z\n\n(t)\nm . These\n\nlosses are summed up to give the final CED loss,\n\nLCED =\nt\u22121\u2211\ni=1\n\nL(i)CED. (7)\n\n3.3 Transferring Knowledge via Contrastive\nKnowledge Sharing (CKS)\n\nCKS aims to capture the shared knowledge among\ntasks and help the new task learn a better represen-\ntation and better classifier. The intuition of CKS\nis as follows: Contrasive learning has the ability\nto capture the shared knowledge between differ-\nent views (Tian et al., 2020b; van den Oord et al.,\n2018). This is achieved by seeking representation\nthat are invariant cross similar views. If we can\ngenerate a view from previous tasks that is similar\nto the current task, the contrastive loss can capture\nthe shared knowledge and learn a representation\nfor knowledge transfer to the new task learning.\nBelow, we first introduce how to construct such a\nview and use it in the CKS objective.\n\n3.3.1 Task-based Self-Attention\nIntuitively, the more similar the two tasks are, the\nmore shared knowledge they have. To achieve our\ngoal, we should combine all similar tasks as the\nshared knowledge view. In order to focus on the\nsimilar tasks, we propose to use task-based self-\nattention mechanism to attend to them. Inspired\nby (Zhang et al., 2018), given the concatenation\nof the output of Adapter-BERT for all previous\nand current tasks, h(\u2264t)m = cat({h\n\n(i)\nm }ti=1), and task\n\ni \u2264 t, we first transform it into two feature spaces\nvia f(h(i)m ) = Wfh\n\n(i)\nm , g(h\n\n(i)\nm ) = Wgh\n\n(i)\nm (see Fig-\n\nure 1(CKS)).\nTo compare the similarity between tasks i \u2264 t\n\nand j \u2264 t, we calculate similarity sij via\n\nsij = f(h\n(i)\nm )\n\nT g(h(j)m ). (8)\n\nWe then compute the attention score \u03b1j,i to indicate\nwhich similar tasks (similar to the current task t)\nshould be attended to based on the current task data,\n\n\u03b1j,i =\nexp(sij)\u2211t\ni=1 exp(sij)\n\n. (9)\n\nThe attention score is applied to each task in h(\u2264t)m\nto get the attention output oj using weighted sum:\n\noj = v(\nt\u2211\n\ni=1\n\n\u03b1j,iq(h\n(i)\nm )), (10)\n\n\n\nwhere v(\u00b7) and q(\u00b7) are two functions for trans-\nforming feature spaces: v(h(i)m ) = Wvh\n\n(i)\nm and\n\nq(h\n(i)\nm ) =Wqh\n\n(i)\nm .\n\nLastly, we multiply the output of the attention\nlayer by a scale parameter and add back to the input\nfeature h(\u2264t)m . The final output of the h\n\n(\u2264t)\nCKS is the\n\nsum over all considered tasks,\n\nh\n(\u2264t)\nCKS =\n\nt\u2211\ni=1\n\n(\u03b3oi + h\n(i)\nm ), (11)\n\nwhere \u03b3 is a learnable scalar and it is initialized\nto 0. This allows the model to first learn on the\ncurrent task and then gradually learn to assign more\nweights to other tasks.\n\n3.3.2 Knowledge Sharing Loss\nThe output of the task-based self-attention provides\nus the knowledge sharing view h(\u2264t)CKS . Along with\nthe output of Adapter-BERT for the current task\nh\n(t)\nm , we can easily perform contrastive learning be-\n\ntween these two views. Note that h(\u2264t)CKS is computed\nbased on the current task data and their correspond-\ning class labels, so we give the two views have\nthe same label and thus we can integrate the label\ninformation in our CKS loss,\n\nLCKS =\nN\u2211\n\nn=1\n\n\u2212\n1\n\nNyn \u2212 1\n\nN\u2211\nj=1\n\n1n\u2264j1yn=yj\n\nlog\nexp((h\n\n(\u2264t)\nCKS:n \u00b7 h\n\n(t)\nm:j)\/\u03c4)\u2211N\n\nk=1\n1n 6=k exp((h\n\n(\u2264t)\nCKS:n \u00b7 h\n\n(t)\nm:k)\/\u03c4)\n\n,\n\n(12)\n\nwhere N is the batch size and Nyn is the number\nof examples in the batch that have the label yn.\nh\n(\u2264t)\nCKS is the first view while h\n\n(t)\nm is the second view.\n\nThe shared knowledge between them represents\nthe shared knowledge between previous and cur-\nrent tasks. Different from the CED loss, the CKS\nloss leverages the class information and thus can\nhave multiple positive pairs decided by whether\ntwo samples share the same class label.\n\n3.4 Contrastive Supervised Learning of the\nCurrent Task (CSC)\n\nWe further improve the performance of the current\ntask by adopting the supervised contrastive loss\n(Khosla et al., 2020) on the current task h(t)m ,\n\nLCSC =\nN\u2211\n\nn=1\n\n\u2212\n1\n\nNyn \u2212 1\n\nN\u2211\nj=1\n\n1n\u2264j1yn=yj\n\nlog\nexp((h\n\n(t)\nm:n \u00b7 h\n\n(t)\nm:j)\/\u03c4)\u2211N\n\nk=1\n1n 6=k exp((h\n\n(t)\nm:n \u00b7 h\n\n(t)\nm:k)\/\u03c4)\n\n.\n\n(13)\n\nData source Task\/domain Train Validation Test\n\nLiu3domain\nSpeaker 352 44 44\nRouter 245 31 31\n\nComputer 283 35 36\n\nHL5domain\n\nNokia6610 271 34 34\nNikon4300 162 20 21\n\nCreative 677 85 85\nCanonG3 228 29 29\nApexAD 343 43 43\n\nDing9domain\n\nCanonD500 118 15 15\nCanon100 175 22 22\n\nDiaper 191 24 24\nHitachi 212 26 27\n\nIpod 153 19 20\nLinksys 176 22 23\n\nMicroMP3 484 61 61\nNokia6600 362 45 46\n\nNorton 194 24 25\n\nSemEval14\nRest. 3452 150 1120\n\nLaptop 2163 150 638\n\nTable 1: Number of sentences in each task or dataset.\nMore detailed statistics are given in Supplementary.\n\n3.5 Final Loss\n\nThe final loss is the weighted average of the super-\nvised cross entropy (CE) loss, CSC loss, and the\nproposed CED and CKS losses:\n\nL = LCE + \u03bb1LCSC + \u03bb2LCED + \u03bb3LCKS. (14)\n\n4 Experiments\n\nThis section evaluates the proposed CLASSIC sys-\ntem and compares it with both non-continual learn-\ning and continual learning baselines.\n\n4.1 Experiment Datasets\n\nWe use 19 ASC datasets to produce sequences of\n19 tasks. Each dataset is a set of aspect and sen-\ntiment annotated review sentences from reviews\nof a particular product and represents a task. The\ndatasets are from 4 sources: (1) HL5Domains (Hu\nand Liu, 2004): review sentences of 5 products;\n(2) Liu3Domains (Liu et al., 2015): review sen-\ntences of 3 products; (3) Ding9Domains (Ding\net al., 2008): review sentences of 9 products; and\n(4) SemEval14: review sentences of 2 products -\nSemEval 2014 Task 4 for laptop and restaurant. To\nbe consistent with the existing research (Tang et al.,\n2016), sentences with both positive and negative\nsentiments about an aspect are not used. Statistics\nof the 19 datasets are given in Table 1.\n\n\n\n4.2 Compared Baselines\n\nWe employ 46 baselines, which include both non-\ncontinual learning and continual learning methods.\nSince little work has been done in DIL, we adapt\nthe recent TIL systems to DIL by merging classifi-\ncation heads to form DIL systems.\n\nNon-Continual Learning Baselines: Each of\nthese baselines builds a separate model for each\ntask independently, which we call a ONE variant.\nIt thus has no knowledge transfer or CF. There are\n8 ONE variants. Four are created using (1) BERT\nwith fine-tuning, (2) BERT (Frozen) without fine-\ntuning (3) Adapter-BERT (Houlsby et al., 2019)\nand (4) W2V (word2vec embeddings trained with\nthe Amazon review data in (Xu et al., 2018) using\nFastText (Grave et al., 2018)). Adding CSC (Con-\ntrastive Supervised learning of the Current task)\ncreates another 4 variants. We adopt the ASC net-\nwork in (Xue and Li, 2018), taking aspect term and\nreview sentence as input for BERT variants. For\nW2V variants, we use their concatenation.\n\nContinual Learning (CL) Baselines. The CL\nsetting has 38 baselines in 5 categories. The first\ncategory uses a naive CL (NCL) approach. It sim-\nply uses a network to learn all tasks with no mech-\nanism to deal with CF or knowledge transfer. Like\nONE, we have 8 NCL variants. The second cate-\ngory has 11 baselines created using recent CL meth-\nods KAN (Ke et al., 2020b), SRK (Lv et al., 2019),\nHAT (Serr\u00e0 et al., 2018), UCL (Ahn et al., 2019),\nEWC (Kirkpatrick et al., 2016), OWM (Zeng et al.,\n2019) and DER++ (Buzzega et al., 2020). KAN\nand SRK are for document sentiment classifica-\ntion. We use the concatenation of the aspect and\nthe sentence as input. HAT, UCL, EWC, OWM\nand DER++ were originally designed for image\nclassification. We replace their original image clas-\nsification networks with CNN for text classification\n(Kim, 2014). HAT is one of the best TIL methods\nwith almost no forgetting. UCL is a recent TIL\nmethod. EWC is a popular CIL method, which was\nadapted for TIL in (Serr\u00e0 et al., 2018). They are\nconverted to DIL versions by merging their classi-\nfication heads. OWM (Zeng et al., 2019) is a CIL\nmethod, which we also adapt to a DIL method like\nEWC. DER++ and SRK can work in the DIL set-\nting. HAT and KAN require task id as an input in\ntesting and cannot function in the DIL setting. We\ncreate two variants of HAT (and KAN): using the\nlast model in testing as CLASSIC does or detect-\ning task id using the entropy method ent in (von\n\nOswald et al., 2020). This category uses BERT\n(Frozen) as the base. The third category has 7 base-\nlines using Adapter-BERT. KAN and SRK cannot\nbe adapted to use adapters. The fourth category\nuses W2V, which gives another 11 baselines. The\nfinal category has one baseline LAMOL (Sun et al.,\n2020), which uses the GPT-2 model.\n\nEvaluation Protocol: We follow the standard CL\nevaluation method in (Lange et al., 2019). We first\npresent CLASSIC a sequence of ASC tasks for it\nto learn. Once a task is learned, its training data is\ndiscarded. After all tasks are learned, we test using\nthe test data of all tasks without giving task ids.\n\n4.3 Hyperparameters\n\nUnless otherwise stated, the adapter uses 2 layers\nof fully connected network with dimensions 2000.\nThe task id embeddings have 2000 dimensions. A\nfully connected layer with softmax output is used as\nthe classification head in the last layer of BERT. We\nuse 400 for smax in Eq. 5, dropout of 0.5 between\nfully connected layers. The temperature \u03c4 in each\ncontrastive objective is set to 1 (see Supplementary\nfor parameter tuning). The weight of each objective\nin Eq. 14 is set to 1. We use the embedding of\n[CLS] as the output of Adapter-BERT. For CKS\nand CSC, we use l2 normalization on the output of\nAdapter-BERT before computing the contrastive\nloss. The training of BERT, Adapter-BERT and\nCLASSIC follow that of (Xu et al., 2019). We\nadopt BERTBASE (uncased). The max length of\nthe sum of sentence and aspect is 128. We use\nAdam optimizer and set the learning rate to 3e-5.\nFor the SemEval datasets, 10 epochs are used and\nfor all other datasets, 30 epochs are used based\non results from validation data. All runs use the\nbatch size 32. For CL baselines, we train all models\nwith the learning rate of 0.05, early-stop training\nwhen there is no improvement in the validation loss\nfor 5 epochs and set the batch size to 64. We use\nthe code provided by their authors and adopt their\noriginal parameters (for EWC, we adopt the variant\nimplemented by (Serr\u00e0 et al., 2018)).\n\n4.4 Results and Analysis\n\nAs the order of the 19 tasks can influence the fi-\nnal results, we randomly select and run 5 task se-\nquences and report their average results in Table 2.\nWe compute both accuracy and Macro-F1, where\nMacro-F1 is the main metric as the imbalanced\nclasses introduce biases in accuracy.\n\n\n\nScenario Category Model Acc. MF1\n\nNon-continual\nLearning\n\nBERT\nONE 0.8584 0.7635\n\nONE+csc 0.8353 0.7388\n\nBERT (Frozen)\nONE 0.7814 0.5813\n\nONE+csc 0.8265 0.7232\n\nAdapter-BERT\nONE 0.8596 0.7807\n\nONE+csc 0.8530 0.7516\n\nW2V\nONE 0.7701 0.5189\n\nONE+csc 0.7761 0.5487\n\nContinual\nLearning\n\nBERT\nNCL 0.8048 0.7085\n\nNCL+csc 0.7727 0.5807\n\nBERT (Frozen)\nNCL 0.8685 0.7873\n\nNCL+csc 0.8693 0.7912\n\nAdapter-BERT\nNCL 0.8667 0.7804\n\nNCL+csc 0.8809 0.7847\n\nW2V\nNCL 0.8408 0.7455\n\nNCL+csc 0.8396 0.7509\n\nBERT\n(Frozen)\n\nKAN \u2014\nKAN+last 0.8320 0.7352\nKAN+ent 0.8278 0.7243\n\nSRK 0.8391 0.7438\nEWC 0.8660 0.7831\nUCL 0.8538 0.7690\n\nOWM 0.8611 0.7665\nDER++ 0.8753 0.8009\n\nHAT \u2014\nHAT+last 0.8473 0.7649\nHAT+ent 0.8418 0.7614\n\nAdapter-BERT\n\nEWC 0.8805 0.7875\nUCL 0.7123 0.3961\n\nOWM 0.8766 0.7882\nDER++ 0.8859 0.7985\n\nHAT \u2014\nHAT+last 0.8823 0.7919\nHAT+ent 0.8854 0.8245\n\nW2V\n\nKAN \u2014\nKAN+last * 0.7123 0.3961\nKAN+ent * 0.7123 0.3961\n\nSRK * 0.7123 0.3961\nEWC 0.7586 0.6545\nUCL 0.8187 0.6965\n\nOWM 0.8256 0.7253\nDER++ 0.8459 0.7722\n\nHAT \u2014\nHAT+last 0.7599 0.5849\nHAT+ent 0.7605 0.5349\n\nLAMOL 0.8891 0.8059\nCLASSIC (forward) 0.8886 0.8365\n\nCLASSIC 0.9022 0.8512\n\nTable 2: Accuracy (Acc.) and Macro-F1 (MF1) av-\neraged over 5 random sequences of 19 tasks. KAN\nand HAT need task id in testing and thus have no re-\nsults. KAN and SRK (RNN based) cannot work with\nAdapters. *: KAN and SRK under W2V fail to train.\nStandard deviation showing statistical significance,\nnetwork size and running time are in Supplementary.\n\nOverall, Table 2 shows that CLASSIC outper-\nforms all baselines markedly.\n\n(1). For non-continual learning baselines (ONE\nvariants), Adapter-BERT performs similarly to\nBERT (fine-tuning). Both BERT (Frozen) and\nW2V variants are weaker, which is understandable.\n\n(2). Comparing ONE variants and NCL variants,\nwe see that under W2V, NCL variants are much bet-\n\nter than ONE variants. This indicates ASC tasks are\nsimilar and have shared knowledge. Catastrophic\nforgetting (CF) is not a major issue for W2V.\n\nHowever, BERT NCL (fine-tuning) is much\nworse than BERT ONE and Adapter-BERT NCL\n(adapter-tuning) as BERT fine-tuning learns highly\ntask specific knowledge (Merchant et al., 2020).\nWhile this is desirable for ONE, it is bad for NCL\nbecause task specific knowledge is hard to share\nacross tasks, which causes forgetting (CF). The\n+csc options are poor for BERT ONE and NCL.\n\n(3). Various continual learning (CL) baselines\nwith BERT (Frozen) are also markedly weaker than\nCLASSIC. Baselines that can use Adapter-BERT\nare also much poorer than CLASSIC. Note that\nSRK and KAN cannot work with Adapter-BERT.\n\n(4). W2V based CL baselines are even weaker.\n(5). Since both KAN and HAT need task id in\n\ntesting and the DIL setting does not provide task\nid, they have no results. But we use the last model\n(+last) or use an existing entropy-based method\n(+ent) (von Oswald et al., 2020) to automatically\nidentify the task id for each test instance. These\nvariants are also markedly weaker than CLASSIC.\n\n(6). LAMOL is based on GPT-2 and its perfor-\nmance is weaker than CLASSIC too.\n\nEffectiveness of Knowledge Transfer. The re-\nsults under CLASSIC(forward) in Table 2 are the\naverage results computed using the accuracy\/MF1\nof each task when it was first learned. The re-\nsults under CLASSIC are the final average results\nafter all tasks are learned, including backward\ntransfer. By comparing ONE variants and CLAS-\nSIC(forward), we can see whether forward transfer\nis effective. By comparing CLASSIC(forward) and\nCLASSIC, we can see whether the backward trans-\nfer can improve further. We see both forward and\nbackward transfers are effective.\n\n4.5 Ablation Experiments\n\nThe results of ablation experiments are given in Ta-\nble 3. \u201c-CKS\u201d, \u201c-CSC\u201d and \u201c-CED\u201d mean without\nconstrastive knowledge sharing, contrastive super-\nvised learning on the current task and contrastive\nensemble distillation, respectively. Table 3 clearly\nshows that each of the components is effective and\nthey work in concert to produce the best final result.\n\n5 Conclusion\n\nThis paper studied domain incremental learning\n(DIL) of a sequence of ASC tasks without knowing\n\n\n\nModel Acc. MF1\nCLASSIC 0.9022 0.8512\n-CSC 0.8872 0.8007\n-CKS 0.8915 0.8232\n-CED 0.8828 0.7934\n-CKS,-CED 0.8864 0.7969\n-CKS,-CSC 0.8926 0.8346\n-CED,-CSC 0.8868 0.8032\n-CED,-CKS,-CSC 0.8823 0.7919\n\nTable 3: Ablation experiment results.\n\nthe task ids in testing. Our method CLASSIC uses\nadapters to exploit BERT and to deal with BERT\nCF in fine-tuning, and the proposed contrastive\ncontinual learning to transfer knowledge across\ntasks and to distill knowledge from previous tasks\nto the current task so that the last model can be\nused for all tasks in testing and no task id is needed.\nOur experimental results show that CLASSIC out-\nperforms the state-of-the-art baselines.\n\nFinally, we believe that the idea of CLASSIC\nis also applicable to some other NLP tasks. For\nexample, in named entity extraction, we can build\na better model to extract the same types of entities\nfrom text of different domains. Each domain works\non the same task but no data sharing (the data may\nbe from different clients with privacy concerns).\nSince this is an extraction task, the backbone model\nneeds to be switched to an extraction model.\n\nAcknowledgments\n\nThis work was supported in part by two grants from\nNational Science Foundation: IIS-1910424 and\nIIS-1838770, a DARPA Contract HR001120C0023,\nand a research gift from Northrop Grumman.\n\nReferences\n\nDavide Abati, Jakub Tomczak, Tijmen Blankevoort, Si-\nmone Calderara, Rita Cucchiara, and Babak Ehte-\nshami Bejnordi. 2020. Conditional channel gated\nnetworks for task-aware continual learning. In\nCVPR-2020, pages 3931\u20133940.\n\nHongjoon Ahn, Sungmin Cha, Donggyu Lee, and Tae-\nsup Moon. 2019. Uncertainty-based continual learn-\ning with adaptive regularization. In NIPS-2019,\npages 4394\u20134404.\n\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Da-\nvide Abati, and Simone Calderara. 2020. Dark expe-\nrience for general continual learning: a strong, sim-\nple baseline. arXiv preprint arXiv:2004.07211.\n\nArslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus\nRohrbach, and Mohamed Elhoseiny. 2019. Efficient\nlifelong learning with A-GEM. In ICLR.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597\u20131607. PMLR.\n\nZhiyuan Chen and Bing Liu. 2014. Topic modeling\nusing topics from many domains, lifelong learning\nand big data. In ICML.\n\nZhiyuan Chen and Bing Liu. 2018. Lifelong machine\nlearning. Synthesis Lectures on Artificial Intelli-\ngence and Machine Learning, 12(3):1\u2013207.\n\nZhiyuan Chen, Nianzu Ma, and Bing Liu. 2015. Life-\nlong learning for sentiment classification. In ACL,\npages 750\u2013756.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171\u20134186. Asso-\nciation for Computational Linguistics.\n\nXiaowen Ding, Bing Liu, and Philip S Yu. 2008. A\nholistic lexicon-based approach to opinion mining.\nIn Proceedings of the 2008 international conference\non web search and data mining, pages 231\u2013240.\n\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In LREC.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for unsu-\npervised visual representation learning. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9729\u20139738.\n\nXu He and Herbert Jaeger. 2018. Overcoming catas-\ntrophic interference using conceptor-aided back-\npropagation. In ICLR.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efficient transfer learning for NLP.\nIn ICML, volume 97 of Proceedings of Machine\nLearning Research, pages 2790\u20132799. PMLR.\n\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of ACM\nSIGKDD, pages 168\u2013177.\n\nNitin Kamra, Umang Gupta, and Yan Liu. 2017. Deep\ngenerative dual memory network for continual learn-\ning. CoRR, abs\/1710.10368.\n\nZixuan Ke, Bing Liu, and Xingchang Huang. 2020a.\nContinual learning of a mixed sequence of similar\nand dissimilar tasks. In NeurIPS.\n\nhttp:\/\/papers.nips.cc\/paper\/8690-uncertainty-based-continual-learning-with-adaptive-regularization\nhttp:\/\/papers.nips.cc\/paper\/8690-uncertainty-based-continual-learning-with-adaptive-regularization\nhttps:\/\/openreview.net\/forum?id=Hkf2_sC5FX\nhttps:\/\/openreview.net\/forum?id=Hkf2_sC5FX\nhttps:\/\/www.aclweb.org\/anthology\/P15-2123\/\nhttps:\/\/www.aclweb.org\/anthology\/P15-2123\/\nhttps:\/\/doi.org\/10.18653\/v1\/n19-1423\nhttps:\/\/doi.org\/10.18653\/v1\/n19-1423\nhttps:\/\/doi.org\/10.18653\/v1\/n19-1423\nhttps:\/\/openreview.net\/forum?id=B1al7jg0b\nhttps:\/\/openreview.net\/forum?id=B1al7jg0b\nhttps:\/\/openreview.net\/forum?id=B1al7jg0b\nhttp:\/\/proceedings.mlr.press\/v97\/houlsby19a.html\nhttp:\/\/arxiv.org\/abs\/1710.10368\nhttp:\/\/arxiv.org\/abs\/1710.10368\nhttp:\/\/arxiv.org\/abs\/1710.10368\n\n\nZixuan Ke, Bing Liu, Hao Wang, and Lei Shu. 2020b.\nContinual learning with knowledge transfer for sen-\ntiment classification. In ECML-PKDD.\n\nZixuan Ke, Hu Xu, and Bing Liu. 2021. Adapting bert\nfor continual learning of a sequence of aspect senti-\nment classification tasks. In NAACL.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang,\nAaron Sarna, Yonglong Tian, Phillip Isola,\nAaron Maschinot, Ce Liu, and Dilip Krishnan.\n2020. Supervised contrastive learning. CoRR,\nabs\/2004.11362.\n\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In EMNLP, pages 1746\u2013\n1751. ACL.\n\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, An-\ndrei A. Rusu, Kieran Milan, John Quan, Tiago Ra-\nmalho, Agnieszka Grabska-Barwinska, Demis Hass-\nabis, Claudia Clopath, Dharshan Kumaran, and Raia\nHadsell. 2016. Overcoming catastrophic forgetting\nin neural networks. CoRR, abs\/1612.00796.\n\nMatthias De Lange, Rahaf Aljundi, Marc Masana, and\nTinne Tuytelaars. 2019. Continual learning: A com-\nparative study on how to defy forgetting in classifi-\ncation tasks. CoRR, abs\/1909.08383.\n\nSang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-\nWoo Ha, and Byoung-Tak Zhang. 2017. Overcom-\ning catastrophic forgetting by incremental moment\nmatching. In NIPS, pages 4652\u20134662.\n\nBing Liu. 2012. Sentiment analysis and opinion min-\ning. Synthesis lectures on human language technolo-\ngies, 5(1):1\u2013167.\n\nQian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang.\n2015. Automated rule selection for aspect extraction\nin opinion mining. In IJCAI.\n\nDavid Lopez-Paz and Marc\u2019Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. In\nNIPS.\n\nGuangyi Lv, Shuai Wang, Bing Liu, Enhong Chen, and\nKun Zhang. 2019. Sentiment classification by lever-\naging the shared knowledge from a sequence of do-\nmains. In DASFAA, pages 795\u2013811.\n\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109\u2013165. El-\nsevier.\n\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to\nBERT embeddings during fine-tuning? CoRR,\nabs\/2004.14448.\n\nGerman Ignacio Parisi, Ronald Kemker, Jose L. Part,\nChristopher Kanan, and Stefan Wermter. 2019. Con-\ntinual lifelong learning with neural networks: A re-\nview. Neural Networks, 113:54\u201371.\n\nQi Qin, Wenpeng Hu, and Bing Liu. 2020. Using the\npast knowledge to improve sentiment classification.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings,\npages 1124\u20131133.\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov,\nGeorg Sperl, and Christoph H. Lampert. 2017. icarl:\nIncremental classifier and representation learning.\nIn CVPR, pages 5533\u20135542.\n\nMohammad Rostami, Soheil Kolouri, and Praveen K.\nPilly. 2019. Complementary learning for overcom-\ning catastrophic forgetting using experience replay.\nIn IJCAI, pages 3339\u20133345.\n\nPaul Ruvolo and Eric Eaton. 2013. ELLA: an efficient\nlifelong learning algorithm. In ICML, pages 507\u2013\n515.\n\nJoan Serr\u00e0, Didac Suris, Marius Miron, and Alexandros\nKaratzoglou. 2018. Overcoming catastrophic forget-\nting with hard attention to the task. In ICML, pages\n4555\u20134564.\n\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon\nKim. 2017. Continual learning with deep generative\nreplay. In NIPS, pages 2990\u20132999.\n\nLei Shu, Hu Xu, and Bing Liu. 2017. Lifelong learning\nCRF for supervised aspect extraction. In ACL, pages\n148\u2013154.\n\nDaniel L. Silver, Qiang Yang, and Lianghao Li. 2013.\nLifelong machine learning systems: Beyond learn-\ning algorithms. In Lifelong Machine Learning, Pa-\npers from the 2013 AAAI Spring Symposium, Palo\nAlto, California, USA, March 25-27, 2013.\n\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Uti-\nlizing BERT for aspect-based sentiment analysis via\nconstructing auxiliary sentence. In NAACL, pages\n380\u2013385, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee.\n2020. LAMOL: language modeling for lifelong lan-\nguage learning. In ICLR. OpenReview.net.\n\nDuyu Tang, Bing Qin, and Ting Liu. 2016. Aspect\nlevel sentiment classification with deep memory net-\nwork. In EMNLP, pages 214\u2013224, Austin, Texas.\nAssociation for Computational Linguistics.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola.\n2020a. Contrastive representation distillation. In\nICLR. OpenReview.net.\n\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,\nCordelia Schmid, and Phillip Isola. 2020b. What\nmakes for good views for contrastive learning? In\nNeurIPS 2020.\n\nhttp:\/\/arxiv.org\/abs\/2004.11362\nhttps:\/\/doi.org\/10.3115\/v1\/d14-1181\nhttps:\/\/doi.org\/10.3115\/v1\/d14-1181\nhttp:\/\/arxiv.org\/abs\/1612.00796\nhttp:\/\/arxiv.org\/abs\/1612.00796\nhttp:\/\/arxiv.org\/abs\/1909.08383\nhttp:\/\/arxiv.org\/abs\/1909.08383\nhttp:\/\/arxiv.org\/abs\/1909.08383\nhttp:\/\/papers.nips.cc\/paper\/7051-overcoming-catastrophic-forgetting-by-incremental-moment-matching\nhttp:\/\/papers.nips.cc\/paper\/7051-overcoming-catastrophic-forgetting-by-incremental-moment-matching\nhttp:\/\/papers.nips.cc\/paper\/7051-overcoming-catastrophic-forgetting-by-incremental-moment-matching\nhttps:\/\/doi.org\/10.1007\/978-3-030-18576-3_47\nhttps:\/\/doi.org\/10.1007\/978-3-030-18576-3_47\nhttps:\/\/doi.org\/10.1007\/978-3-030-18576-3_47\nhttp:\/\/arxiv.org\/abs\/2004.14448\nhttp:\/\/arxiv.org\/abs\/2004.14448\nhttps:\/\/doi.org\/10.1016\/j.neunet.2019.01.012\nhttps:\/\/doi.org\/10.1016\/j.neunet.2019.01.012\nhttps:\/\/doi.org\/10.1016\/j.neunet.2019.01.012\nhttps:\/\/doi.org\/10.1109\/CVPR.2017.587\nhttps:\/\/doi.org\/10.1109\/CVPR.2017.587\nhttps:\/\/doi.org\/10.24963\/ijcai.2019\/463\nhttps:\/\/doi.org\/10.24963\/ijcai.2019\/463\nhttp:\/\/proceedings.mlr.press\/v28\/ruvolo13.html\nhttp:\/\/proceedings.mlr.press\/v28\/ruvolo13.html\nhttp:\/\/proceedings.mlr.press\/v80\/serra18a.html\nhttp:\/\/proceedings.mlr.press\/v80\/serra18a.html\nhttp:\/\/papers.nips.cc\/paper\/6892-continual-learning-with-deep-generative-replay\nhttp:\/\/papers.nips.cc\/paper\/6892-continual-learning-with-deep-generative-replay\nhttps:\/\/doi.org\/10.18653\/v1\/P17-2023\nhttps:\/\/doi.org\/10.18653\/v1\/P17-2023\nhttp:\/\/www.aaai.org\/ocs\/index.php\/SSS\/SSS13\/paper\/view\/5802\nhttp:\/\/www.aaai.org\/ocs\/index.php\/SSS\/SSS13\/paper\/view\/5802\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1035\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1035\nhttps:\/\/doi.org\/10.18653\/v1\/N19-1035\nhttps:\/\/openreview.net\/forum?id=Skgxcn4YDS\nhttps:\/\/openreview.net\/forum?id=Skgxcn4YDS\nhttps:\/\/doi.org\/10.18653\/v1\/D16-1021\nhttps:\/\/doi.org\/10.18653\/v1\/D16-1021\nhttps:\/\/doi.org\/10.18653\/v1\/D16-1021\nhttps:\/\/openreview.net\/forum?id=SkgpBJrtvS\nhttps:\/\/proceedings.neurips.cc\/paper\/2020\/hash\/4c2e5eaae9152079b9e95845750bb9ab-Abstract.html\nhttps:\/\/proceedings.neurips.cc\/paper\/2020\/hash\/4c2e5eaae9152079b9e95845750bb9ab-Abstract.html\n\n\nGido M. van de Ven and Andreas S. Tolias.\n2019. Three scenarios for continual learning.\nhttps:\/\/arxiv.org\/abs\/1904.07734.\n\nA\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. CoRR, abs\/1807.03748.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS, pages 5998\u20136008.\n\nJohannes von Oswald, Christian Henning, Jo\u00e3o Sacra-\nmento, and Benjamin F Grewe. 2020. Continual\nlearning with hypernetworks. In ICLR.\n\nHao Wang, Bing Liu, Shuai Wang, Nianzu Ma, and\nYan Yang. 2019. Forward and backward knowl-\nedge transfer for sentiment classification. In ACML,\npages 457\u2013472.\n\nShuai Wang, Guangyi Lv, Sahisnu Mazumder, Geli Fei,\nand Bing Liu. 2018. Lifelong learning memory net-\nworks for aspect sentiment classification. In IEEE\nInternational Conference on Big Data, pages 861\u2013\n870.\n\nRui Xia, Jie Jiang, and Huihui He. 2017. Distantly su-\npervised lifelong learning for large-scale social me-\ndia sentiment analysis. IEEE Trans. Affective Com-\nputing, 8(4):480\u2013491.\n\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019.\nBERT post-training for review reading compre-\nhension and aspect-based sentiment analysis. In\nNAACL-HLT, pages 2324\u20132335. Association for\nComputational Linguistics.\n\nHu Xu, Sihong Xie, Lei Shu, and Philip S. Yu. 2018.\nDual attention network for product compatibility\nand function satisfiability analysis. In AAAI.\n\nWei Xue and Tao Li. 2018. Aspect based sentiment\nanalysis with gated convolutional networks. In ACL,\npages 2514\u20132523. Association for Computational\nLinguistics.\n\nGuanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu.\n2019. Continuous learning of context-dependent\nprocessing in neural networks. Nature Machine In-\ntelligence.\n\nHan Zhang, Ian J. Goodfellow, Dimitris N. Metaxas,\nand Augustus Odena. 2018. Self-attention genera-\ntive adversarial networks. CoRR, abs\/1805.08318.\n\nA Appendix\n\nA.1 Detailed Datasets Statistics\nTable 1 in the main paper has already showed the\nnumber of examples in each dataset. Here we pro-\nvide additional details about aspects and sentiments.\nThe detailed statistics of the 19 datasets or tasks\nare given in Table 4 here.\n\nA.2 Standard Deviations\nTable 5 reports the standard deviation of CLASSIC\nand the considered baselines over 5 runs with ran-\ndom seeds using one random task sequence. We\ncan see the results are stable.\n\nA.3 CLASSIC in TIL Scenario\nTable 6 shows that our proposed method CLASSIC\ncan be adapted for the Task Incremental Learning\n(TIL) setting of continual learning, which requires\nthe task ids during testing, but our DIL setting\ndoes not require. We can observe that CLASSIC\nin the TIL setting also outperforms existing TIL\nbaselines.\n\nA.4 Execution Time and Number of\nParameters\n\nTable 7 reports the number of parameters (regard-\nless of trainable or non-trainable) and the training\nexecution times of different models. The execution\ntime is computed as the average training time per\ntask. Our experiments were run on GeForce GTX\n2080 Ti with 11G GPU memory.\n\nA.5 Hyper-parameters and Validation\nResults\n\nSec. 4.3 in the main paper reported the best hyper-\nparameters. Regarding hyper-parameter search, we\nperformed grid search on the temperature param-\neter \u03c4 within {0.03, 0.5, 0.8, 1}, batch size within\n{32, 64, 128}, and smax within {140, 200, 300,\n400}. We also experimented with whether to apply\nthe l2 normalization before contrast and whether to\nuse the logits or the second last layer to do the con-\ntrast. We did not save the validation results but the\nreported test results in the paper are given by the\nparameters with the best validation performance.\nWe encourage the reviewers and interested readers\nto play with the submitted code.\n\nhttp:\/\/arxiv.org\/abs\/1807.03748\nhttp:\/\/arxiv.org\/abs\/1807.03748\nhttp:\/\/proceedings.mlr.press\/v101\/wang19f.html\nhttp:\/\/proceedings.mlr.press\/v101\/wang19f.html\nhttps:\/\/doi.org\/10.1109\/BigData.2018.8622304\nhttps:\/\/doi.org\/10.1109\/BigData.2018.8622304\nhttps:\/\/doi.org\/10.1109\/TAFFC.2017.2771234\nhttps:\/\/doi.org\/10.1109\/TAFFC.2017.2771234\nhttps:\/\/doi.org\/10.1109\/TAFFC.2017.2771234\nhttps:\/\/doi.org\/10.18653\/v1\/n19-1242\nhttps:\/\/doi.org\/10.18653\/v1\/n19-1242\nhttps:\/\/doi.org\/10.18653\/v1\/P18-1234\nhttps:\/\/doi.org\/10.18653\/v1\/P18-1234\nhttp:\/\/arxiv.org\/abs\/1805.08318\nhttp:\/\/arxiv.org\/abs\/1805.08318\n\n\nDataset Domains Training Validating Testing\n\nLiu3domain\nSpeaker 233 S.\/352 A.\/287 P.\/65 N.\/0 Ne. 30 S.\/44 A.\/35 P.\/9 N.\/0 Ne. 38 S.\/44 A.\/40 P.\/4 N.\/0 Ne.\nRouter 200 S.\/245 A.\/142 P.\/103 N.\/0 Ne. 24 S.\/31 A.\/19 P.\/12 N.\/0 Ne. 22 S.\/31 A.\/24 P.\/7 N.\/0 Ne.\n\nComputer 187 S.\/283 A.\/218 P.\/65 N.\/0 Ne. 25 S.\/35 A.\/23 P.\/12 N.\/0 Ne. 29 S.\/36 A.\/29 P.\/7 N.\/0 Ne.\n\nHL5domain\n\nNokia6610 209 S.\/271 A.\/198 P.\/73 N.\/0 Ne. 29 S.\/34 A.\/30 P.\/4 N.\/0 Ne. 28 S.\/34 A.\/25 P.\/9 N.\/0 Ne.\nNikon4300 131 S.\/162 A.\/135 P.\/27 N.\/0 Ne. 15 S.\/20 A.\/18 P.\/2 N.\/0 Ne. 15 S.\/21 A.\/19 P.\/2 N.\/0 Ne.\n\nCreative 582 S.\/677 A.\/422 P.\/255 N.\/0 Ne. 68 S.\/85 A.\/42 P.\/43 N.\/0 Ne. 70 S.\/85 A.\/52 P.\/33 N.\/0 Ne.\nCanonG3 190 S.\/228 A.\/180 P.\/48 N.\/0 Ne. 25 S.\/29 A.\/21 P.\/8 N.\/0 Ne. 24 S.\/29 A.\/24 P.\/5 N.\/0 Ne.\nApexAD 281 S.\/343 A.\/146 P.\/197 N.\/0 Ne. 35 S.\/43 A.\/16 P.\/27 N.\/0 Ne. 28 S.\/43 A.\/31 P.\/12 N.\/0 Ne.\n\nDing9domain\n\nCanonD500 103 S.\/118 A.\/96 P.\/22 N.\/0 Ne. 11 S.\/15 A.\/14 P.\/1 N.\/0 Ne. 13 S.\/15 A.\/11 P.\/4 N.\/0 Ne.\nCanon100 137 S.\/175 A.\/123 P.\/52 N.\/0 Ne. 19 S.\/22 A.\/20 P.\/2 N.\/0 Ne. 16 S.\/22 A.\/21 P.\/1 N.\/0 Ne.\n\nDiaper 166 S.\/191 A.\/143 P.\/48 N.\/0 Ne. 22 S.\/24 A.\/18 P.\/6 N.\/0 Ne. 24 S.\/24 A.\/22 P.\/2 N.\/0 Ne.\nHitachi 152 S.\/212 A.\/153 P.\/59 N.\/0 Ne. 23 S.\/26 A.\/19 P.\/7 N.\/0 Ne. 23 S.\/27 A.\/14 P.\/13 N.\/0 Ne.\n\nIpod 124 S.\/153 A.\/101 P.\/52 N.\/0 Ne. 18 S.\/19 A.\/14 P.\/5 N.\/0 Ne. 19 S.\/20 A.\/15 P.\/5 N.\/0 Ne.\nLinksys 152 S.\/176 A.\/128 P.\/48 N.\/0 Ne. 19 S.\/22 A.\/13 P.\/9 N.\/0 Ne. 20 S.\/23 A.\/16 P.\/7 N.\/0 Ne.\n\nMicroMP3 384 S.\/484 A.\/340 P.\/144 N.\/0 Ne. 42 S.\/61 A.\/48 P.\/13 N.\/0 Ne. 51 S.\/61 A.\/39 P.\/22 N.\/0 Ne.\nNokia6600 298 S.\/362 A.\/244 P.\/118 N.\/0 Ne. 26 S.\/45 A.\/32 P.\/13 N.\/0 Ne. 39 S.\/46 A.\/30 P.\/16 N.\/0 Ne.\n\nNorton 168 S.\/194 A.\/54 P.\/140 N.\/0 Ne. 17 S.\/24 A.\/15 P.\/9 N.\/0 Ne. 24 S.\/25 A.\/5 P.\/20 N.\/0 Ne.\n\nSemEval14\nRest 1893 S.\/3452 A.\/2094 P.\/779 N.\/579 Ne. 84 S.\/150 A.\/70 P.\/26 N.\/54 Ne. 600 S.\/1120 A.\/728 P.\/196 N.\/196 Ne.\n\nLaptop 1360 S.\/2163 A.\/930 P.\/800 N.\/433 Ne. 98 S.\/150 A.\/57 P.\/66 N.\/27 Ne. 411 S.\/638 A.\/341 P.\/128 N.\/169 Ne.\n\nTable 4: Statistics of the datasets. S.: number of sentences; A: number of aspects; P., N., and Ne.: number of\npositive, negative and neutral aspect polarities respectively. Note that SemEval14 has 3 classes of polarities while\nthe others have only 2 classes (positive and negative) because in these datasets, those sentences without sentiment\n(neutral) are not annotated with aspects. Thus, we cannot use them for aspect sentiment classification (ASC).\n\n\n\nScenario Category Model\nDIL\n\nAcc. MF1\n\nNon-continual\nLearning\n\nBERT\nONE \u00b10.0145 \u00b10.0300\n\nONE+csc \u00b10.0127 \u00b10.0336\n\nBERT (Frozen)\nONE \u00b10.0100 \u00b10.0024\n\nONE+csc \u00b10.0140 \u00b10.0149\n\nAdapter-BERT\nONE \u00b10.0170 \u00b10.0379\n\nONE+csc \u00b10.0094 \u00b10.0327\n\nW2V\nONE \u00b10.0129 \u00b10.0206\n\nONE+csc \u00b10.0092 \u00b10.0079\n\nContinual\nLearning\n\nBERT\nNCL \u00b10.0137 \u00b10.0228\n\nNCL+csc \u00b10.0266 \u00b10.0328\n\nBERT (Frozen)\nNCL \u00b10.0065 \u00b10.0110\n\nNCL+csc \u00b10.0085 \u00b10.0116\n\nAdapter-BERT\nNCL \u00b10.0102 \u00b10.0128\n\nNCL+csc \u00b10.0102 \u00b10.0130\n\nW2V\nNCL \u00b10.0192 \u00b10.0230\n\nNCL+csc \u00b10.0121 \u00b10.0108\n\nBERT\n(Frozen)\n\nKAN \u2014\nKAN+last \u00b10.0032 \u00b10.0045\nKAN+ent \u00b10.0056 \u00b10.0065\n\nSRK \u00b10.0069 \u00b10.0099\nEWC \u00b10.0094 \u00b10.0093\nUCL \u00b10.0084 \u00b10.0100\n\nOWM \u00b10.0092 \u00b10.0140\nDER++ \u00b10.0096 \u00b10.0120\n\nHAT \u2014\nHAT+last \u00b10.0095 \u00b10.0098\nHAT+ent \u00b10.0029 \u00b10.0097\n\nAdapter-BERT\n\nEWC \u00b10.0110 \u00b10.0110\nUCL \u00b10.0000 \u00b10.0000\n\nOWM \u00b10.0052 \u00b10.0109\nDER++ \u00b10.0137 \u00b10.0179\n\nHAT \u2014\nHAT+last \u00b10.0038 \u00b10.0055\nHAT+ent \u00b10.0059 \u00b10.0106\n\nW2V\n\nKAN \u2014\nKAN+last \u00b10.0000 \u00b10.0000\nKAN+ent \u00b10.0000 \u00b10.0000\n\nSRK \u00b10.0000 \u00b10.0000\nEWC \u00b10.0059 \u00b10.0076\nUCL \u00b10.0097 \u00b10.0128\n\nOWM \u00b10.0077 \u00b10.0081\nDER++ \u00b10.0075 \u00b10.0041\n\nHAT \u2014\nHAT+last \u00b10.0053 \u00b10.0082\nHAT+ent \u00b10.0103 \u00b10.0199\n\nLAMOL \u00b10.0027 \u00b10.0062\nCLASSIC \u00b10.0048 \u00b10.0101\n\nTable 5: Standard deviations. HAT and KAN have no\nresults because they require task ids in testing, but in\nthe DIL setting, no task ids are provided in testing.\n\nScenario Category Model\nTIL DIL\n\nAcc. MF1 Acc. MF1\n\nNon-continual\nLearning\n\nBERT\nONE 0.8584 0.7635 0.8584 0.7635\n\nONE+csc 0.8353 0.7388 0.8353 0.7388\n\nBERT (Frozen)\nONE 0.7814 0.5813 0.7814 0.5813\n\nONE+csc 0.8265 0.7232 0.8265 0.7232\n\nAdapter-BERT\nONE 0.8596 0.7807 0.8596 0.7807\n\nONE+csc 0.8530 0.7516 0.8530 0.7516\n\nW2V\nONE 0.7701 0.5189 0.7701 0.5189\n\nONE+csc 0.7761 0.5487 0.7761 0.5487\n\nContinual\nLearning\n\nBERT\nNCL 0.4960 0.4308 0.8048 0.7085\n\nNCL+csc 0.5939 0.3416 0.7727 0.5807\n\nBERT (Frozen)\nNCL 0.8551 0.7664 0.8685 0.7873\n\nNCL+csc 0.8783 0.8271 0.8693 0.7912\n\nAdapter-BERT\nNCL 0.5403 0.4481 0.8667 0.7804\n\nNCL+csc 0.8630 0.8090 0.8809 0.7847\n\nW2V\nNCL 0.8269 0.7356 0.7736 0.6765\n\nNCL+csc 0.8421 0.7418 0.8396 0.7509\n\nBERT\n(Frozen)\n\nKAN 0.8549 0.7738 \u2014\nKAN+last \u2014 0.8320 0.7352\nKAN+ent \u2014 0.8278 0.7243\n\nSRK 0.8476 0.7852 0.8391 0.7438\nEWC 0.8637 0.7452 0.8660 0.7831\nUCL 0.8389 0.7482 0.8538 0.7690\n\nOWM 0.8702 0.7931 0.8611 0.7665\nDER++ 0.8427 0.7508 0.8753 0.8009\n\nHAT 0.8674 0.7816 \u2014\nHAT+last \u2014 0.8473 0.7649\nHAT+ent \u2014 0.8418 0.7614\n\nAdapter-BERT\n\nEWC 0.5630 0.4958 0.8805 0.7875\nUCL 0.6446 0.3664 0.7123 0.3961\n\nOWM 0.7299 0.6651 0.8766 0.7882\nDER++ 0.4763 0.3554 0.8859 0.7985\n\nHAT 0.8614 0.7852 \u2014\nHAT+last \u2014 0.8823 0.7919\nHAT+ent \u2014 0.8854 0.8245\n\nW2V\n\nKAN 0.7206 0.4001 \u2014\nKAN+last \u2014 0.7123 0.3961\nKAN+ent \u2014 0.7123 0.3961\n\nSRK 0.7101 0.3963 0.7123 0.3961\nEWC 0.8416 0.7229 0.7586 0.6545\nUCL 0.8441 0.7599 0.8187 0.6965\n\nOWM 0.8270 0.7118 0.8256 0.7253\nDER++ 0.8327 0.6993 0.8459 0.7722\n\nHAT 0.8083 0.6363 \u2014\nHAT+last \u2014 0.7599 0.5849\nHAT+ent \u2014 0.7605 0.5349\n\nLAMOL 0.8891 0.8059 0.8891 0.8059\nCLASSIC (forward) 0.8897 0.8338 0.8886 0.8365\n\nCLASSIC 0.8942 0.8393 0.9022 0.8512\n\nTable 6: Accuracy (Acc.) and Macro-F1 (MF1) aver-\naged over 5 random sequences of 19 tasks.\n\nScenarios Category Model #parameters (M) Running time (s)\n\nNon-continual\nLearning\n\nBERT ONE 109.5 600.0\nBERT (Frozen) ONE 110.4 500.0\nAdapter-BERT ONE 183.3 684.0\n\nW2V ONE 6.7 189.0\n\nContinual\nLearning\n\nBERT NCL 109.5 600.0\nBERT (Frozen) NCL 110.4 500.0\nAdapter-BERT NCL 183.3 684.0\n\nW2V NCL 6.7 189.0\n\nBERT (frozen)\n\nKAN 116.6 550.0\nSRK 117.8 600.0\nEWC 110.4 580.0\nUCL 110.4 638.0\n\nOWM 110.6 635.0\nDER++ 115.0 650.0\n\nHAT 111.3 610.0\n\nAdapter-BERT\n\nEWC 183.3 840.0\nUCL 183.4 870.0\n\nOWM 184.4 780.0\nDER++ 184.0 880.0\n\nHAT 185.2 840.0\n\nW2V\n\nKAN 7.0 150.0\nSRK 7.2 160.0\nEWC 6.2 162.0\nUCL 6.2 135.0\n\nOWM 6.4 125.0\nDER++ 6.8 135.0\n\nHAT 6.4 180.0\nLAMOL 124.4 650.0\n\nCLASSIC 185.2 900.0\n\nTable 7: Network size (number of parameters, regard-\nless of trainable or non-trainable) and average training\nexecution time per task of each model in seconds.\n\n\n","11":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving Forgetting Prevention and Knowledge\nTransfer in Continual Learning\n\nZixuan Ke1, Bing Liu1, Nianzu Ma1, Hu Xu2 and Lei Shu3\u2217\n1Department of Computer Science, University of Illinois at Chicago\n\n2Facebook AI Research\n3Amazon AWS AI\n\n1{zke4,liub,nma4}@uic.edu\n2huxu@fb.com\n\n3shulindt@gmail.com\n\nAbstract\n\nContinual learning (CL) learns a sequence of tasks incrementally with the goal\nof achieving two main objectives: overcoming catastrophic forgetting (CF) and\nencouraging knowledge transfer (KT) across tasks. However, most existing tech-\nniques focus only on overcoming CF and have no mechanism to encourage KT,\nand thus do not do well in KT. Although several papers have tried to deal with\nboth CF and KT, our experiments show that they suffer from serious CF when\nthe tasks do not have much shared knowledge. Another observation is that most\ncurrent CL methods do not use pre-trained models, but it has been shown that such\nmodels can significantly improve the end task performance. For example, in natural\nlanguage processing, fine-tuning a BERT-like pre-trained language model is one of\nthe most effective approaches. However, for CL, this approach suffers from serious\nCF. An interesting question is how to make the best use of pre-trained models for\nCL. This paper proposes a novel model called CTR to solve these problems. Our\nexperimental results demonstrate the effectiveness of CTR.2\n\n1 Introduction\n\nThis paper studies continual learning (CL) of a sequence of natural language processing (NLP) tasks\nin the task continual learning (Task-CL) setting. It aims to (i) prevent catastrophic forgetting (CF),\nand (ii) transfer knowledge across tasks. (ii) is particularly important because many tasks in NLP\nshare similar knowledge that can be leveraged to achieve better accuracy. CF means that in learning a\nnew task, the existing network parameters learned for the previous tasks may be modified, which\ndegrades the performance of previous tasks [40]. In the Task-CL setting, the task id is provided for\neach test case in testing so that the specific model for the task in the network can be applied to classify\nthe test case. Another popular CL setting is class continual learning, which does not provide the task\nid during testing but it is for solving a different type of problems.\n\nMost existing CL papers focus on dealing with CF [21, 5]. There are also some papers that perform\nknowledge transfer. To achieve both objectives is highly challenging. To overcome CF in the Task-CL\nsetting, we don\u2019t want the training of the new task to update the model parameters learned for previous\ntasks to achieve model separation. But to transfer knowledge across tasks, we want the new task to\nleverage the knowledge learned from previous tasks for learning a better model (forward transfer)\nand also want the new task to enhance the performance of similar previous tasks (backward transfer).\n\n\u2217Work was done prior to joining Amazon.\n2The code of CTR can be found at https:\/\/github.com\/ZixuanKe\/PyContinual\n\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\n\nar\nX\n\niv\n:2\n\n11\n2.\n\n02\n70\n\n6v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 5\n\n D\nec\n\n 2\n02\n\n1\n\nhttps:\/\/github.com\/ZixuanKe\/PyContinual\n\n\nThis means it is necessary to update previous model parameters. This is a dilemma. Although several\npapers have tried to deal with both [22, 37], they were only tested using sentiment analysis tasks with\nstrong shared knowledge. When tested with tasks that don\u2019t have much shared knowledge, they suffer\nfrom severe CF (see Sec. 5.4). Those existing papers that focus on dealing with CF do not do well\nwith knowledge transfer as they have no explicit mechanism to facilitate the transfer.\n\nAnother observation about the current CL research is that most techniques do not use pre-trained\nmodels. But such pre-trained models or feature extractors can significantly improve the CL perfor-\nmance [18, 24]. An important question is how to make the best use of pre-trained models in CL. This\npaper studies the problem as well using NLP tasks, but we believe that the developed ideas are also\napplicable to computer vision tasks because most pre-trained models are based on the transformer\narchitecture [60]. We will see that the naive or the conventional way of directly adding the CL module\non top of a pre-trained model is not the best choice (see Sec. 5.4).\n\nIn NLP, fine-tuning a BERT [8] like pre-trained language model has been regarded as one of the\nmost effective techniques in applications [65, 57]. However, fine-tuning works poorly for continual\nlearning. This is because the fine-tuned BERT for a task captures highly task-specific information [41],\nwhich is difficult to be used by other tasks. When fine-tuning for a new task, it has to update the\nalready fine-tuned parameters for previous tasks, which causes serious CF (see Sec. 5.4).\n\nThis paper proposes a novel neural architecture to achieve both CF prevention and knowledge transfer,\nwhich also deals with the CF problem with BERT fine-tuning. The proposed system is called CTR\n(Capsules and Transfer Routing for continual learning). CTR inserts a continual learning plug-in\n(CL-plugin) module in two locations in BERT. With the pair of CL-plugin modules added to BERT,\nwe no longer need to fine-tune BERT for each task, which causes CF in BERT, and yet we can\nachieve the power of BERT fine-tuning. CTR has some similarity to Adapter-BERT [16], which\nadds adapters in BERT for parameter efficient transfer learning such that different end tasks can have\ntheir separate adapters (which are very small in size) to adapt BERT for individual end tasks and to\ntransfer the knowledge from BERT to the end tasks. Then, there is no need to employ a separate\nBERT and fine-tuning it for each task, which is extremely parameter inefficient if many tasks need to\nbe learned. An adapter is a simple 2-layer fully-connected network for adapting BERT to a specific\nend task. A CL-plugin is very different from an adapter. We do not use a pair of CL-plugin modules\nto adapt BERT for each task. Instead, CTR learns all tasks using only one pair of CL-plugin modules\ninserted into BERT. A CL-plugin is a full CL network that can leverage a pre-trained model and\ndeal with both CF and knowledge transfer. Specifically, it uses a capsule [15] to represent each task\nand a proposed transfer routing algorithm to identify and transfer knowledge across tasks to achieve\nimproved accuracy. It further learns and uses task masks to protect task-specific knowledge to avoid\nforgetting. Empirical evaluations show that CTR outperforms strong baselines. Ablation experiments\nhave also been conducted to study where to insert the CL-plugin module in BERT in order to achieve\nthe best performance (see Sec. 5.4).\n\n2 Related Work\n\nCatastrophic Forgetting: Existing work in CL mainly focused on overcoming CF using the fol-\nlowing approaches. (1) Regularization-based approaches, such as those in [27, 30, 51, 69], add a\nregularization in the loss to consolidate weights for previous tasks when learning a new task. (2)\nReplay-based approaches, such as those in [45, 36, 4, 63], retain some training data of old tasks and\nuse them in learning a new task. The methods in [54, 20, 47, 14] learn data generators and generate\nold task data for learning a new task. (3) Parameter isolation-based approaches, such as those in\n[52, 21, 39, 10], allocate model parameters dedicated to different tasks and mask them out when\nlearning a new task. (4) Gradient projection-based approaches [68] ensure the gradient updates occur\nonly in the orthogonal direction to the input of old tasks and thus will not affect old tasks. Some\nrecent papers used pre-trained models [18, 23, 24] and learn one class per task [18]. Tackling CF\nonly deals with model deterioration. These methods perform worse than learning each task separately.\nAn empirical study of the cause of CF and the impact of task similarity on CF was done in [44].\n\nSome NLP applications have also dealt with CF. For example, CL models have been proposed for\nsentiment analysis [23, 24, 37, 43], dialogue slot filling [53], language modeling [58, 7], language\nlearning [31], sentence embedding [33], machine translation [25], cross-lingual modeling [35], and\nquestion answering [12]. A dialogue CL dataset is also reported in [38].\n\n2\n\n\n\nKnowledge Transfer: Ideally, learning from a sequence of tasks should also allow multiple tasks to\nsupport each other via knowledge transfer. CAT [21] (a Task-CL system) works on a mixed sequence\nof similar and dissimilar tasks and can transfer knowledge among similar tasks detected automatically.\nProgressive Network [48] does forward transfer but it is for class continual learning (Class-CL).\n\nKnowledge transfer in this paper is closely related to lifelong learning (LL), which aims to improve\nthe new\/last task learning without handling CF [56, 49, 5]. In the NLP area, NELL [3] performs LL\ninformation extraction, and several other papers worked on lifelong document sentiment classification\n(DSC) and aspect sentiment classification (ASC). [6] and [61] proposed two Naive Bayesian methods\nto help improve the new task learning. [64] proposed a LL approach based on voting. [55] used LL\nfor aspect extraction. [43] and [62] used neural networks for DSC and ASC, respectively. Several\npapers also studied lifelong topic modeling [5, 13]. However, all these works do not deal with CF.\n\nSRK [37] and KAN [22] try to deal with both CF and knowledge transfer in continual sentiment\nclassification. However, they have two critical weaknesses: (i) Their RNN architectures cannot use\nplug-in or adapter modules to tune BERT, which significantly limits their power. (ii) Since they were\nmainly designed for knowledge transfer, they suffer from serious CF (see Sec. 5.4). B-CL [24] uses\nthe adapter idea [16] to adapt BERT for sentiment analysis tasks, which are similar to each other.\nHowever, since its mechanism of dynamic routing for knowledge transfer is very week, its knowledge\ntransfer ability is markedly poorer than CTR (see Sec. 5.4). CLASSIC [23] is another recent work\non continual learning for knowledge transfer, but its CL setting is domain continual learning. Its\nknowledge transfer method is based on contrastive learning.\n\nAdapterFusion [42] used adapters proposed in [16]. It proposes a two-stage method to learn a set of\ntasks. In the first stage, it learns one adapter for each task independently using the task\u2019s training\ndata. In the second stage, it uses the training data again to learn a good composition of the learned\nadapters in the first stage to produce the final model for all tasks. AdapterFusion basically tries to\nimprove multi-task learning. It is not for continual learning and thus has no CF. As explained in\nSec. 1, the CL-plugin concept in CTR is different from that of adapters for adapting BERT for each\ntask. CL-plugins are continual learning systems that make use of a pre-trained model.\n\n3 CTR Architecture\n\nThis section describes the general architecture of CTR. The details about its key component CL-\nplugin is presented in the next section. Due to its good performance, BERT [8] and its transformer\n[60] architecture are used as the base in our model CTR. Since BERT fine-tuning is prone to CF\n(Sec. 1), we propose the CL plug-in idea, which is inspired by Adapter-BERT [16]. CL-plugin is a\nfull continual learning module designed to interact with a pre-trained model, in our case, BERT.\n\nInserting CL-plugins in BERT. A commonly used method of leveraging a pre-trained model is to\nadd the end task module on top of the pre-trained model. However, as explained in Sec. 1, fine-tuning\nthe pre-trained model can cause serious CF for CL. The CL system PCL [18], which uses this\napproach, has the pre-trained model frozen to avoid forgetting. But as we will see in Sec. 5.4, this\nis not the best choice. CTR inserts the proposed CL-plugin in two locations in BERT, i.e., in each\ntransformer layer of BERT. We will also see in Sec. 5.4 that inserting only one CL-plugin in one\nlocation is sub-optimal. Figure 1 gives the CTR architecture and we can see the two CL-plugins are\nadded into BERT. In learning, only the two CL-plugins and the classification heads are trained. The\ncomponents of the original pre-trained BERT are fixed.\n\nContinual learning plug-in (CL-plugin). CL-plugin employs a capsule network (CapsNet) [15, 50]\nlike architecture. In the classic neural network, a neuron outputs a scalar, real-valued activation\nas a feature detector. CapsNets replaces that with a vector-output capsule to preserve additional\ninformation. A simple CapsNet consists of two capsule layers. The first layer stores low-level feature\nmaps, and the second layer generates the classification probability with each capsule corresponding\nto one class. CapsNet uses a dynamic routing algorithm to make each lower-level capsule to send its\noutput to a similar (or \u201cagreed\u201d, computed by dot product) higher-level capsule. This property can\nalready be used to group similar tasks and their shareable features to produce a CL system (see the\nablation study in Sec. 5.4). One of the key ideas of CL-plugin (see Figure 2(A)) is a transfer capsule\nlayer with a new transfer routing algorithm to explicitly identify transferable features\/knowledge\nfrom previous tasks to transfer to the new task. Additionally, transfer routing avoids the need for\nhyper-parameter tuning on the number of iterations of dynamic routing [50] to update the agreements.\n\n3\n\n\n\nFigure 1: Architecture of BERT (left) and the proposed system CTR (right), which inserts two\nCL-plugins in BERT. Each CL-plugin module (far right) has two sub-modules and a skip connection:\nknowledge sharing sub-module (KSM) and task-specific sub-module (TSM).\n\n\u2026\n\nInput features\n\nOutput features\n\nt\nInput Task ID\n\nTask ID \nEmbedding \nlookup and \n\nTK-Layer\n\nKSM\n\n\u2026\n\nTR-Layer\n\nTSM\n\n\u2026 \u2026\n\nSE\n\nTR\n\nSE\n\nTR\n\nSE\n\nTR\n\nTransfer \nRouting\n\n\u2026\n\nBefore Training After Training\nTask 0\n\nTask 1\n\nTask 2\n\n(B) Task Masks (TM) in TSM\n\n100 0 0\n\n1 0 0 0 0\n\n0 100 0\n\n0 1 0 0 0\n\n0\n\n0\n\n0 0\n\n0 0\n\n0 001 0\n\n0 1 0 1 0\n\n(A) Overall Architecture of  CBA\n\nCapsules\n\nNeurons\n\n\u2026\n\n\u2026\n\n\u2026\n\nFigure 2: (A) CL-plugin Architecture. (B) Illustration of task masking. Cells\/neurons in brown, green\nand red are respectively used by tasks 0, 1 and 2. Neurons with two colors are used by two tasks\n\n4 Continual Learning Plug-in (CL-plugin)\n\nThe architecture of our continual learning plug-in (CL-plugin) is shown in Figure 2(A). CL-plugin\ntakes two inputs: (1) hidden states h(t) from the feed-forward layer inside a transformer layer and\n(2) task ID t, which is required by task continual learning (Task-CL). The outputs are hidden states\nwith features suitable for the t-th task for classification. Inside CL-plugin, there are two modules: (1)\nknowledge sharing module (KSM) for identifying and transferring the shareable knowledge from\nsimilar previous tasks to the new task t, and (2) task specific module (TSM) for learning task specific\nneurons and their masks (which can protect the neurons from being updated by future tasks to deal\nwith CF). Since TSM is differentiable, the whole system CTR can be trained end-to-end.\n\n4.1 Knowledge Sharing Module (KSM)\n\nKSM achieves knowledge transfer among similar tasks via a task capsule layer (TK-Layer), a transfer\ncapsule layer (TR-Layer), and a transfer routing mechanism.\n\n4\n\n\n\n4.1.1 Task Capsule Layer (TK-Layer)\n\nEach capsule in the TK-Layer represents a task, and it prepares the low-level features derived\nfrom each task (Figure 2(A)). As such, a capsule is added to the TK-Layer for each new task.\nThis incremental growth is efficient and easy because these capsules are discrete and do not share\nparameters. Also, each capsule is simply a 2-layer fully connected network with a small number of\nparameters. Let h(t) \u2208 Rdt\u00d7de be the input of CL-plugin, where dt is the number of tokens, de the\nnumber of dimensions, and t is the current task. In the TK-Layer, we have one capsule for each task.\nAssume we have learned t tasks so far. The capsule for the i-th (i \u2264 t) task is\n\np\n(t)\ni = fi(h\n\n(t)), (1)\n\nwhere fi(\u00b7) = MLPi(\u00b7) denotes a 2-layer fully-connected network.\n\n4.1.2 Transfer Routing and Transfer Capsule Layer\n\nEach capsule in the transfer capsule layer (TR-Layer) represents the transferable representation\nextracted from TK-Layer. As shown in Figure 2(A), transfer routing between the lower-level capsules\nin TK-Layer and high-level capsules in TR-Layer has three components: pre-route vector generator\n(PVG), similarity estimator (SE) and task router (TR). Given the task capsules in the TK-Layer, we\nfirst transform the feature through a trainable weight matrix. We call the output of this transformation\nthe pre-route vector. Each SE estimates the similarity between a previous task and the current task\nusing the pre-route vector, resulting in a similarity score for each higher-level capsule. Additionally,\neach SE is augmented by a TR module, a differentiable task router acting as a gate. This router\nestimates a binary signal that decides whether to connect or disconnect the current route between\nthe two consecutive capsule layers (i.e. TK-Layer and TR-Layer in CL-plugin). The binary signal\nestimated by TR can be seen as a differentiable binary attention. Conceptually, each SE and TR pair\ntogether learns the connectivity between capsules in a stochastic and differentiable manner, which can\nbe seen as a task similarity-based connectivity search mechanism. This transfer routing identifies the\nshared features\/knowledge from multiple task capsules and helps knowledge transfer across similar\ntasks. Next, we discuss the pre-route vector generator, similarity estimator and task router.\n\nPre-route Vector Generator (PVG). We first turns each transfer capsule p(t)i into a pre-route vector,\n\nu\n(t)\nj|i =Wijp\n\n(t)\ni , (2)\n\nwhere Wij \u2208 Rds\u00d7dk is the weight matrix, ds and dk are the dimensions of task capsule i (also\nrepresenting a task) and transfer capsule j, and t is the current task. The number of transfer capsules\nnj is a hyperparameter detailed in Sec. 5.\n\nSimilarity Estimator (SE). Since tasks i and t are different, it is crucial to determine what in task\ni\u2019s representation is transferable. Inspired by [67], we use a convolution layer and activation units to\ncompare task i with task t to determine the transferable proportion from the previous task i. In SE,\nwe compute the task similarity as follows:\n\nq\n(t)\nj|t = MaxPool(Relu(u\n\n(t)\nj|t \u2217Wq + bq)), (3)\n\na\n(t)\nj|i = MaxPool(Relu(u\n\n(t)\nj|i \u2217Wa + fa(q\n\n(t)\nj|t) + ba)), (4)\n\nwhere ba, bq \u2208 R are the bias, Wa,Wq \u2208 Rde\u00d7dw are convolutions filters and dw is the windows size.\nWe extract important features from the current task representation u(t)\n\nj|t via the convolution network in\nEq. (3). The MaxPool helps remove the insignificant features to generate a fixed-size vector with the\nsize equal to the number of filters nw. Similarly, we extract important features from the previous task\ni\u2019s representation u(t)\n\nj|i . Using the important features for the current and previous tasks, we compute a\nsimilarity score between them in Eq. (4) with ReLU activation. Note fa is a 1-layer fully-connected\nnetwork to match the dimensions. As a result, a(t)\n\nj|i indicates how similar the representation of the i-th\n\ntask is to the current task t. For those tasks with a very low a(t)\nj|i , their representations are less similar\n\nto the current task and thus has little transferable knowledge.\n\nTask Router (TR). TR controls which previous task representation should flow to the next layer with\nthe goal of letting only the transferable information to flow. Given the similarity a(t)\n\nj|i , TR estimates\n\n5\n\n\n\na binary decision signal \u03b4(t)\nj|t \u2208 {0:disconnect, 1:connect}. We first apply a convolution\n\nlayer with 2 output channels and 1 \u00d7 1 kernel size to generate un-normalized decision value. To\nestimate the binary decision, we need to generate a decision chosen from the set of two mutually\nexclusive and exhaustive events (disconnect and connect). In our work, we adopt the Gumbel-Softmax\n[19] to help make the TR gate differentiable.\n\n\u03b4\n(t)\nj|i = Gumbel_softmax(a\n\n(t)\nj|i \u2217W\u03b4 + b\u03b4). (5)\n\nGiven the similarity a(t)\nj|t, binary decision \u03b4\n\n(t)\nj|t and the pre-route vector u\n\n(t)\nj|t, we compute the transfer-\n\nable representation vtran(t)\nj|i and final output v\n\n(t)\nj as follows:\n\nv\ntran(t)\nj|i = a\n\n(t)\nj|i \u2297 u\n\n(t)\nj|i , v\n\n(t)\nj =\n\nn+1\u2211\ni=1\n\n\u03b4\n(t)\nij\n\n=1\n\nv\ntran(t)\nj|i . (6)\n\nThis makes sure only task capsules for tasks that are salient or similar to the new task are used, and\nthe others task capsules are ignored (and thus protected) to learn more general shareable knowledge.\nAs many NLP applications have similar tasks, such learning of task-sharing features can be very\nimportant. Note that in backpropagation, only the similar tasks with connected gate (\u03b4(t)ij =1) are\nupdated, encouraging backward knowledge transfer of similar tasks.\n\n4.2 Task Specific Module (TSM)\n\nWe now discuss how to preserve task-specific knowledge of previous tasks to prevent forgetting (CF).\nTo achieve this, we use task masks (Figure 2(B)). Specifically, we first detect the neurons used by\neach old task and then block off or mask out all the used neurons when learning a new task.\n\nThe task-specific module consists of differentiable layers (a 2-layer fully-connected network is used).\nEach layer\u2019s output is further applied with a task mask to indicate which neurons should be protected\nfor that task to overcome CF and forbids gradient updates for those neurons during backpropagation\nfor a new task. Those tasks with overlapping masks indicate some parameter sharing. Due to KSM,\nthe features flowing into those overlapping neurons enable the related old tasks to also improve\nin learning the new task. Here we borrow the hard attention idea in [52] and leverage the task ID\nembedding to train the task mask. Further details can be found in Supplementary.\n\nIllustration. The task masking process is illustrated in Figure 2(B), which shows the learning process\nof three tasks. Before training, those solid cells with a 0 are the neurons that have been used by some\nprevious tasks and should be protected (masked). Those empty cells are free neurons (not used).\nAfter training, those solid cells with a 1 are neurons that are important for the current task, which\nwill be used as masks in the future. Those solid cells with a 0 are masked as they are important for\nprevious tasks. Those non-solid cells with a 0 are neurons that are not used so far.\n\nLet us walk through the learning process of the three tasks. After training task 0, we obtain its useful\nneurons indicated by the 1 entries. Before training task 1, those useful neurons for task 0 are first\nmasked (those previous 1\u2019s entries are turned to 0\u2019s). After training task 1, two neurons with 1 are\nused by the task. When task 2 arrives, all used neurons by tasks 0 and 1 are masked before training,\ni.e., their entries are set to 0. After training task 2, we see that tasks 2 and 1 have a shared neuron (the\ncell with two colors, red and green), which is used by both of tasks.\n\n5 Experiments\n\nWe evaluate CTR using three applications. We follow the continual learning (CL) evaluation method\ngiven in [29]. CTR first learns a sequence of tasks. After a task is trained, the training data of the task\nis no longer accessible. After all tasks are learned, their task models are tested using their respective\ntest sets. In training each task, we use its validation set to decide when to stop training.\n\n5.1 Three Applications and Their Datasets\n\nThe first two applications (and datasets) are used to show the knowledge transferability of CTR\nbecause their tasks are similar and have shared knowledge. Catastrophic forgetting (CF) is not a\n\n6\n\n\n\nData\nsource Liu3domain HL5domain Ding9domain SemEval14\n\nTask\/\ndomain S\n\npe\nak\n\ner\n\nR\nou\n\nte\nr\n\nC\nom\n\npu\nte\n\nr\n\nN\nok\n\nia\n66\n\n10\n\nN\nik\n\non\n43\n\n00\n\nC\nre\n\nat\niv\n\ne\n\nC\nan\n\non\nG\n\n3\n\nA\npe\n\nxA\nD\n\nC\nan\n\non\nD\n\n50\n0\n\nC\nan\n\non\n10\n\n0\n\nD\nia\n\npe\nr\n\nH\nit\n\nac\nhi\n\nIp\nod\n\nL\nin\n\nks\nys\n\nM\nic\n\nro\nM\n\nP\n3\n\nN\nok\n\nia\n66\n\n00\n\nN\nor\n\nto\nn\n\nR\nes\n\nta\nur\n\nan\nt\n\nL\nap\n\nto\np\n\nTrain 352 245 283 271 162 677 228 343 118 175 191 212 153 176 484 362 194 3452 2163\nVal. 44 31 35 34 20 85 29 43 15 22 24 26 19 22 61 45 24 150 150\nTest 44 31 36 34 21 85 29 43 15 22 24 27 20 23 61 46 25 1120 638\n\nTable 1: Statistics of datasets for ASC. The datasets statistics for DSC and 20News have been\ndescribed in the text. More detailed data statistics are given in Supplementary.\n\nmajor concern for them. The third application (and dataset) is mainly used to test CTR\u2019s ability to\novercome CF as its tasks are very different and have little shared knowledge to transfer.\n\n1. Document Sentiment Classification (DSC). This application is to classify each full product\nreview into one of the two opinion classes (positive and negative). The training data of each task\nconsists of reviews of a particular type of product. We adopt the text classification formulation in [8],\nwhere a [CLS] token is used to predict the opinion polarity.\n\nWe employ a set of 10 DSC datasets (reviews of 10 products) to produce sequences of tasks. The\nproducts are Sports, Toys, Tools, Video, Pet, Musical, Movies, Garden, Offices, and Kindle [22]. Two\nexperiments are conducted: (1) using small data in each task: 100 positive and 100 negative training\nreviews per task; (2) using the full data in each task: 2500 positive and 2500 negative training reviews\nper task [22]. (1) is more useful in practice because labeling a large number of examples is very\ncostly. The same validation reviews (250 positive and 250 negative) and the same test reviews (250\npositive and 250 negative) are used in both experiments.\n\n2. Aspect Sentiment Classification (ASC). It classifies a review sentence on the aspect-level\nsentiment (one of positive, negative, and neutral). For example, the sentence \u201cThe picture is great but\nthe sound is lousy\u201d about a TV expresses a positive opinion about the aspect \u201cpicture\u201d and a negative\nopinion about the aspect \u201csound.\u201d We adopt the ASC formulation in [65], where the aspect term and\nsentence are concatenated via [SEP] in BERT. The opinion is predicted on top of the [CLS] token.\n\nWe employ a set of 19 ASC datasets (review sentences of 19 products) to produce sequences of\ntasks. Each dataset represents a task. The datasets are from 4 sources: (1) HL5Domains [17] with\nreviews of 5 products; (2) Liu3Domains [32] with reviews of 3 products; (3) Ding9Domains [9]\nwith reviews of 9 products; and (4) SemEval14 with reviews of 2 products - SemEval 2014 Task 4\nfor laptop and restaurant. For (1), (2) and (3), we use about 10% of the original data as the validate\ndata, another about 10% of the original data as the testing data. For (4), we use 150 examples from\nthe training set for validation. To be consistent with existing research [59], sentences with conflict\nopinions about a single aspect are not used. Statistics of the 19 datasets are given in Table 1.\n\n3. Text classification using 20News data. This dataset [28] has 20 classes and each class has about\n1000 documents. The data split for train\/validation\/test is 1600\/200\/200. We created 10 tasks, 2\nclasses per task. Since this is topic-based text classification data, the classes are very different and\nhave little shared knowledge. As mentioned above, this application (and dataset) is mainly used to\nshow CTR\u2019s ability to overcome forgetting.\n\n5.2 Baselines\n\nWe setup 38 baselines, including both standalone and continual learning methods.\n\nMultitask learning (MTL: Results of multitask learning is considered the upper-bound of those of\ncontinual learning. Here MTL fine-tunes BERT.\n\nStandalone (SDL) Baselines: The SDL setting builds a model for each task independently using\na separate network. It clearly has no knowledge transfer or forgetting. We have 4 baselines under\nSDL, BERT, BERT (Frozen), Adapter-BERT and W2V (word2vec embeddings). For BERT, we\nuse trainable BERT, i.e., fine-tuning. BERT (Frozen) uses frozen BERT with a trainable CNN text\nclassification network [26] on top of it.; Adapter-BERT adapts the BERT as in [16], where only the\nadapter blocks are trainable; W2V uses embeddings trained on the Amazon review data in [66] using\nFastText [11]. For ASC, we adopt the ASC classification network in [67], which takes aspect term\nand review sentence as input. For DSC and 20News, we adopt the classification network in [8].\n\n7\n\n\n\nContinual Learning (CL) Baselines. CL setting includes 4 baselines with no forgetting handling\n(NFH) (corresponding to the 4 standalone baselines), and 25 baselines from 9 state-of-the art task\ncontinual learning (Task-CL) methods that deal with forgetting (CF). NFH baselines learn the tasks\none by one with no awareness of forgetting\/transfer.\n\nThe 12 state-of-the-art CL systems are: KAN [22] and SRK [37] are Task-CL methods for document\nsentiment classification. The rest were designed initially for image classification. Therefore, we\nreplace their original MLP or CNN image classification network with CNN for text classification [26].\nHAT [52] is one of the best Task-CL methods with almost no forgetting. CAT [21] is similar to HAT\nbut can work with a mixed sequence. UCL [1] is a recent Task-CL method. EWC [27] is a popular\nregularization-based class continual learning (Class-CL) method, which was adapted for Task-CL by\nonly training on the corresponding head of the specific task ID during training and only considering\nthe corresponding head\u2019s prediction during testing. OWM [68] is also a Class-CL method, which we\nalso adapt to Task-CL. L2 [27] is a classic regularization based Class-CL method, which we adapt\nto Task-CL. A-GEM [4] is an efficient version of the replay Task-CL method GEM [36], which\npenalizes the previous task loss from being increased. DER++ [2] is a recent replay method using\ndistillation to regularize the new task training and it can function as a Task-CL method. B-CL [24] is\nsimilar to CTR but uses dynamic routing for knowledge transfer and it performs markedly worse than\nCTR. LAMOL [58] is a pseudo-replay method based on GPT-2.\n\nFrom the 10 systems, we created 10 baselines using W2V embeddings with the aspect term added\nbefore the sentence so that the Task-CL methods can take both aspect and the review sentence for\nASC; 7 baselines using Adapter-BERT (SRK, KAN and CAT\u2019s architectures cannot work with\nadapters); and 10 baselines using BERT (Frozen) (which replaces W2V embeddings). The BERT\nformulation in Sec. 3 naturally takes both aspect and review sentence in the ASC case.\n\n5.3 Hyper-parameters\n\nUnless otherwise stated, the same hyper-parameters are used in experiments for ASC, DSC and\n20News datasets. For the knowledge sharing module (KSM), we employ 2 layers of fully connected\nnetwork with dimensions 768 in the TK-Layer. We employ 3 transfer capsules. We experimented\nwith 2 to 15 capsules and selected 3 based on the validation data accuracy. For the task specific\nmodule (TSM), we use 2000 dimensions as the final and the hidden layers of the TSM. The task\nID embeddings have 2000 dimensions. A fully connected layer with softmax output is used as the\nclassification heads in the last layer of the BERT, together with the categorical cross-entropy loss.\ndropout of 0.5 between fully connected layers. The training of BERT, Adapter-BERT and CTR\nfollows that of [65]. We adopt BERTBASE (uncased). The maximum input length is set to 128 which\nis long enough for both ASC and DSC. We use Adam optimizer and set the learning rate to 3e-5.\nWe use 10 epochs for SemEval datasets and 30 epochs for the other datasets in the ASC application.\nFor DSC, we use 20 epochs. For 20News, we use 10 epochs. These are selected based on the\nvalidation results. The batch size is set to 32 for all cases. For all the other CL baselines, we use the\ncode provided by their authors and adapt them for text classification. We also adopt their original\nparameters, including learning rate, early stopping, and batch size.\n\n5.4 Evaluation Results and Analysis\n\nSince the order of the tasks in a sequence may impact the final results, we randomly sampled 5 task\nsequences and averaged their results. We compute both accuracy and Macro-F1, where Macro-F1 is\nthe primary metric as the imbalanced classes (in ASC) introduce biases on accuracy. Table 2 gives\nthe average results of all the 19 tasks (or datasets) for ASC and 10 tasks (or datasets) for DSC and\n20News over the 5 random task sequences.\n\nOverall Performance. Table 2 shows that CTR clearly outperforms all baselines.\n\n(1). For the standalone (SDL) baselines, BERT (with fine-tuning) and Adapter-BERT perform\nsimilarly. W2V and BERT (Frozen) are poorer, which is understandable.\n\n(2). Comparing SDL (standalone learning) and NFH (continual learning with no forgetting handling),\nwe see NFH is much better than SDL for W2V and BERT (Frozen). This indicates ASC and DSC\ntasks have similarities and thus shared knowledge. Catastrophic forgetting (CF) is not a major issue\nfor W2V and BERT (Frozen). However, for 20News, NFH variants have serious CF. NFH with BERT\n(fine-tuning) is much worse than SDL and Adapter-BERT, which we explained in Introduction.\n\n8\n\n\n\nScenarios Category Model\nASC DSC (small) DSC (full) 20News 20News (FR)\n\nAcc. MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. MF1\n\nNon-continual\nLearning (SDL)\n\nBERT MTL 91.91 88.11 85.05 84.03 89.77 89.28 96.77 96.77 \u2014 \u2014\nBERT SDL 85.84 76.35 78.04 74.17 87.84 86.80 96.49 96.48 \u2014 \u2014\nBERT (Frozen) SDL 78.14 58.13 73.88 67.97 85.34 80.17 96.49 96.48 \u2014 \u2014\nAdapter-BERT SDL 85.96 78.07 76.31 71.04 88.30 87.31 96.20 96.19 \u2014 \u2014\nW2V SDL 77.01 51.89 62.06 53.80 69.57 65.51 94.72 94.72 \u2014 \u2014\n\nContinual\nLearning (CL)\n\nBERT NFH 49.60 43.08 73.87 69.44 73.08 71.81 52.50 39.22 24.29 30.52\nBERT (Frozen) NFH 85.51 76.64 83.12 79.23 61.88 45.79 83.28 81.81 8.76 9.73\nAdapter-BERT NFH 54.03 44.81 63.76 53.95 64.94 63.40 68.29 61.70 30.59 3.79\nW2V NFH 82.69 73.56 65.16 57.48 70.40 68.03 90.74 90.59 4.30 4.47\n\nBERT (frozen)\n\nL2 56.04 38.40 59.17 48.39 69.80 62.63 72.14 65.39 24.57 32.05\nA-GEM 86.06 78.44 59.33 45.94 70.67 61.77 93.31 92.95 4.09 4.48\nDER++ 84.27 75.08 72.29 66.28 86.70 85.46 60.44 49.67 10.54 12.16\nKAN 85.49 77.38 77.27 72.34 82.32 81.23 73.07 69.97 15.52 18.87\nSRK 84.76 78.52 78.58 76.03 83.99 82.66 79.64 77.89 12.06 13.97\nEWC 86.37 74.52 82.38 78.41 72.77 65.76 80.26 78.60 3.50 3.03\nUCL 83.89 74.82 80.12 74.13 74.76 69.48 94.65 94.63 0.48 0.48\n\nOWM 87.02 79.31 58.07 42.63 86.30 85.36 84.54 82.73 13.80 15.81\nHAT 86.74 78.16 79.48 72.78 87.29 86.14 93.51 92.93 2.26 2.89\nCAT 83.68 68.64 67.41 56.22 87.34 86.51 95.17 95.16 0.80 0.81\n\nAdapter-BERT\n\nL2 63.97 52.43 67.26 62.76 73.03 71.50 69.56 65.50 23.12 27.39\nA-GEM 45.88 28.21 62.89 55.96 71.22 69.94 60.29 50.40 40.22 51.20\nDER++ 47.63 35.54 70.52 63.56 59.67 57.82 58.95 49.58 36.39 45.30\nEWC 56.30 49.58 58.23 51.03 62.69 61.51 61.86 53.94 37.79 46.58\nUCL 64.46 36.64 48.30 32.07 57.06 55.86 51.75 36.06 4.70 6.60\n\nOWM 72.99 66.51 73.97 71.96 85.46 84.57 71.10 66.25 27.38 32.76\nHAT 86.14 78.52 80.83 78.41 88.00 87.26 95.22 95.21 0.33 0.34\n\nW2V\n\nL2 60.36 39.13 54.34 43.19 57.71 48.00 59.54 54.40 7.83 11.89\nA-GEM 81.33 63.35 69.80 60.07 77.67 70.75 90.72 90.60 3.94 4.08\nDER++ 83.27 69.93 77.51 73.13 74.79 66.68 89.28 89.19 4.32 4.42\nKAN 72.06 40.01 57.13 43.75 69.35 64.78 57.92 51.65 20.98 27.02\nSRK 71.01 39.63 64.47 55.93 69.65 65.25 61.07 58.47 7.26 8.81\nEWC 84.16 72.29 64.82 57.20 70.00 65.11 91.86 91.80 2.64 2.71\nUCL 84.41 75.99 56.23 41.34 70.56 67.01 90.61 90.46 4.53 4.70\n\nOWM 82.70 71.18 53.40 38.44 67.15 65.42 71.97 68.75 24.00 27.56\nHAT 80.83 63.63 62.57 50.83 69.75 65.44 67.73 64.43 26.04 29.70\nCAT 76.28 54.65 55.19 35.28 79.58 75.99 70.38 68.04 24.37 26.95\n\nB-CL 88.29 81.40 82.01 80.63 79.76 76.51 95.07 95.04 0.58 0.59\nLAMOL 88.91 80.59 89.12 86.58 92.11 91.72 66.13 45.74 20.03 16.60\n\nCTR (forward) 87.89 80.25 83.75 82.55 89.86 89.16 95.63 95.62 \u2014 \u2014\nCTR 89.47 83.62 84.34 83.29 89.31 88.75 95.25 95.23 0.42 0.43\n\nTable 2: Accuracy (Acc.) and Macro-F1 (MF1), averaged over 5 random sequences. The architectures\nof SRK, KAN and CAT cannot work with Adapter-BERT. \u201c\u2014\u201d means not applicable. Due to the\nlimited space, standard deviation, running time and network size are given in Supplementary.\n\n(3). Unlike BERT and Adapter-BERT, CTR does very well in both forgetting avoidance and knowl-\nedge transfer (outperforming all baselines). For baselines, EWC, UCL, OWM and HAT, although\nthey perform better than NFH, they are significantly poorer than CTR as they don\u2019t have methods to\nencourage knowledge transfer for ASC and DSC. KAN and SRK do knowledge transfer but they are\nweaker than many other CL methods. They perform very poorly for 20News as they have limited\nability to overcome CF. CAT works well with large datasets, but is weak for small datasets.\n\n(4). CTR\u2019s improvements over SDL variants for DSC (large) is less than for DCS (small). This is\nunderstandable because when the training data is large, learning a separate model is already good,\nand knowledge transfer is less important.\n\n(5). Compared with the SDL results, we can see that CTR has the least forgetting on 20News.\n\n(6). Compared to B-CL, CTR is markedly better in knowledge transfer. The forgetting rates (FR) of\nB-CL and CTR are both low. The comparison is in fact the same as comparing dynamics routing and\ntransfer routing. We can see that the proposed transfer routing is dramatically better than dynamic\nrouting for knowledge transfer. Additionally, transfer routing eliminates the need for hyper-parameter\ntuning on the number of iterations of dynamic routing [50] to update the agreements.\n\n(7). CTR outperforms LAMOL in ASC and 20News even with the less powerful BERT model that\nCTR adopts. LAMOL outperforms BERT-based MTL in DSC. This may be because LAMOL is\nbased on GPT-2, which is known to be more powerful than BERT (also shown in the LAMOL paper).\nFor 20News, since its tasks are very different\/dissimilar, there is little shared knowledge. Dealing\nwith CF is the main issue. LAMOL has serious forgetting as its FR values show.\n\n(8). The results of MTL (the upper bound) are only slightly better than CTR, which again shows that\nCTR is highly effective in overcoming forgetting and encouraging knowledge transfer.\n\n9\n\n\n\nModel\nASC DSC (small) DSC (full) 20News\n\nAcc. MF1 Acc. MF1 Acc. MF1 Acc. MF1\nCTR (-KSM;-TSM) 0.5403 0.4481 0.6376 0.5395 0.6494 0.6340 0.6829 0.6170\n\nCTR (-TSM) 0.8312 0.7107 0.7085 0.6759 0.8545 0.8380 0.8275 0.8064\nCTR (-KSM) 0.8614 0.7852 0.8083 0.7841 0.8800 0.8726 0.9522 0.9521\n\nCTR (-TR\/KSM) 0.8819 0.8155 0.8244 0.8119 0.8831 0.8762 0.9476 0.9469\nCTR (dynamic routing) (B-CL) 0.8829 0.8140 0.8201 0.8063 0.7976 0.7651 0.9507 0.9504\n\nCTR (on top) 0.8135 0.6390 0.7301 0.6875 0.8266 0.8105 0.8927 0.8920\nCTR (after the first FF layer) 0.8741 0.8014 0.8300 0.8183 0.8699 0.8596 0.9381 0.9373\n\nCTR (after the other two FF layers) 0.8662 0.7863 0.8269 0.8161 0.8714 0.8612 0.9339 0.9316\nCTR 0.8947 0.8362 0.8434 0.8329 0.8931 0.8875 0.9525 0.9523\n\nTable 3: Ablation experiment results.\n\nEffectiveness of Knowledge Transfer. Forward transfer is defined as the forward performance\n(CTR(forward) in Table 2) subtracting the standalone (SDL) result. CTR(forward) is the test\naccuracy or MF1 of each task when it was first learned. Backward transfer is defined as the difference\nbetween the backward performance (CTR in Table 2, the final result after all tasks are learned) and\nthe forward performance. The average results of CTR (forward) and CTR are given in Table 2. We\nobserve that forward transfer of CTR is highly effective for the three datasets with similar tasks. For\nDSC, the less the data, the more effective is the transfer, which is reasonable. Backward transfer\nimproves the accuracy and MF1 of ASC and DSC (small). For DSC (full), it is slightly weaker and for\n20News, it is also slightly weaker due to a very small amount of forgetting, but the less than 0.0055\nCF is negligible. Note that in [36, 46], forward transfer is measured by comparing the test results of\nthe new task on the current learned network and a random initialized network before\/without training\nthe new task (like zero-shot). This method indicates whether the learned network contains some\nuseful knowledge for the new task. However, it does not tell how much forward transfer actually\nhappens after learning the new task, which is more important and is what our method measures.\n\nOvercoming Forgetting. To validate CTR\u2019s effectiveness in dealing forgetting with a sequence of\ndissimilar tasks, we compute the Forgetting Rate [34], FR = 1\n\nt\u22121\n\u2211t\u22121\ni=1 Ai,i \u2212 At,i, where Ai,i is\n\nthe forward accuracy of task i and At,i is the accuracy of task i after training the last task t. We\naverage over all tasks except the last one because the last task obviously has no forgetting. We report\nthe forgetting rate FR (averaged over 5 random task sequences) for the 20News data on the two\nevaluation metrics in the last two columns of Table 2 (the other two datasets are mainly for knowledge\ntransfer). CTR has very low FR values which indicate very little forgetting.\n\nAblation Study. The results of ablation experiments are in Table 3. \u201c-KSM;-TSM\u201d means that\nthe knowledge sharing module (KSM) and the task specific module (TSM) are not used, and we\nsimply deploy the Adapter-BERT. \u201c-KSM\u201d means that the knowledge sharing module (KSM) is\nnot used. \u201c-TSM\u201d means that the task specific module (TSM) is not used. \u201c-TR\/KSM\u201d means that\nthe task router (TR) in KSM is not used. We directly send the transferable representation vtran(t)\n\nj|i\nto the next layer. \u201cdynamic routing\u201d means that we replace our transfer routing (-(SE&TR)\/KSM)\nwith dynamic routing [50], which is one of the most popular routing algorithms in capsule networks.\n\u201con top\u201d means adding a CL-plugin on top of BERT. \u201cafter the first FF layer\u201d means adding only\none CL-plugin there in BERT. \u201dafter the other two FF layers\u201d means adding only one CL-plugin\nmodule there in BERT. Table 3 shows that the full CTR system gives the best results, indicating every\ncomponent contributes to the model and other options of adding CL-plugins are all poorer.\n\n6 Conclusion\n\nThis paper studied task continual learning (Task-CL) using the pre-trained model BERT to achieve\nboth CF presentation and knowledge transfer. It proposed a novel technique called CTR to leverage\nthe pre-trained BERT for CL. The key component of CTR is the CL-plugin inserted in BERT. A\nCL-plugin is a capsule network with a new transfer routing mechanism to encourage knowledge\ntransfer among tasks and also to isolate task-specific knowledge to avoid forgetting. Experimental\nresults using three NLP applications showed that CTR markedly improves the performance of both\nthe new task and the old tasks via knowledge transfer and is also effective at overcoming catastrophic\nforgetting. One limitation of our work is the efficiency due to the use of capsules. Capsules try to\nrepresent a group of neurons in a vector reflecting properties of an entity. In NLP, an entity is a\nsentence\/document which contains many tokens (e.g., 128) and features (e.g. 768 in BERTBASE).\nGrouping them makes the capsule very large (e.g., 128 \u00d7 768), which slows down training.\n\n10\n\n\n\nAcknowledgments\n\nThis work was supported in part by two National Science Foundation (NSF) grants (IIS-1910424 and\nIIS-1838770), a DARPA Contract HR001120C0023, and a Northrop Grumman research gift.\n\nReferences\n[1] H. Ahn, S. Cha, D. Lee, and T. Moon. Uncertainty-based continual learning with adaptive\n\nregularization. In NIPS, pages 4394\u20134404, 2019.\n[2] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for general\n\ncontinual learning: a strong, simple baseline. arXiv preprint arXiv:2004.07211, 2020.\n[3] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka, and T. Mitchell. Toward an\n\narchitecture for never-ending language learning. In AAAI, volume 24, 2010.\n[4] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efficient lifelong learning with\n\nA-GEM. In ICLR, 2019.\n[5] Z. Chen and B. Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence\n\nand Machine Learning, 12(3):1\u2013207, 2018.\n[6] Z. Chen, N. Ma, and B. Liu. Lifelong learning for sentiment classification. In ACL, pages\n\n750\u2013756, 2015.\n[7] Y.-S. Chuang, S.-Y. Su, and Y.-N. Chen. Lifelong language knowledge distillation. arXiv\n\npreprint arXiv:2010.02123, 2020.\n[8] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional\n\ntransformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,\nNAACL-HLT, pages 4171\u20134186. Association for Computational Linguistics, 2019.\n\n[9] X. Ding, B. Liu, and P. S. Yu. A holistic lexicon-based approach to opinion mining. In\nProceedings of the 2008 international conference on web search and data mining, pages\n231\u2013240, 2008.\n\n[10] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wierstra.\nPathnet: Evolution channels gradient descent in super neural networks. CoRR, abs\/1701.08734,\n2017.\n\n[11] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov. Learning word vectors for 157\nlanguages. In LREC, 2018.\n\n[12] C. Greco, B. Plank, R. Fern\u00e1ndez, and R. Bernardi. Psycholinguistics meets continual\nlearning: Measuring catastrophic forgetting in visual question answering. arXiv preprint\narXiv:1906.04229, 2019.\n\n[13] P. Gupta, Y. Chaudhary, T. Runkler, and H. Schuetze. Neural topic modeling with continual\nlifelong learning. In International Conference on Machine Learning, pages 3907\u20133917. PMLR,\n2020.\n\n[14] X. He and H. Jaeger. Overcoming catastrophic interference using conceptor-aided backpropaga-\ntion. In ICLR, 2018.\n\n[15] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In International\nconference on artificial neural networks, pages 44\u201351. Springer, 2011.\n\n[16] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At-\ntariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In K. Chaudhuri and\nR. Salakhutdinov, editors, ICML, volume 97 of Proceedings of Machine Learning Research,\npages 2790\u20132799. PMLR, 2019.\n\n[17] M. Hu and B. Liu. Mining and summarizing customer reviews. In Proceedings of ACM\nSIGKDD, pages 168\u2013177, 2004.\n\n[18] W. Hu, Q. Qin, M. Wang, J. Ma, and B. Liu. Continual learning by using information of each\nclass holistically. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npages 7797\u20137805, 2021.\n\n[19] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In ICLR.\nOpenReview.net, 2017.\n\n11\n\n\n\n[20] N. Kamra, U. Gupta, and Y. Liu. Deep generative dual memory network for continual learning.\nCoRR, abs\/1710.10368, 2017.\n\n[21] Z. Ke, B. Liu, and X. Huang. Continual learning of a mixed sequence of similar and dissimilar\ntasks. In NeurIPS, 2020.\n\n[22] Z. Ke, B. Liu, H. Wang, and L. Shu. Continual learning with knowledge transfer for sentiment\nclassification. In ECML-PKDD, 2020.\n\n[23] Z. Ke, B. Liu, H. Xu, and L. Shu. Classic: Continual and contrastive learning of aspect sentiment\nclassification tasks. In EMNLP, 2021.\n\n[24] Z. Ke, H. Xu, and B. Liu. Adapting bert for continual learning of a sequence of aspect sentiment\nclassification tasks. In NAACL, pages 4746\u20134755, 2021.\n\n[25] H. Khayrallah, B. Thompson, K. Duh, and P. Koehn. Regularized training objective for\ncontinued training for domain adaptation in neural machine translation. In Proceedings of the\n2nd Workshop on Neural Machine Translation and Generation, pages 36\u201344, 2018.\n\n[26] Y. Kim. Convolutional neural networks for sentence classification. In A. Moschitti, B. Pang,\nand W. Daelemans, editors, EMNLP, pages 1746\u20131751. ACL, 2014.\n\n[27] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,\nJ. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and\nR. Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, abs\/1612.00796,\n2016.\n\n[28] K. Lang. Newsweeder: Learning to filter netnews. In Proceedings of the Twelfth International\nConference on Machine Learning, pages 331\u2013339, 1995.\n\n[29] M. D. Lange, R. Aljundi, M. Masana, and T. Tuytelaars. Continual learning: A comparative\nstudy on how to defy forgetting in classification tasks. CoRR, abs\/1909.08383, 2019.\n\n[30] S. Lee, J. Kim, J. Jun, J. Ha, and B. Zhang. Overcoming catastrophic forgetting by incremental\nmoment matching. In NIPS, pages 4652\u20134662, 2017.\n\n[31] Y. Li, L. Zhao, K. Church, and M. Elhoseiny. Compositional language continual learning. In\nInternational Conference on Learning Representations, 2019.\n\n[32] Q. Liu, Z. Gao, B. Liu, and Y. Zhang. Automated rule selection for aspect extraction in opinion\nmining. In IJCAI, 2015.\n\n[33] T. Liu, L. Ungar, and J. Sedoc. Continual learning for sentence representations using conceptors.\narXiv preprint arXiv:1904.09187, 2019.\n\n[34] Y. Liu, Y. Su, A. Liu, B. Schiele, and Q. Sun. Mnemonics training: Multi-class incremental\nlearning without forgetting. In CVPR, pages 12242\u201312251. IEEE, 2020.\n\n[35] Z. Liu, G. I. Winata, A. Madotto, and P. Fung. Exploring fine-tuning techniques for pre-trained\ncross-lingual models via continual learning. arXiv preprint arXiv:2004.14218, 2020.\n\n[36] D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. In NIPS,\npages 6467\u20136476, 2017.\n\n[37] G. Lv, S. Wang, B. Liu, E. Chen, and K. Zhang. Sentiment classification by leveraging the\nshared knowledge from a sequence of domains. In DASFAA, pages 795\u2013811, 2019.\n\n[38] A. Madotto, Z. Lin, Z. Zhou, S. Moon, P. Crook, B. Liu, Z. Yu, E. Cho, and Z. Wang. Continual\nlearning in task-oriented dialogue systems. arXiv preprint arXiv:2012.15504, 2020.\n\n[39] A. Mallya and S. Lazebnik. Packnet: Adding multiple tasks to a single network by iterative\npruning. In CVPR, pages 7765\u20137773, 2018.\n\n[40] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learning and motivation, volume 24, pages\n109\u2013165. Elsevier, 1989.\n\n[41] A. Merchant, E. Rahimtoroghi, E. Pavlick, and I. Tenney. What happens to BERT embeddings\nduring fine-tuning? CoRR, abs\/2004.14448, 2020.\n\n[42] J. Pfeiffer, A. Kamath, A. R\u00fcckl\u00e9, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive\ntask composition for transfer learning. In EACL, 2020.\n\n12\n\n\n\n[43] Q. Qin, W. Hu, and B. Liu. Using the past knowledge to improve sentiment classification. In\nEMNLP-findings, 2020.\n\n[44] V. V. Ramasesh, E. Dyer, and M. Raghu. Anatomy of catastrophic forgetting: Hidden represen-\ntations and task semantics. In ICLR, 2021.\n\n[45] S. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and\nrepresentation learning. In CVPR, pages 5533\u20135542, 2017.\n\n[46] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu, , and G. Tesauro. Learning to\nlearn without forgetting by maximizing transfer and minimizing interference. In International\nConference on Learning Representations, 2019.\n\n[47] M. Rostami, S. Kolouri, and P. K. Pilly. Complementary learning for overcoming catastrophic\nforgetting using experience replay. In IJCAI, pages 3339\u20133345, 2019.\n\n[48] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu,\nR. Pascanu, and R. Hadsell. Progressive neural networks. CoRR, abs\/1606.04671, 2016.\n\n[49] P. Ruvolo and E. Eaton. ELLA: an efficient lifelong learning algorithm. In ICML, pages\n507\u2013515, 2013.\n\n[50] S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. In NIPS, pages\n3856\u20133866, 2017.\n\n[51] A. Seff, A. Beatson, D. Suo, and H. Liu. Continual learning in generative adversarial nets.\nCoRR, abs\/1705.08395, 2017.\n\n[52] J. Serr\u00e0, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard\nattention to the task. In ICML, pages 4555\u20134564, 2018.\n\n[53] Y. Shen, X. Zeng, and H. Jin. A progressive model to enable continual learning for semantic\nslot filling. In EMNLP-IJCNLP, Nov. 2019.\n\n[54] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. In NIPS,\npages 2990\u20132999, 2017.\n\n[55] L. Shu, H. Xu, and B. Liu. Lifelong learning CRF for supervised aspect extraction. In ACL,\npages 148\u2013154, 2017.\n\n[56] D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning\nalgorithms. In Lifelong Machine Learning, Papers from the 2013 AAAI Spring Symposium, Palo\nAlto, California, USA, March 25-27, 2013, 2013.\n\n[57] C. Sun, L. Huang, and X. Qiu. Utilizing BERT for aspect-based sentiment analysis via\nconstructing auxiliary sentence. In NAACL, pages 380\u2013385, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics.\n\n[58] F.-K. Sun, C.-H. Ho, and H.-Y. Lee. Lamol: Language modeling is all you need for lifelong\nlanguage learning. In ICLR, 2020.\n\n[59] D. Tang, B. Qin, and T. Liu. Aspect level sentiment classification with deep memory network. In\nEMNLP, pages 214\u2013224, Austin, Texas, Nov. 2016. Association for Computational Linguistics.\n\n[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin. Attention is all you need. In NeurIPS, pages 5998\u20136008, 2017.\n\n[61] H. Wang, B. Liu, S. Wang, N. Ma, and Y. Yang. Forward and backward knowledge transfer for\nsentiment classification. In ACML, pages 457\u2013472, 2019.\n\n[62] S. Wang, G. Lv, S. Mazumder, G. Fei, and B. Liu. Lifelong learning memory networks for\naspect sentiment classification. In IEEE International Conference on Big Data, pages 861\u2013870,\n2018.\n\n[63] Z. Wang, S. V. Mehta, B. P\u00f3czos, and J. Carbonell. Efficient meta lifelong-learning with limited\nmemory. In EMNLP, 2020.\n\n[64] R. Xia, J. Jiang, and H. He. Distantly supervised lifelong learning for large-scale social media\nsentiment analysis. IEEE Trans. Affective Computing, 8(4):480\u2013491, 2017.\n\n[65] H. Xu, B. Liu, L. Shu, and P. S. Yu. BERT post-training for review reading comprehension and\naspect-based sentiment analysis. In J. Burstein, C. Doran, and T. Solorio, editors, NAACL-HLT,\npages 2324\u20132335. Association for Computational Linguistics, 2019.\n\n13\n\n\n\n[66] H. Xu, S. Xie, L. Shu, and P. S. Yu. Dual attention network for product compatibility and\nfunction satisfiability analysis. In AAAI, 2018.\n\n[67] W. Xue and T. Li. Aspect based sentiment analysis with gated convolutional networks. In\nI. Gurevych and Y. Miyao, editors, ACL, pages 2514\u20132523. Association for Computational\nLinguistics, 2018.\n\n[68] G. Zeng, Y. Chen, B. Cui, and S. Yu. Continuous learning of context-dependent processing in\nneural networks. Nature Machine Intelligence, 2019.\n\n[69] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In\nInternational Conference on Machine Learning, pages 3987\u20133995. PMLR, 2017.\n\n14\n\n\n\t1 Introduction\n\t2 Related Work\n\t3 CTR Architecture\n\t4 Continual Learning Plug-in (CL-plugin)\n\t4.1 Knowledge Sharing Module (KSM)\n\t4.1.1 Task Capsule Layer (TK-Layer)\n\t4.1.2 Transfer Routing and Transfer Capsule Layer\n\n\t4.2 Task Specific Module (TSM)\n\n\t5 Experiments\n\t5.1 Three Applications and Their Datasets\n\t5.2 Baselines\n\t5.3 Hyper-parameters\n\t5.4 Evaluation Results and Analysis\n\n\t6 Conclusion\n\n","12":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Application Independent Natural Language Interface\n\nSahisnu Mazumder, Bing Liu, Shuai Wang, Sepideh Esmaeilpour\nDepartment of Computer Science, University of Illinois at Chicago, USA\n\nsahisnumazumder@gmail.com,liub@uic.edu\nshuaiwanghk@gmail.com,sesmae2@uic.edu\n\nAn extended and revised version of this\nwork has been published in the HLDS Work-\nshop of NeurIPS 2020 as follows:\n\nSahisnu Mazumder, Bing Liu, Shuai\nWang, and Sepideh Esmaeilpour.\nAn Application-Independent Ap-\nproach to Building Task-Oriented\nChatbots with Interactive Contin-\nual Learning. In NeurIPS-2020\nWorkshop on Human in the Loop Di-\nalogue Systems, 2020.\n\nHLDS@NeurIPS: main paper & sup-\nplementary\n\nclick here for the pdf of main paper:\nHLDS@NeurIPS 2020 version link\n\nPlease consider the HLDS@NeurIPS 2020\nversion for citation as mentioned above.\n\nAbstract\n\nTraditional approaches to building natural lan-\nguage (NL) interfaces typically use a semantic\nparser to parse the user command and convert\nit to a logical form, which is then translated\nto an executable action in an application. How-\never, it is still challenging for a semantic parser\nto correctly parse natural language. For a dif-\nferent domain, the parser may need to be re-\ntrained or tuned, and a new translator also\nneeds to be written to convert the logical forms\nto executable actions. In this work, we propose\na novel and application independent approach\nto building NL interfaces that does not need a\nsemantic parser or a translator. It is based on\nnatural language to natural language matching\nand learning, where the representation of each\naction and each user command are both in nat-\nural language. To perform a user intended ac-\ntion, the system only needs to match the user\n\ncommand with the correct action representa-\ntion, and then execute the corresponding ac-\ntion. The system also interactively learns new\n(paraphrased) commands for actions to expand\nthe action representations over time. Our ex-\nperimental results show the effectiveness of\nthe proposed approach.\n\n1 Introduction\n\nExisting techniques (Zelle and Mooney, 1996;\nArtzi and Zettlemoyer, 2013; Andreas and Klein,\n2015; Zettlemoyer and Collins, 2012; Li and Rafiei,\n2018) for building natural language interfaces\n(NLIs) often use a semantic parser to parse the\nnatural language (NL) command from the user and\nconvert it to a logical form (LF) and then, trans-\nlate LF into an executable action in the application.\nThis approach has several limitations: (1) it is still\nvery challenging for a semantic parser to correctly\nparse natural language. (2) For different applica-\ntions, the parser may need to be retrained\/tuned\nwith an application domain corpus. (3) For each\napplication, a different translator is needed to con-\nvert the logic forms into executable actions. (4)\nDue to the last two limitations, it is difficult to\nbuild an application-independent system.\n\nThis work proposes a novel approach to NLI de-\nsign, called Natural Language to Natural Language\n(NL2NL). This approach works in the following set-\nting: Let the underlying application has a finite set\nof executable actions\/functions that the user can\nuse to accomplish his\/her goal. These actions or\nfunctions for end users are commonly defined by\nthe application as Application Programming Inter-\nfaces (APIs). Most applications work in this setting\nas providing APIs is a standard approach. For each\nAPI, the proposed approach attaches a natural lan-\nguage representation of it, which is a set of one or\nmore API seed commands (ASCs) written in nat-\nural language (e.g., by the application developer)\n\nar\nX\n\niv\n:1\n\n91\n0.\n\n14\n08\n\n4v\n2 \n\n [\ncs\n\n.C\nL\n\n] \n 2\n\n5 \nN\n\nov\n 2\n\n02\n1\n\nhttps:\/\/sites.google.com\/view\/hlds-2020\/home\nhttps:\/\/sites.google.com\/view\/hlds-2020\/home\nhttps:\/\/drive.google.com\/file\/d\/11NTjKfCRvEPu1__CPOrwJ1QYlBrNrjIJ\/view\n\n\njust like a natural language command from the user\nto invoke the API. The only difference is that the\nobjects to be acted upon in each ASC are replaced\nwith variables, which indicate the arguments of the\nAPI. These variables will be instantiated to actual\nobjects\/arguments in a real user command. Let\u2019s\nsee an example.\n\nConsider an application like Microsoft Paint and\nan API like drawCircle(X1,X2) (drawing a\ncircle having color X1 at coordinate X2). One\nASC for this API can be \u201cdraw a X1 circle at X2\u201d.\nClearly, X1 and X2 are arguments of the API and\nrepresented as variables in the ASC. When the user\ngives a natural language command, the system sim-\nply matches the command with one of the ASCs\nand in doing so, also instantiates the objects (i.e.,\nAPI arguments referred in the user command) for\nthe associated API to be executed. For example, the\nuser command \u201cdraw a blue circle at (20, 40)\u201d can\nbe grounded to the aforementioned ASC, where\nthe grounded API arguments are X1 =\u2018blue\u2019 and\nX2 =(20, 40). Since both the user command and\nthe ASCs are written in natural language, we call\nthis approach natural language to natural language\n(NL2NL).\n\nSince the user may use many different language\nexpressions to express the same command, the\nmatching process is still challenging. For exam-\nple, given the ASC (\u2018draw a X1 circle at X2\u2019),\nthe user may say \u201cinsert a circle with color blue\nat (20, 40).\u201d The matching algorithm may not\nbe able to match them. Also, if the user says\n\u201cmove the circle with color blue to (20, 40)\u201d, it\nshould be grounded\/matched to a different API\n[e.g., moveCircle(X1, X2)]. As the developer\nhas to provide API seed commands (ASCs) for the\nAPIs of the underlying application, it can be hard\nfor the developer to write all possible paraphrased\ncommands for a given API. This affects the cover-\nage of the proposed NL2NL system in grounding\nuser commands. To deal with this issue, we also\npropose an interactive learning mechanism to inter-\nact with the end user in natural language to learn\nnew (paraphrased) commands and convert them\nto new ASCs and add them to the existing set of\nASCs (the set enlarges) so that when similar com-\nmands are issued by this or other users in the future,\nthe NL2NL approach can handle it. This enables\ncontinuous learning of new ASCs from users to\nimprove the subsequent grounding performance.\n\nThe proposed system based on NL2NL is called\n\nCML (Command Matching and Learning). CML\nhas three key advantages: (1) Due to the NL2NL\nmatching, we no longer need a semantic parser to\nconvert the user command to a logical form (or\naction formalism) or to write a program to translate\nthe logical form to an API call. This makes the\ndesign of natural language interfaces much sim-\npler and quick because ASCs are written in natu-\nral language (NL) just like user commands, and\ncan be easily written by the application developer\nfor the APIs of their application. (2) Again, as\nASCs are written in NL, the matching algorithm\nof CML can be used for any application and thus\nis application-independent. CML simply maps a\nuser command to a correct ASC, and the system ex-\necutes the API attached to the ASC. (3) CML also\nlearns new ASCs in the process of being used to\nmake it more powerful. To our knowledge, no exist-\ning approach is NL2NL, application independent,\nor able to learn after deployment.\n\nWe evaluate the proposed system CML using\ntwo representative applications: (1) Blocks-World,\nand (2) Webpage Design. Experimental results\nshow its effectiveness.\n\n2 Related Work\n\nMany existing works have been done on building\nnatural language interfaces (NLI) for various ap-\nplications. For robot navigation, (Artzi and Zettle-\nmoyer, 2013) proposed a weakly supervised learn-\ning method to train semantic parsers for grounding\nnavigation instructions. (Tellex et al., 2011) pro-\nposed a related method also for navigation and\nmobile manipulation. (Janner et al., 2018) worked\non understanding spatial references in NL for NLIs.\n(Guu et al., 2017) learns a reinforcement learning\n(RL) based semantic parser with indirect supervi-\nsion. (Andreas and Klein, 2015) gave a sequence-\nprediction based model for following NL instruc-\ntions. (Fried et al., 2017) proposed an unified prag-\nmatic model for instruction following. Other promi-\nnent works include NLI for scrutable robots (Garcia\net al., 2018) and dialog agents for mobile robots to\nunderstand instructions through semantic parsing\n(Thomason et al., 2015).\n\nIn NLI for databases, (Zelle and Mooney, 1996)\nused inductive logic programming to construct\nan NLI for database querying. (Zettlemoyer and\nCollins, 2007) learns a weighted combinatory cate-\ngorial grammar (CCG) for flight database queries.\n(Berant et al., 2013; Yih et al., 2015) proposed\n\n\n\na semantic parser for question-answering using\na knowledge base. Other prominent works on\ndatabase querying are (Baik et al., 2019; Xiong\nand Sun, 2019; Neelakantan et al., 2016; Li et al.,\n2019; Ferre\u0301, 2017; Liang, 2016; Zhong et al., 2017)\nand data exploration and visual analysis (Setlur\net al., 2016; Utama et al., 2018; Lawrence and Rie-\nzler, 2016; Gao et al., 2015). More information can\nbe found in (Li and Rafiei, 2018).\n\nFor webpages and GUIs, (Branavan et al., 2010)\nproposed a RL-based solution for mapping high-\nlevel instructions to API calls. (Su et al., 2017)\nproposed an end-to-end approach to designing NLI\nfor web APIs. A similar approach is also used in\n(Pasupat and Liang, 2015) for performing compu-\ntations on web tables, (Soh, 2017; Pasupat et al.,\n2018) designed an NLI for submitting web forms\nand interacting with webpages and (Lin et al., 2018)\nproposed an NLI for Bash commands.\n\nBesides these, there are works on selecting cor-\nrect objects referenced in utterances (Frank and\nGoodman, 2012; Golland et al., 2010; Smith et al.,\n2013; Celikyilmaz et al., 2014), learning language\ngames (Wang et al., 2016) and discovering com-\nmands in multimodal interfaces (Srinivasan et al.,\n2019).\n\nAll these approaches differ substantially from\nour work as they are based on learning semantic\nparsers of various kinds or end-to-end models with\nlabeled examples. CML is mainly based on natural\nlanguage to natural language (NL2NL) matching.\nDue to NL2NL matching, our approach presents\nan application-independent solution to NLI, in the\nsense that the application developer does not need\nto collect application-specific training examples to\nlearn a parser and our matcher can work with any\napplication. We only require the developer to write\nASCs in natural language for their APIs, which\nis much easier to do than collecting labeled data\nand training a parser or an end-to-end model. Fur-\nthermore, CML can learn new ASCs in the usage\nprocess (after the system is deployed) to make it\nmore and more powerful.\n\n3 Proposed Approach\n\nThe proposed approach CML consists of four parts:\n(1) an ASC (API seed command) specification, (2)\na utility constraint marker, (3) a command ground-\ning module, and (4) an interactive ASC learner.\nASC specification enables the application devel-\noper to specify a set of ASCs for each API of their\n\napplication. The utility constraint marker identi-\nfies some sub-expressions in each ASC that utility\nASCs should not be applied to. The command\ngrounding module grounds a user command to an\naction ASC (both are in natural language) for an\nAPI call. The ASC learner learns new ASCs from\nthe user to make the system more powerful. An nat-\nural language interface (NLI) is then built with no\napplication specific programming or data collection\nrequired.\n\n3.1 ASC Specification\n\nSince ASCs are written in natural language, they\ncan be specified by the application developer (with-\nout knowing how CML works). To achieve the\ngoal of application-independent NLI design, the\nproposed CML has to automatically read and un-\nderstand the ASCs for each application. We define\nan ASC specification for developers that can be fol-\nlowed to easily write ASCs for their applications.\n\nThe ASC specification consists of three parts:\n(1) properties and value domains of the applica-\ntion, (2) action ASC specification and (3) utility\nASC specification. Let the set of actions that can\nbe performed in the application be A. Each action\nai \u2208 A causes a change in the state of the (instan-\ntiated) objects in the application, specified by the\nobject properties and their values. For example,\nin the Microsoft Paint application, a circle drawn\non the editor is an example of instantiated object\nand it can have properties like color, name, shape,\netc, with their values like the color of the circle\nbeing red. And examples of actions are: draw a\ncircle, change the shape of the circle to make it a\nsquare, etc. Each of these actions is performed by\nan unique API call, referred to as an action API.\nThe developer defines one or more action ASCs\n[in (paraphrased) natural language] for each action\nAPI, where the arguments of the API are variables\nin the corresponding action ASC (see the Introduc-\ntion section).\n\nBesides action APIs, there are also Utility APIs,\nwhich are used to retrieve information about the\nproperties of the instantiated object and their val-\nues from the current application state and are used\nas helper functions to A. The concept of Utility\nAPI is explained as follows.\n\nOften a natural language command from the user\ninvolves one or more referential expressions to in-\nstantiated objects, which need to be resolved to\ninterpret the user command. A referential expres-\n\n\n\nTable 1: Properties and their domains for Blocks-World\n\nProperty Domain\n\ncolor (object)\n\u2019red\u2019, \u2019green\u2019, \u2019orange\u2019,\n\u2019blue\u2019, \u2019yellow\u2019\n\nshape (object)\n\u2019triangular\u2019, \u2019circular\u2019, \u2019cube\u2019,\n\u2019square\u2019, \u2019rectangular\u2019\n\nlocation (object) (x, y) coordinate in 2D space\nname (object) English alphabets A-Z\ndirection (action) \u2019right\u2019, \u2019left\u2019, \u2019above\u2019, \u2019below\u2019\n\nsion is a phrase used to refer an object indirectly\nby a property value. For example, in the command:\n\u201cmove the blue block to the left of the cube\u201d, \u2018blue\nblock\u201d, \u201ccube\u201d, \u201cleft of the cube\u201d are referential ex-\npressions, where \u2018blue block\u201d and \u201ccube\u201d refers to\nsome blocks having color \u201cblue\u201d and shape \u201ccube\u201d\nrespectively. The phrase \u201cleft of the cube\u201d refers to\nsome location in the application space, where the\nblue block is to be moved to.\n\nSince there may not be any action API that con-\nsider such indirect object references as a part of its\narguments, the aforementioned user command can-\nnot be directly grounded to any of the action APIs.\nIn such cases, utility APIs are used to resolve the\nreferential expressions by identifying the referred\nobjects, having a given property and a value. Thus,\nunlike action APIs, a utility API returns one or\nmore values as output (e.g., object ids) to the API\ncall which is used by an action API and\/or other\nutility APIs to unambiguously ground the user com-\nmand. Similar to action ASCs, the developer also\ndefines one or more utility ASCs [in (paraphrased)\nnatural language] for each utility API, where the\narguments of a utility API becomes variables in the\ncorresponding utility ASC.\n\n3.2 Blocks-World Specification: An Example\n\nWe now give an example specification for a Blocks-\nWorld application, which is about arranging dif-\nferent blocks or tiles on a 2D grid or adding them\nto form a goal arrangement. Note that, our CML\nsystem is not concerned with the goal or how to\nachieve it, which is the responsibility of the user.\nCML only matches each user command utterance\nto an action ASC for the associated action API to\nbe executed in the environment, with the help of\nutility ASCs.\n\nIn blocks-world, the objects are basically tiles of\ndifferent shapes, colors and names and the action\nAPIs are designed to manipulate these objects in\nthe 2D space. The utility APIs retrieve values of\nthe properties like shape, color and name, etc., of\nan instantiated tile. We use the term block and tile\n\ninterchangeably on-wards.\n\n3.2.1 Properties and Domains\nTable 1 shows different properties of an object or\naction (specified in the bracket next to the property\nname). The domain column in Table 1 lists the\ndomain of each property. For example, \u201ccolor\u201d\nis a property of a tile, which can take one of five\npossible values {\u2018red\u2019, \u2018green\u2019, \u2018orange\u2019, \u2018blue\u2019,\n\u2018yellow\u2019}. Similarly, shape denotes the shape of a\ntile and can be one of five categories: {\u2018triangular\u2019,\n\u2018circular\u2019, \u2018cube\u2019, \u2018square\u2019, \u2018rectangular\u2019}. The\nname of a block is denoted by any letter from A-Z.\nThe location of an object is specified by a 2D co-\nordinate (x, y) and so on. Besides these, \u2018direction\u2019\nis a property with domain {\u2018left\u2019, \u2018right\u2019, \u2018above\u2019,\n\u2018below\u2019} needed to execute an action for moving\nan object in a given direction.\n\n3.2.2 ASC Specification\nTable 2 shows the action ASCs and Table 3 shows\nthe utility ASCs and also, some example user com-\nmands and sub-expressions (explained later) that\ncan fire them. Note that, all ASCs are written\nin natural language and there is no fixed format.\nHere, the argument type \u2018block set\u2019 denotes a set\nof unique (instantiated) block object ids. We de-\nfine seven action ASCs and five utility ASCs to\nground a user command for Blocks-World. The\naction ASCs are: adding a block at location (x, y)\n[AID 1], removing a block [AID 2], moving blocks\n[AID 3-4], changing properties of a block [AID\n5-7]. Utility ASCs either retrieve the block set for\na given input property value [AID 8-11] or returns\na location object (x, y) in a given direction of a\nblock [AID 12]. The arguments marked (*) in Ta-\nble 2 denote the utility constraints automatically\nidentified by CML for restricting the use of utility\nASCs, which we discuss next.\n\n3.3 Utility Constraint Marker\n\nFor some actions, some variables (arguments) in\nthe user command should not be reduced as it can\nresult in incorrect subsequent grounding. As utility\nASCs are used to resolve sub-expressions in a user\ncommand, if a sub-expression (a sub-sequence of\nuser command, defined in next subsection) matches\nboth partially with an action ASC and fully with\na utility ASC, it creates ambiguity (see below). A\nsub-expression is said to be matched with an ASC,\nif the sequence of argument types of the variables\nappearing in the sub-expression from left to right\n\n\n\nTable 2: Action ASC specifications for Blocks-World and groundable example NL commands from user. (*) denotes that the\nvariable do not take part in command reduction (Utility Constraints), which is automatically detected and marked by CML. (X\ndenotes input)\n\nAPI Function AID Action ASCs Variable\/Argument: Type Example User Commands\n\nAdd(X1, X2) 1\nadd a block at X1;\ninsert a block at X1\n\n\u2019X1\u2019: \u2019location\u2019(*) add a block at row 2 and column 3; put a block\nat (2, 3)\n\nRemove(X1) 2 remove X1 \u2019X1\u2019: \u2019block set\u2019 delete blue block; take away blue\n\nMove(X1, X2) 3 move X1 to X2; shift X1 to X2 \u2019X1\u2019: \u2019block set\u2019, \u2019X2\u2019: \u2019location\u2019(*)\nmove blue block to the left of cube; shift green\ncube to (4, 5)\n\nMoveByUnits(X1,\nX2, X3)\n\n4\nmove X1 along X2\nby X3 units\n\n\u2019X1\u2019: \u2019block set\u2019, \u2019X2\u2019: \u2019direction\u2019,\n\u2019X3\u2019: \u2019number\u2019\n\nmove blue block left by 2 units; shift green\ncube down by 3 units\n\nUpdateColor(X1, X2) 5 change color of X1 to X2;\ncolor X1 X2\n\n\u2019X1\u2019:\u2019block set\u2019, \u2019X2\u2019:\u2019color\u2019(*) color A red; change color of B to blue\n\nUpdateShape(X1, X2) 6 change shape of X1 to X2 \u2019X1\u2019:\u2019block set\u2019, \u2019X2\u2019:\u2019shape\u2019(*) set the shape of A to cube; make B square\nRename(X1, X2) 7 rename block X1 to X2 \u2019X1\u2019: \u2019block set\u2019, \u2019X2\u2019:\u2019name\u2019(*) Name the block at (4, 5) as C; rename A to D\n\nTable 3: Utility ASC specifications for Blocks-World and groundable example sub-expressions (O denotes output, X denotes\ninput)\n\nAPI Function AID Utility ASC Argument: Type Example sub-expressions\nGetBlocksbyColor(X1) 8 color\/X1 X1: \u2019color\u2019, O1: \u2019block set\u2019 blue block\nGetBlocksbyShape(X1) 9 shape\/X1 X1: \u2019shape\u2019, O1: \u2019block set\u2019 square block; cube\nGetBlocksbyName(X1) 10 name\/X1 X1: \u2019name\u2019, O1: \u2019block set\u2019 block A\nGetBlocksbyLocation(X1) 11 location\/X1 X1: \u2019location\u2019, O1: \u2019block set\u2019 block at row 2 and column 3; block at (2, 3)\n\nGetLocation(X1, X2) 12\nget location\nalong X1 of X2\n\n\u2019X1\u2019: \u2019direction\u2019, \u2019X2\u2019: \u2019block set\u2019,\n\u2019O1\u2019: \u2019location\u2019\n\nat the left of blue block;\nbelow block B\n\nalso appears in the ASC. And by full match, we\nmean the number of argument types matched is\nequal to the number of argument types present in\nthe ASC. We mark each such argument appearing\nin the action ASC with a * for the action ASC indi-\ncating a utility constraint. Note, if a sub-expression\nmatches fully with an action ASC, there is no am-\nbiguity as the corresponding action API will be\nautomatically fired.\n\nLet\u2019s have an example: Consider the ASC with\nAID 1 in Table 2 for adding a block at location\nX1. Here, the argument X1:location [marked (*)]\nshould not be resolved using utility ASC of AID 11\n(getting the block(s) at the location). Otherwise, it\nwill mislead the grounding by reducing a user com-\nmand like \u201cadd a block at (2, 3)\u201d to \u201cadd a block at\nblock set\/X1\u201d. Similar problems arise when CML\nattempts to resolve arguments like color (X2) in\nASC with AID 5, shape (X2) in ASC with AID\n6, etc., using utility ASCs with AID 8 and AID 9\nrespectively, while grounding user commands like\n\u201ccolor block A to red\u201d and \u201cmake the blue block\nsquare\u201d respectively. we use * to mark these action\nASC arguments to indicate no reduction should be\napplied.\n\nFormally, let useq =\n\u3008type(X1u), ..., type(XNu)\u3009 be the sequence of\nvariable types in a utility ASC u from left to right,\nwhere Xiu is the ith variable in u. Similarly, let\naseq be the sequence of variable types (from left to\nright) in an action ASC a. Let Mseq be the longest\n\ncommon sub-sequence of useq and aseq. Then if\n|Mseq| = |useq|, all variables corresponding to the\nargument types in Mseq should be marked with (*)\nto indicate utility constraints for action ASC a.\n\n3.4 Command Grounding Module of CML\n\nGiven a natural language command C from the\nuser and the ASC specification S for an application,\nthe command grounding module (CGM) returns\na grounded ASC tuple A\u0302, consisting of one action\nASC and a set of utility ASCs. They together tell\nthe system what action (API call) to perform. If the\ngrounding is not possible, \u2205 is returned.\n\nCGM consists of two primary modules: a tag-\nging and rephrasing module (R) and an ASC\nmatching module [or simply Matcher] (M). R\ntags the input user command C and then, repharses\nthe tagged C to get a command C \u2032 that is lexi-\ncally closer to the ASCs. Here, a tag is an argu-\nment type of an action ASC. For example, in com-\nmand \u201dmove the cube to (2,3)\u201d, the phrase \u201ccube\u201d\nis tagged as \u201cshape\u201d and (2, 3) is tagged as \u201cloca-\ntion\u201d for blocks-world. This work uses a dictionary\nlookup based and regular expression-based tagger\nforR1. While reading the specification S,R forms\na tagset by enumerating all argument types in action\nASCs and also, builds a vocabulary V consisting\nof all the words in the ASCs. Given C, R either\n\n1As most NLI applications have finite domains of objects\nand properties, we found simple lookup based tagging works\nwell. In complex scenarios, R can be learned through user\nfeedback or provided by the developer.\n\n\n\nmaps individual words or phrases in C into one of\nthe tags, or repharses them by replacing synonym\nwords\/phrases from V using WordNet (Miller et al.,\n1990) and ConceptNet (Speer et al., 2017).\n\nGiven the rephrased and tagged user command\nC \u2032 and the set T of (action or utility) ASCs for an\napplication, MatcherM computes a match score\ns(t, C \u2032) for each t \u2208 T and returns the top ranked\nASC t\u0302 = argmaxt \u2208 T s(t, C \u2032). Any paraphrasing\nmodel can be used as M. This work uses infor-\nmation retrieval (IR) based unsupervised matching\nmodels forM (we compare different types of un-\nsupervised IR matching models in the Experiment\nsection).\n\n3.4.1 Working of Command Grounding\nModule (CGM)\n\nAlgorithm 1 shows the iterative grounding process\nof a user command C by CGM. We again use the\nBlocks-World application (see Figure 1) to explain\nthe algorithm on-wards.\n\u2022 CGM first performs tagging and rephras-\n\ning of C using R (Line 1). For example, the\nuser command C in Figure 1 is tagged to \u201dre-\nlocate the color\/X1 block to the direction\/X2 of\nname\/X3\u201d [where, X1=\u201dblue\u201d, X2=\u201dleft\u201d, X3=\u201dD\u201d]\nand rephrased command is C \u20320 in Figure 1 (C\n\n\u2032\n0 is\n\nC \u2032 in Line 1 of Algorithm 1 at the moment), where\n\u201crelocate\u201d is replaced with synonym \u201cmove\u201d found\nin vocabulary V .\n\u2022 Next, CGM enumerates and extracts the list\n\nof sub-expressions (SP ) present in C \u2032 (Line 2) to\nassist compositional grounding of C \u2032 using utility\nASCs in subsequent steps of the algorithm. A sub-\nexpression of length-m is defined as a substring of\nC \u2032 involving m consecutive variables (with types)\nand intermediate words linking them. For example,\ngiven C \u20320 in Figure 1, the list of all sub-expressions:\nSP = [ \u201ccolor\/X1\u201d, \u201cdirection\/X2\u201d, \u201cname\/X3\u201d,\n\u201ccolor\/X1 block to the direction\/X2\u201d, \u201cdirection\/X2\nof name\/X3\u201d], where the first three in SP are\nlength-1 and last two are length-2 sub-expressions.\nLength-3 sub-expression is the full command C \u20320\nand is discarded from SP as it does not take part\nin matching the utility ASCs, only matched with\naction ASCs.\n\nHere we also filter out those sub-expressions\nthat are marked with (*) (with utility constraints)\nas they should not be reduced by utility ASCs. For\nthis, CGM first uses MatcherM to identify the top-\nranked action ASC semantically close to C \u2032 (e.g.,\nAID 5 for the user command \u201ccolor block A to\n\nAlgorithm 1 Iterative Command Grounding\nInput: C: natural language command issued by user;\n\nT : ASC store;\nR: Command tagger and rephraser;\nM: ASC Matcher;\n\nOutput: A\u0302: predicted AID set for grounding C;\n\n1: C\u2032 \u2190 Semantic Tagging Rephrasing(C, R)\n{C\u2032 is the tagged and rephrased user command}\n\n2: SP \u2190 Enumerate Filter Subexpressions(C\u2032,\nT ,M) {SP is the list of enumerated and filtered sub-expr.\nin C\u2032}\n\n3: A\u0302 \u2190 \u2205; j \u2190 0\n4: while TRUE do\n5: Aset \u2190 GetCandidateASCs(T , C\u2032, \u201caction\u201d)\n6: if Aset = \u2205 then\n7: if SP = \u2205 or j > |SP | \u2212 1 then\n8: return \u2205\n9: else\n\n10: Uset \u2190 GetCandidateASCs(T , SPj ,\n\u201cutility\u201d) {SPj is the jth sub-expression in SP}\n\n11: if Uset 6= \u2205 then\n12: ur \u2190 GetTopRankedASC(Uset, SPj ,\n\nM)\n13: A\u0302 \u2190 A\u0302 \u222a {ur}\n14: C\u2032 \u2190 ReduceCommand(C\u2032, SPj ,\n\nur.output)\n15: SP \u2190\n\nEnumerate Filter Subexpressions\n(C\u2032, T ,M)\n\n16: j \u2190 0\n17: end if\n18: j \u2190 j + 1\n19: end if\n20: else\n21: ak \u2190 GetTopRankedASC(Aset, C\u2032,M)\n22: return A\u0302 \u222a {ak}\n23: end if\n24: end while\n\nred\u201d) and then, delete all sub-expressions involving\nsuch arguments (e.g., \u201ccolor\/X2\u201d) from SP [Line\n2].\n\nWith the resulting SP , the ASC grounding for\nC \u2032 [Lines 4-24] happens as follows:\n\u2022 First, a candidate set Aset of action ASCs is\n\nretrieved from the set of action ASCs such that\nfor any a \u2208 Aset, C \u2032 and a has an identical set of\nvariables and their types. If Aset 6= \u2205,M finds and\nreturns the top ranked action ASC ak [Lines 20-21]\nfor C \u2032. In Figure 1, none of the action ASCs are\nmatched directly for C \u20320 and so, Aset = \u2205.\n\u2022 If Aset = \u2205, MatcherM works as follows: If\n\nSP = \u2205 or all sub-expressions in SP have been\nchecked, it returns \u2205 indicating grounding of C\nis not possible [Lines 7-8]. Otherwise, it selects\nthe sub-expressions from SP one by one and then,\nreduces C \u2032 further by resolving bindings for the\nvariables present in that sub-expression [Lines 10-\n18]: Given the jth sub-expression SPj , Matcher\n\n\n\nFigure 1: An example of working of CGM on a user com-\nmand for Blocks-World. Here, AID denotes ASC ID (see\nTables 2 and 3).\n\nfirst selects a candidate set Uset of utility ASCs\nfrom T such that for any u \u2208 Uset, SPj and u\nhas identical set of variables and their types [Line\n10]. Note, while matching the variables with utility\nASCs, we rename the variables in SPj in order to\navoid error in matching due to different variable\nnames. For example, variable X3 in sub-expression\n\u201cname\/X3\u201d [see C \u20321 in Figure 1] is renamed as X1\n[i.e.\u201cname\/X1\u201d] (not shown), so that it can match\nwith AID 10, while reducing C \u20321 to C\n\n\u2032\n2. Similarly,\n\nX2 and O1 [in C \u20322 of Figure 1] in sub-expression\n\u201cdirection\/X2 of block set\/O1\u201d are renamed as X1\nand X2 [i.e., \u201cdirection\/X1 of block set\/X2\u201d] so that\nit can match with AID 12.\n\n\u2022 If Uset = \u2205, MatcherM cannot perform re-\nduction of C \u2032 for SPj and only j is incremented\n[Line 18]. Otherwise, M returns the top ranked\nutility ASC ur for SPj [Line 12]. Next, C \u2032 is re-\nduced by replacing SPj with \u201dtype(O1)\/O1\u201d [the\noutput variable and its type corresponding to ur]\n(Line 14). For example, given SPj =\u201cname\/X3\u201d\n[see C \u20321 in Figure 1], utility AID 10 gets matched\nand C \u20321 gets reduced to C\n\n\u2032\n2, where \u201cO1\u201d is the out-\n\nput variable and type(O1)= \u201cblock set\u201d [i.e., set of\nblock ids]. As a part of reduction, the variables of\nC \u2032 obtained after replacement are also renamed [i.e.\nfrom block set\/O1 in C \u20322 in Figure 1 to block set\/X3\n(not shown)] and aliases are recorded, so that in\nthe next iteration, it can be matched with the ac-\ntion ASCs [in Lines 5 and 21]. After reduction,\nwe again enumerate and filter SP using the new\n(reduced) C \u2032 and set j to 0 (Line 15-16) similar to\nLine 2. CGM also stores the values of the output\nvariables in a buffer obtained by executing the func-\ntions of the matched utility ASCs for subsequent\ngrounding. In Figure 1, C \u20323 matches with action\nAPI of AID 3 and the iterative grounding completes\nhere. The final grounded AID list returned by CGM\nfor the example in Figure 1 is [8, 10, 12, 3].\n\n3.5 ASC Learner of CML\n\nUsually the developer can provide only a small set\nof action ASCs for each action API. But the users\nmay come up with many different paraphrased\ncommands for which CML cannot find significant\nmatch in the current ASC set provided by the de-\nveloper, which results in low recall. To deal with\nthis issue, we enable CML to learn new ASCs from\nusers through interactions, as discussed below.\n\nGiven a user command C and CML has\ngrounded C into some action ASC\/API or the\ngrounding has failed (i.e., Line 8 in Algorithm-1 is\nexecuted), it asks user to verify whether the ground-\ning output is correct or not. In this step, the verifica-\ntion question asked to the user is formulated based\non fixed natural language (NL) templates (e.g. \u201cAm\nI correct?[yes\/No]\u201d or \u201cDo you agree?[yes\/No]\u201d)\nand expects a yes\/no answer from the user. If the\nuser says \u201cyes\u201d or remains silent, the system as-\nsumes its prediction is correct and requests next\ncommand to be grounded. If the user says \u201cNo\u201d\n(i.e., prediction is incorrect), CML shows a top-k\nlist of action ASCs (ranked based on match score\nby M) in natural language (NL, for easy under-\nstanding) one for each action API for the user to\nchoose the correct one from (see below). As the\nnumber of action APIs in our application is small,\nwe choose k to be the total number of action APIs\nin the experiments. Thus, the user can choose the\ncorrect ASC by scrolling down the list of k options\nin one go.\n\nThe ranked list of action ASCs in NL are gen-\nerated as follows. Given a user command C [say,\n\u201cput a block to the left of A\u201d], CML tags C with\nR and iteratively reduces the command to get a\nfinal reduced command C \u2032 [in this case, \u201cput a\nblock to location\/X1\u201d, where X1=(2, 3) is detected\nwhile resolving the reference \u201cleft of A\u201d using util-\nity ASCs]. Given the final reduced command C \u2032,\nCML replaces the variables in C \u2032 with the corre-\nsponding value to get a NL command equivalent\nto the original user command C. In the example,\nthe reduced NL command will be \u201cput a block to\n(2,3)\u201d. Such a NL command is generated for each\naction ASC, provided there is at least one match in\nargument values.\n\nIf the user chooses one from the ranked list, the\nlearner asks for verifying the correctness of the de-\ntected argument values. If the user confirms, ASC\nlearner gathers ground truth action API along with\na new ASC and add it into the action ASC set. If\n\n\n\nTable 4: Dataset statistics. Here, m-UC denotes the number\nof user commands that needs m utility ASCs for ground-\ning. \u201dNon-Groundable\u201d denotes the number of commands for\nwhich no grounding exists for the given API set.\n\n0-UC 1-UC 2-UC > 2-UC\nNon-\n\nGroundable\nTotal\n\nBlocks-World\n15 160 20 76 42 313\n\nWebpage Design\n13 146 13 20 14 206\n\nthe user thinks none of the commands are correct or\nthe intended action is not there in the rank list, user\nenters a rephrased\/rectified version of the command\n(in a text box), which is again utilized to generate a\nnew rank list of action ASCs. This process repeats\nuntil the user chooses an option from the rank list to\nprovide the ground truth or maximum m attempts\nare made (in which case, the learner learns no new\nASC for the current interaction process).\n\n4 Experiments\n\nWe evaluate CML on two representative applica-\ntions: (1) Block-World and (2) Webpage Design.2\n\nTo create the test data for each application, we\nshowed the supported API functions of each ap-\nplication to five users (graduate students, who are\nunaware of the working of CML) and asked them\nto write commands to play with the application\nto gather the evaluation data (Table 4). We also\nasked them to write down some commands that are\nnot groundable to any of the action APIs. We ran-\ndomly shuffle the list of commands and run CML to\nground them one by one. Next, we asked the same\nusers to mark the correctness of the grounding re-\nsults. Based on this, we compute the accuracy of\nvarious versions of CML. In computing accuracy,\nwe consider the true action API of non-groundable\nuser commands as AID 0, indicating that it cannot\nbe grounded to any of the action ASCs (or APIs)\npresent in the specification. If CML can detect it\nis non-groundable, it is considered correct. ASC\nspecifications for the Blocks-World applications\nwere given in Tables 2 and 3. The ASC specifica-\ntions for Webpage Design are given in the Supple-\nmentary material. We will release the code and the\ndatasets after paper acceptance.\n\n2In terms of user commands, many applications are similar\nto our experimental applications, e.g., building graphical user\ninterfaces, robot navigation, drone control, etc. This initial\nwork does not handle compositions of actions which is re-\nquired by some complex tasks such as database queries. We\nplan to do it next.\n\n4.1 Compared Models\n\nAs there is no existing work using the NL2NL ap-\nproach for NLI design, we compare various ver-\nsions of the CML model for evaluation. Note that\nwe don\u2019t compare with existing parsing and\/or end-\nto-end methods as they need application training\ndata and\/or are thus not application independent.\n\n(1) CML-jaccard : This version of CML uses\nthe Jaccard similarity as the scoring function of\nMatcherM.\n\n(2) CML-vsm : This version uses tf-idf based\nvector space model forM where all ASCs associ-\nated with each API are regarded as one document\nand the user command as query. It is slightly poor\nif ASCs are treated individually.\n\n(3) CML-emb: This version uses word embed-\nding based matching model forM. Given an ASC\nt and a user command (after tagging) C \u2032, we re-\ntrieve the pre-trained word embedding vectors for\neach word in C \u2032 (t) and average them to get the\nvector representation of C \u2032 (t) as vI\u2032 (vt). Next,\nwe use cosine similarity as the scoring function to\nmeasure the similarity between vI\u2032 and vt. We use\n50D Glove embeddings for evaluation.\n\n(4) CML-vsm (-R): Variant of CML-vsm,\nwhere the rephrasing of words in the input com-\nmand [Line 1, algorithm 1] is disabled.\n\n(5) CML-vsm (-U): Variant of CML-vsm,\nwhere the use of utility ASCs while grounding [i.e.,\nLines 7-19, algorithm 1] is disabled.\n\nFor (4) and (5), although we only discuss CML-\nvsm variant in next subsection, we also compared\nthese variants for CML-emb and CML-jaccard as\nwell and found poorer results. Note also for all\nCML variants mentioned above, we disabled the\nuse of ASC Learner for learning new ASCs.\n\n(6) CML-Y + ASC Learner: This is the ver-\nsion of CML-Y (where Y is jaccard, vsm, or emb),\nwhere we allow CML-Y to learn new ASCs through\nuser interactions. Since the emb version is rela-\ntively poorer (discussed next), we chose to compare\nthe jaccard and vsm variants of ASC Learner.\n\n4.2 Results and Analysis\n\nTable 5 shows the accuracy comparison of CML\nvariants on Blocks-World and Webpage Design.\nCML-jaccard and CML-vsm perform better over-\nall. The drop in performance for CML-emb in\nWebpage Design is primarily due to the out of\nvocabulary words (no pre-trained embedding is\navailable) in user commands. The performance\n\n\n\nTable 5: Overall accuracy comparison of CML variants\n\nCompared Models Blocks-World Webpage Design\nCML-jaccard 67.73 80.58\nCML-vsm 67.41 81.06\nCML-emb 67.73 77.18\nCML-vsm (-R) 58.86 67.82\nCML-vsm (-U) 15.97 13.10\nCML-jaccard +\nASC Learner 68.69 83.98\nCML-vsm +\nASC Learner 67.73 83.01\n\nof CML-vsm(-R) drops significantly, which shows\nthat rephrasing helps substantially in command re-\nduction and grounding. CML-vsm(-U) performs\nthe worst among all variants which shows the im-\nportance of user command reduction using utility\nASCs.\n\nAs CML-jaccard and CML-vsm perform the best\noverall for both applications, we also compare the\nperformance of ASC Learning variants of these\ntwo CML versions in Table 5. We can see that\nASC learning clearly improves the performance for\nboth applications. It is very important to note that\nthese improvements are gained from the existing\ndatasets, which do not have many similar com-\nmands to the newly learned ASCs. In practice, if\nsimilar commands are repeated by many users, the\nimprovement will grow substantially. The perfor-\nmance improvement for Webpage Design is more\nthan for Blocks-World, which can be explained\nas follows. For Blocks-World, the arguments and\ntheir types in API and ASC specifications (see Ta-\nbles 2 and 3) are quite distinguishable from each\nother. Thus, correctly identifying arguments and\ntheir values in user commands plays a major role\nin the success of command grounding. Learning\nof new action ASCs does not make significant im-\npact here. But, for Webpage Design specifications\n(see Supplementary), action ASCs for APIs 8, 9\nand 10 have exactly the same arguments and types,\nbut they differ significantly in action intents. Thus,\nlearning new ASCs helps greatly.\n\nTo investigate the effect of ASC learning further,\nwe evaluate CML-jaccard and CML-jaccard + ASC\nLearner (CML-vsm and CML-vsm + ASC Learner)\non user commands that are only groundable to any\nof the action APIs with AIDs 8, 9 and 10, as shown\nin Table 6. Here, we observe almost 8% improve-\nment in accuracy for ASC Leaner variants of CML,\nwhich justifies the explanation.\n\nIn Table 7, we compare CML-vsm over various\ncommand types (listed in Table 4). Here, we see\n\nTable 6: Accuracy improvement of CML ASC Learner vari-\nants on user commands groundable to action APIs with AIDs\n8, 9 and 10 in Webpage Design (see Supplementary)\n\nCompared Models Webpage Design\nCML-jaccard 82.92\nCML-vsm 78.04\nCML-jaccard + ASC Learner 90.24\nCML-vsm + ASC Learner 85.36\n\nTable 7: Accuracy for various command categories. Here,\nNOG denotes \u201cNon-groundable\u201d.\n\nApplication 0-UC 1-UC 2-UC >2-UC NOG\nBlocks-World 53.33 85.0 40.0 59.21 33.33\nWebpage\nDesign\n\n100 84.93 69.23 55.00 71.42\n\nthat, for 0-UC and 1-UC, CML-vsm performs sig-\nnificantly better than for 2-UC, >2-UC and NOG\n(note, for NOG user commands, the true AID is\nconsidered as 0) as these commands are harder to\nground due to the requirement of multiple (recur-\nsive) reduction steps using utility ASCs.\n\nError Analysis. Overall, we can see that CML\nvariants perform better for Webpage Design than\nfor Blocks-World. On analyzing the datasets, we\nfound that due to the flexibility of the Blocks-\nWorld application, the user commands for it are\nmore ambiguous and less specific compared to\nthose in Webpage Design, where users use more\napplication-specific terminologies, which helps\ngrounding. Apart from that, we identified some\ncommon grounding errors: (1) Argument detec-\ntion and rephrasing: failures of CML in detecting\nargument values in user command or rephrasing\ndue to lack of semantic knowledge in R. E.g.,\ngiven the command \u201cget the ring shaped block\nout\u201d, CML cannot map the phrase \u2018get out\u2019 to \u2018re-\nmove\u2019 and \u2018ring shaped\u2019 to shape \u2019circular\u2019, which\ncause failure in grounding. (2) Ambiguous seman-\ntics of words: In user command \u201cwrite down A on\nred cube\u201d, word \u201cdown\u201d is tagged as \u201cdirection\u201d,\nwhereas \u201cwrite down\u201d is a single phrase referring\nto \u201crename\u201d action. (3) Wrong ASC matching: In\nuser command \u201cmove the green block to first row\u201d,\nfirst row is not a coordinate and thus not ground-\nable. However, due to a small similarity with ac-\ntion ASC of AID 2 [see Table 2], the command is\nwrongly reduced to that. (4) implicitly parameter-\nized language expressions: Considering the user\ncommand \u201cEnlarge paragraph 1\u201d, the phrase \u201cen-\nlarge\u201d is implicitly parameterized, which is equiv-\nalent to \u201cset font size to large\u201d and is difficult to\nground to action ASC with AID 7 for Webpage\n\n\n\nDesign (see Supplementary).\n\n5 Conclusion\n\nThis paper proposed an natural language to natural\nlanguage (NL2NL) approach to building natural\nlanguage interfaces and a system CML, which are\nvery different from traditional approaches. Due to\nthe NL2NL approach, CML is application indepen-\ndent except that the ASCs (API seed commands)\nneed to be specified by the application developer.\nOur evaluation using two applications show that\nCML is effective and highly promising. In our fu-\nture work, apart from improving CML\u2019s accuracy,\nwe will study how the composition of actions can\nbe handled as well for more complex applications\n(e.g., database querying) and how to learn referen-\ntial expressions through dialogues.\n\nAcknowledgements\n\nThis work was partially supported by a research\ngift from Northrop Grumman Corporation.\n\nReferences\nJacob Andreas and Dan Klein. 2015. Alignment-based\n\ncompositional semantics for instruction following.\narXiv preprint.\n\nYoav Artzi and Luke Zettlemoyer. 2013. Weakly su-\npervised learning of semantic parsers for mapping\ninstructions to actions. TACL.\n\nChristopher Baik, HV Jagadish, and Yunyao Li. 2019.\nBridging the semantic gap with sql query logs in nat-\nural language interfaces to databases. In ICDE.\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In EMNLP.\n\nSRK Branavan, Luke S Zettlemoyer, and Regina Barzi-\nlay. 2010. Reading between the lines: Learning to\nmap high-level instructions to commands. In ACL.\n\nAsli Celikyilmaz, Zhaleh Feizollahi, Dilek Hakkani-\nTur, and Ruhi Sarikaya. 2014. Resolving refer-\nring expressions in conversational dialogs for natural\nuser interfaces. In EMNLP.\n\nSe\u0301bastien Ferre\u0301. 2017. Sparklis: an expressive query\nbuilder for sparql endpoints with guidance in natural\nlanguage. Semantic Web.\n\nMichael C Frank and Noah D Goodman. 2012. Pre-\ndicting pragmatic reasoning in language games. Sci-\nence.\n\nDaniel Fried, Jacob Andreas, and Dan Klein. 2017.\nUnified pragmatic models for generating and follow-\ning instructions. arXiv preprint arXiv:1711.04987.\n\nTong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu,\nand Karrie G Karahalios. 2015. Datatone: Manag-\ning ambiguity in natural language interfaces for data\nvisualization. In UIST.\n\nFrancisco J Chiyah Garcia, David A Robb, Xingkun\nLiu, Atanas Laskov, Pedro Patron, and Helen Hastie.\n2018. Explain yourself: A natural language in-\nterface for scrutable autonomous robots. arXiv\npreprint.\n\nDave Golland, Percy Liang, and Dan Klein. 2010. A\ngame-theoretic approach to generating spatial de-\nscriptions. In EMNLP.\n\nKelvin Guu, Panupong Pasupat, Evan Zheran Liu,\nand Percy Liang. 2017. From language to pro-\ngrams: Bridging reinforcement learning and maxi-\nmum marginal likelihood. arXiv preprint.\n\nMichael Janner, Karthik Narasimhan, and Regina\nBarzilay. 2018. Representation learning for\ngrounded spatial reasoning. TACL.\n\nCarolin Lawrence and Stefan Riezler. 2016. Nlmaps:\nA natural language interface to query openstreetmap.\nIn COLING.\n\nJingjing Li, Wenlu Wang, Wei-Shinn Ku, Yingtao Tian,\nand Haixun Wang. 2019. Spatialnli: A spatial do-\nmain natural language interface to databases using\nspatial comprehension. arXiv preprint.\n\nYunyao Li and Davood Rafiei. 2018. Natural Lan-\nguage Data Management and Interfaces. Morgan\n& Claypool Publishers.\n\nPercy Liang. 2016. Learning executable semantic\nparsers for natural language understanding. arXiv\npreprint.\n\nXi Victoria Lin, Chenglong Wang, Luke Zettlemoyer,\nand Michael D Ernst. 2018. Nl2bash: A corpus and\nsemantic parser for natural language interface to the\nlinux operating system. arXiv preprint.\n\nGeorge A Miller, Richard Beckwith, Christiane Fell-\nbaum, Derek Gross, and Katherine J Miller. 1990.\nIntroduction to wordnet: An on-line lexical database.\nInternational journal of lexicography.\n\nArvind Neelakantan, Quoc V Le, Martin Abadi, An-\ndrew McCallum, and Dario Amodei. 2016. Learn-\ning a natural language interface with neural program-\nmer. arXiv preprint arXiv:1611.08945.\n\nPanupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin\nGuu, and Percy Liang. 2018. Mapping natural lan-\nguage commands to web elements. arXiv preprint.\n\nPanupong Pasupat and Percy Liang. 2015. Compo-\nsitional semantic parsing on semi-structured tables.\narXiv preprint.\n\nVidya Setlur, Sarah E Battersby, Melanie Tory, Rich\nGossweiler, and Angel X Chang. 2016. Eviza: A\nnatural language interface for visual analysis. In\nUIST. ACM.\n\nhttps:\/\/doi.org\/10.2200\/S00866ED1V01Y201807DTM049\nhttps:\/\/doi.org\/10.2200\/S00866ED1V01Y201807DTM049\n\n\nNathaniel J Smith, Noah Goodman, and Michael Frank.\n2013. Learning and using language via recursive\npragmatic reasoning about other agents. In NIPS.\n\nK. Soh. 2017. Tagui: Rpa \/ cli tool for automating\nuser interactions. In https:\/\/github.com\/ kelabere-\ntiv\/TagUI.\n\nRobert Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In AAAI.\n\nArjun Srinivasan, Mira Dontcheva, Eytan Adar, and\nSeth Walker. 2019. Discovering natural language\ncommands in multimodal interfaces. In IUI.\n\nYu Su, Ahmed Hassan Awadallah, Madian Khabsa,\nPatrick Pantel, Michael Gamon, and Mark Encarna-\ncion. 2017. Building natural language interfaces to\nweb apis. In CIKM.\n\nStefanie Tellex, Thomas Kollar, Steven Dickerson,\nMatthew R Walter, Ashis Gopal Banerjee, Seth\nTeller, and Nicholas Roy. 2011. Understanding nat-\nural language commands for robotic navigation and\nmobile manipulation. In AAAI.\n\nJesse Thomason, Shiqi Zhang, Raymond J Mooney,\nand Peter Stone. 2015. Learning to interpret natu-\nral language commands through human-robot dialog.\nIn IJCAI.\n\nPrasetya Utama, Nathaniel Weir, Fuat Basik, Carsten\nBinnig, Ugur Cetintemel, Benjamin Ha\u0308ttasch, Amir\nIlkhechi, Shekar Ramaswamy, and Arif Usta. 2018.\nAn end-to-end neural natural language interface for\ndatabases. arXiv preprint arXiv:1804.00401.\n\nSida I Wang, Percy Liang, and Christopher D Manning.\n2016. Learning language games through interaction.\narXiv preprint.\n\nHongvu Xiong and Ruixiao Sun. 2019. Transfer-\nable natural language interface to structured queries\naided by adversarial generation. In ICSC. IEEE.\n\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and\nJianfeng Gao. 2015. Semantic parsing via staged\nquery graph generation: Question answering with\nknowledge base. In ACL-IJCNLP.\n\nJohn M Zelle and Raymond J Mooney. 1996. Learn-\ning to parse database queries using inductive logic\nprograming. In AAAI.\n\nLuke Zettlemoyer and Michael Collins. 2007. Online\nlearning of relaxed ccg grammars for parsing to log-\nical form. In EMNLP.\n\nLuke S Zettlemoyer and Michael Collins. 2012. Learn-\ning to map sentences to logical form: Structured\nclassification with probabilistic categorial grammars.\narXiv preprint.\n\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning.\narXiv preprint arXiv:1709.00103.\n\nTable 8: Properties and their domains for Webpage Design\n\nProperty Domain\n\ncolor (object)\n\u2019red\u2019, \u2019green\u2019, \u2019brown\u2019,\n\u2019blue\u2019, \u2019black\u2019\n\ntype (object)\n\u2019image\u2019, \u2019button\u2019, \u2019title\u2019,\n\u2019paragraph\u2019\n\nlocation (object) (x, y) coordinate in 2D space\n\nname (object)\n\nobject\u2019s type followed by a\nnumber, (like image 1, title 2\netc.) or xyz.jpeg, xyz.png,\nwhere xyz is a string\n\ntext (object) string in quote ( \u201d .... \u201d)\nfont size (object) \u2019small\u2019, \u2019medium\u2019, \u2019large\u2019\ngraphics size (object) \u2019height\u2019, \u2019width\u2019\ndirection (action) \u2019right\u2019, \u2019left\u2019, \u2019above\u2019, \u2019below\u2019\n\nA Webpage Design\n\nThe goal of the Webpage Design application is to\nprovide functions to the end user to help them de-\nsign a web page. Our current environment deals\nwith various html objects like image, title, para-\ngraph, button. Action functions (APIs) are de-\nsigned to manipulate these objects in the 2D space.\n\nA.1 Properties and Domains\n\nTable 1 shows different properties of an object or\naction (specified in the bracket next to the name of\nthe property in the property column). The domain\ncolumn in Table 1 lists the value domain of each\nproperty. E.g., \u201ccolor\u201d is a property of an html\nobject, which can take five values {\u2019red\u2019, \u2019green\u2019,\n\u2019brown\u2019, \u2019blue\u2019, \u2019black\u2019}. Similarly, type denotes\nhtml object type and can be one of four categories:\n{\u2019image\u2019, \u2019button\u2019, \u2019title\u2019, \u2019paragraph\u2019}. The text\nwritten on an element like title or paragraph is de-\nnoted by double quote (\u201c \u201d) in the command. The\nname of an element is denoted by its type followed\nby an id number (like title 1, image 2, etc.) and\/or\nsome string followed by image extension for im-\nage objects (like myphoto.jpg) in the command.\nThe location of an object is specified by an 2D\nco-ordinate (x, y) and so on. Besides these, \u2018direc-\ntion\u2019 is a application-specific property with domain\n{\u2018left\u2019, \u2018right\u2019, \u2018above\u2019, \u2018below\u2019} needed to execute\nan action for moving an object in a given direction.\n\nA.2 ASC Specification\n\nTable 2 shows the action ASC (API seed command)\nand Table 3 shows the utility ASC specifications for\nWebpage Design. Some example user commands\nor sub-expressions that can fire them are also given.\nHere, the argument type \u2018element set\u2019 denotes a\nset of unique html (instantiated) object ids. We\n\n\n\nTable 9: Action ASC specifications for Webpage Design and groundable example commands from user. (*) denotes that the\nvariable do not take part in command reduction (Utility Constraint), which is automatically identified by CML.\n\nAPI Function AID Action ASCs Variable\/Argument: Type Example User Commands\n\nAdd(X1, X2) 1\nadd X1 at\nlocation X2\n\n\u2019X1\u2019: \u2019type\u2019(*),\n\u2019X2\u2019: \u2019location\u2019(*)\n\nadd a title at (20, 30);\nadd an image at (30, 40)\n\nWrite(X1, X2) 2 write text X1 on X2\n\u2019X1\u2019: \u2019text\u2019(*),\n\u2019X2\u2019: \u2019element set\u2019\n\nwrite \u201dMy Home Page\u201d on title 1\n\nRemove(X1) 3 remove X1 \u2019X1\u2019: \u2019element set\u2019\ndelete title 1\nremove image photo.png\n\nMove(X1, X2) 4\nmove X1 to\nlocation X2\n\n\u2019X1\u2019: \u2019element set\u2019,\n\u2019X2\u2019: \u2019location\u2019(*)\n\nmove title 1 to (20, 30)\n\nMoveByUnits(X1,\nX2, X3)\n\n5\nmove X1\nalong X2 by X3\nunits\n\n\u2019X1\u2019: \u2019element set\u2019,\n\u2019X2\u2019: \u2019direction\u2019,\n\u2019X3\u2019: \u2019number\u2019\n\nmove image 1 left by 10 units\n\nUpdateColor(X1, X2) 6\nset color of\nX1 to X2\n\n\u2019X1\u2019: \u2019element set\u2019,\n\u2019X2\u2019: \u2019color\u2019(*)\n\ncolor paragraph 1 as red;\nchange color of title 1 to blue\n\nUpdateFont(X1, X2) 7\nset font size of\nX1 to X2\n\n\u2019X1\u2019: \u2019element set\u2019,\n\u2019X2\u2019: \u2019font size\u2019(*)\n\nmake title 1 large\n\nSetGraphicsSize(X1,\nX2, X3)\n\n8\nset the X1 of X2\nto X3\n\n\u2019X1\u2019: \u2019graphics size\u2019(*),\n\u2019X2\u2019: \u2019element set\u2019,\n\u2019X3\u2019: \u2019number\u2019(*)\n\nset the height of image 1 to 30\nset the width of paragraph 1\nto 40\n\nIncreaseSize(X1, X2, X3) 9\nincrease the X1 of\nX2 by X3 units\n\n\u2019X1\u2019: \u2019graphics size\u2019(*),\n\u2019X2\u2019: \u2019element set\u2019,\n\u2019X3\u2019: \u2019number\u2019(*)\n\nincrease the height of image 1\nby 10 units\n\nDecreaseSize(X1, X2, X3) 10\ndecrease the X1 of\nX2 by X3 units\n\n\u2019X1\u2019: \u2019graphics size\u2019(*),\n\u2019X2\u2019: \u2019element set\u2019,\n\u2019X3\u2019: \u2019number\u2019(*)\n\nreduce the width of image 1\nby 5 units\n\nTable 10: Utility ASC specifications for Webpage Design and groundable example command sub-expressions (O for output).\n\nFunction AID Utility ASCs Argument: Type Example sub-expressions\n\nGetElementbyLocation(X1) 11 location\/X1\nX1: \u2019location\u2019,\nO1: \u2019element set\u2019\n\nelement at (20, 30)\n\nGetElementbyType(X1) 12 X1\nX1: \u2019type\u2019,\nO1: \u2019element set\u2019\n\nget all titles;\nget all paragraphs\n\nGetElementbyFont(X1) 13 X1\nX1: \u2019font size\u2019,\nO1: \u2019element set\u2019\n\nelements having\nsize large\n\nGetElementby\nGraphicsSize(X1, X2)\n\n14 X1 of X2\nX1: \u2019graphics size\u2019,\nX2: \u2019number\u2019,\nO1: \u2019element set\u2019\n\nelements having\nheight of 20\n\nGetElementbyColor(X1) 15 X1\nX1: \u2019color\u2019,\nO1: \u2019element set\u2019\n\nred element;\nblue title\n\nGetElementbyText(X1) 16 X1\nX1: \u2019text\u2019,\nO1: \u2019element set\u2019\n\nelement with text\n\u201dwelcome!\u201d\n\nGetLocation(X1, X2) 17\nget location\nalong X1 of X2\n\nX1: \u2019direction\u2019,\nX2: \u2019element set\u2019,\nO1: \u2019location\u2019\n\nlocation at the left of\nimage 2; location\n\nbelow title 1\n\nGetElementbyName(X1) 18 X1\nX1: \u2019name\u2019,\nO1: \u2019element set\u2019\n\ntitle 1; image 2;\nprofile.jpeg\n\ndefine 10 action ASCs and 8 utility ASCs (for their\nrespective APIs) to ground a user command. The\naction ASCs are: adding an html object at location\n(x,y) [AID 1], writing some text on an element like\ntitle or paragraph [AID 2], removing an element\n[AID 3], moving elements [AID 4-5], changing\nproperties of an object [AID 6-10]. Utility ASCs\neither retrieve the element set having a given prop-\n\nerty and value [AID 11-16, 18] or return a location\nobject (x, y) in a given direction of an object [AID\n17].\n\n\n","13":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActiveEA: Active Learning for Neural Entity Alignment\n\nBing Liu1,\ufffd, Harrisen Scells1, Guido Zuccon1, Wen Hua1, Genghong Zhao2\n1The University of Queensland, Australia\n\n2Neusoft Research of Intelligent Healthcare Technology, Co. Ltd., China\n{bing.liu, h.scells, g.zuccon, w.hua}@uq.edu.au\n\nzhaogenghong@neusoft.com\n\nAbstract\n\nEntity Alignment (EA) aims to match equival-\nent entities across different Knowledge Graphs\n(KGs) and is an essential step of KG fu-\nsion. Current mainstream methods \u2013 neural\nEA models \u2013 rely on training with seed align-\nment, i.e., a set of pre-aligned entity pairs\nwhich are very costly to annotate. In this paper,\nwe devise a novel Active Learning (AL) frame-\nwork for neural EA, aiming to create highly\ninformative seed alignment to obtain more ef-\nfective EA models with less annotation cost.\nOur framework tackles two main challenges\nencountered when applying AL to EA:\n\n(1) How to exploit dependencies between entit-\nies within the AL strategy. Most AL strategies\nassume that the data instances to sample are\nindependent and identically distributed. How-\never, entities in KGs are related. To address\nthis challenge, we propose a structure-aware\nuncertainty sampling strategy that can measure\nthe uncertainty of each entity as well as its im-\npact on its neighbour entities in the KG.\n\n(2) How to recognise entities that appear in\none KG but not in the other KG (i.e., bachel-\nors). Identifying bachelors would likely save\nannotation budget. To address this challenge,\nwe devise a bachelor recognizer paying atten-\ntion to alleviate the effect of sampling bias.\n\nEmpirical results show that our proposed AL\nstrategy can significantly improve sampling\nquality with good generality across different\ndatasets, EA models and amount of bachelors.\n\n1 Introduction\n\nKnowledge Graphs (KGs) store entities and their\nrelationships with a graph structure and are used as\nknowledge drivers in many applications (Ji et al.,\n2020). Existing KGs are often incomplete but com-\nplementary to each other. A popular approach used\nto tackle this problem is KG fusion, which attempts\nto combine several KGs into a single, comprehens-\nive one. Entity Alignment (EA) is an essential\n\nRepublican\nParty\n\nDonald\nTrump\n\nNew York\nCity\n\n14 June,\n1946\n\nUS\n\nBirthDate\n\nBornIn\n\nMemberOf\n\nPresidentOf\n\nTrump\nTower\n\nD.J.\nTrump\n\nThe Trump\nOrganization\n\nAmerica\n\nWealth\n\nOwnerOf\n\nOwnerOf\n\nPresidentOf\n\n2.5 Billion\nUSD\n\nPolitical knowledge Business knowledge\n\nFigure 1: An example of Entity Alignment.\n\nstep for KG fusion: it identifies equivalent entities\nacross different KGs, supporting the unification of\ntheir complementary knowledge. For example, in\nFig. 1 Donald Trump and US in the first KG cor-\nrespond to D.J. Trump and America respectively in\nthe second KG. By aligning them, the political and\nbusiness knowledge about Donald Trump can be\nintegrated within one KG.\n\nNeural models (Chen et al., 2017, 2018; Wang\net al., 2018; Cao et al., 2019) are the current state-\nof-the-art in EA and are capable of matching en-\ntities in an end-to-end manner. Typically, these\nneural EA models rely on a seed alignment as train-\ning data which is very labour-intensive to annotate.\nHowever, previous EA research has assumed the\navailability of such seed alignment and ignored the\ncost involved with their annotation. In this paper,\nwe seek to reduce the cost of annotating seed align-\nment data, by investigating methods capable of\nselecting the most informative entities for labelling\nso as to obtain the best EA model with the least\nannotation cost: we do so using Active Learning.\nActive Learning (AL) (Aggarwal et al., 2014) is a\nMachine Learning (ML) paradigm where the an-\nnotation of data and the training of a model are per-\nformed iteratively so that the sampled data is highly\ninformative for training the model. Though many\ngeneral AL strategies have been proposed (Settles,\n2012; Ren et al., 2020), there are some unique chal-\nlenges in applying AL to EA.\n\nar\nX\n\niv\n:2\n\n11\n0.\n\n06\n47\n\n4v\n1 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n3 \nO\n\nct\n 2\n\n02\n1\n\n\n\nThe first challenge is how to exploit the de-\npendencies between entities. In the EA task,\nneighbouring entities (context) in the KGs natur-\nally affect each other. For example, in the two\nKGs of Fig. 1, we can infer US corresponds to\nAmerica if we already know that Donald Trump\nand D.J. Trump refer to the same person: this is\nbecause a single person can only be the presid-\nent of one country. Therefore, when we estimate\nthe value of annotating an entity, we should con-\nsider its impact on its context in the KG. Most AL\nstrategies assume data instances are independent,\nidentically distributed and cannot capture depend-\nencies between entities (Aggarwal et al., 2014). In\naddition, neural EA models exploit the structure\nof KGs in different and implicit ways (Sun et al.,\n2020b). It is not easy to find a general way of\nmeasuring the effect of entities on others.\n\nThe second challenge is how to recognize the\nentities in a KG that do not have a counterpart\nin the other KG (i.e., bachelors). In the first KG\nof Fig. 1, Donald Trump and US are matchable\nentities while New York City and Republican Party\nare bachelors. Selecting bachelors to annotate will\nnot lead to any aligned entity pair. The impacts of\nrecognizing bachelors are twofold:\n\n1. From the perspective of data annotation, recog-\nnizing bachelors would automatically save an-\nnotation budget (because annotators will try to\nseek a corresponding entity for some time be-\nfore giving up) and allow annotators to put their\neffort in labelling matchable entities. This is\nparticularly important for the existing neural EA\nmodels, which only consider matchable entities\nfor training: thus selecting bachelors in these\ncases is a waste of annotation budget.\n\n2. From the perspective of EA, bachelor recogni-\ntion remedies the limitation of existing EA mod-\nels that assume all entities to align are match-\nable, and would enable them to be better used\nin practice (i.e., real-life KGs where bachelors\nare popular).\n\nTo address these challenges, we propose a novel\nAL framework for EA. Our framework follows the\ntypical AL process: entities are sampled iteratively,\nand in each iteration a batch of entities with the\nhighest acquisition scores are selected. Our novel\nacquisition function consists of two components:\na structure-aware uncertainty measurement mod-\nule and a bachelor recognizer. The structure-aware\nuncertainty can reflect the uncertainty of a single\n\nentity as well as the influence of that entity in the\ncontext of the KG, i.e., how many uncertainties it\ncan help its neighbours eliminate. In addition, we\ndesign a bachelor recognizer, based on Graph Con-\nvolutional Networks (GCNs). Because the bach-\nelor recognizer is trained with the sampled data\nand used to predict the remaining data, it may suf-\nfer from bias (w.r.t. the preference of sampling\nstrategy) of these two groups of data. We apply\nmodel ensembling to alleviate this problem.\n\nOur major contributions in this paper are:\n1. A novel AL framework for neural EA, which\n\ncan produce more informative data for training\nEA models while reducing the labour cost in-\nvolved in annotation. To our knowledge, this is\nthe first AL framework for neural EA.\n\n2. A structure-aware uncertainty sampling strategy,\nwhich models uncertainty sampling and the re-\nlation between entities in a single AL strategy.\n\n3. An investigation of bachelor recognition, which\ncan reduce the cost of data annotation and rem-\nedy the defect of existing EA models.\n\n4. Extensive experimental results that show our\nproposed AL strategy can significantly improve\nthe quality of data sampling and has good gener-\nality across different datasets, EA models, and\nbachelor quantities.\n\n2 Background\n2.1 Entity Alignment\n\nEntity alignment is typically performed between\ntwo KGs G1 and G2, whose entity sets are denoted\nas E1 and E2 respectively. The goal of EA is to\nfind the equivalent entity pairs A = {(e1, e2) \u2208\nE1\u00d7E2|e1 \u223c e2}, where\u223c denotes an equivalence\nrelationship and is usually assumed to be a one-to-\none mapping. In supervised and semi-supervised\nmodels, a subset of the alignment Aseed \u2282 A,\ncalled seed alignment, are annotated manually be-\nforehand and used as training data. The remaining\nalignment form the test set Atest = A \\ Aseed.\nThe core of an EA model F is a scoring function\nF (e1, e2), which takes two entities as input and\nreturns a score for how likely they match. The\neffectiveness of an EA model is essentially determ-\nined by Aseed and we thus denote it as m(Aseed).\n\n2.2 Active Learning\n\nAn AL framework consists of two components: (1)\nan oracle (annotation expert), which provides la-\nbels for the queries (data instances to label), and\n\n\n\nOracle\/Experts\n\nBachelor recognizer\n!!\n\n!\"\n\nQuery system\n\n\u00d7\n\nSelected\nentities\n\nNew\nannotations\n\nTraining\ndata\n\nPool \" Structure-aware\nuncertainty !#$\n\nLabelled data \u2112 !!\nEA\n\nmodel \"\n\nFigure 2: Overview of ActiveEA.\n\n(2) a query system, which selects the most inform-\native data instances as queries. In pool-based scen-\nario, there is a pool of unlabelled data U . Given a\nbudget B, some instances U\u03c0,B are selected from\nthe pool following a strategy \u03c0 and sent to the ex-\nperts to annotate, who produce a training set L\u03c0,B .\nWe train the model on L\u03c0,B and the effectiveness\nm(L\u03c0,B) of the obtained model reflects how good\nthe strategy \u03c0 is. The goal is to design an optimal\nstrategy \u03c0\u2217 such that \u03c0\u2217 = argmax\u03c0m(L\u03c0,B).\n\n3 ActiveEA: Active Entity Alignment\n3.1 Problem Definition\n\nGiven two KGs G1, G2 with entity sets E1, E2,\nan EA model F , a budget B, the AL strategy \u03c0 is\napplied to select a set of entities U\u03c0,B so that the an-\nnotators label the counterpart entities to obtain the\nlabelled data L\u03c0,B . L\u03c0,B consists of annotations\nof matchable entities L+\u03c0,B , which form the seed\nalignment Aseed\u03c0,B , and bachelors L\n\n\u2212\n\u03c0,B . We measure\n\nthe effectiveness m(Aseed\u03c0,B ) of the AL strategy \u03c0\nby training the EA model on Aseed\u03c0,B and then eval-\nuating it with Atest\u03c0,B = A \\ A\n\nseed\n\u03c0,B . Our goal is to\n\ndesign an optimal entity sampling strategy \u03c0\u2217 so\nthat \u03c0\u2217 = argmax\u03c0m(Aseed\u03c0,B ).\n\nIn our annotation setting, we select entities from\none KG and then let the annotators identify their\ncounterparts from the other KG. Under this set-\nting, we assume the pool of unlabelled entities\nis initialized with U = E1. The labelled data\nwill be like L+\u03c0,B = {(e\n\n1 \u2208 E1, e2 \u2208 E2)} and\nL\u2212\u03c0,B = {(e\n\n1 \u2208 E1, null)}.\n\n3.2 Framework Overview\n\nThe whole annotation process, as shown in Fig. 2,\nis carried out iteratively. In each iteration, the query\nsystem selects N entities from U and sends them\nto the annotators. The query system includes (1) a\nstructure-aware uncertainty measurement module\nfsu, which combines uncertainty sampling with\nthe structure information of the KGs, and (2) a\n\nbachelor recognizer f b, which helps avoid selecting\nbachelor entities. The final acquisition f\u03c0 used\nto select which entities to annotate is obtained by\ncombining the outputs of these two modules. After\nthe annotators assign the ground-truth counterparts\nto the selected entities, the new annotations are\nadded to the labelled data L. With the updated L,\nthe query system updates the EA model and the\nbachelor recognizer. This process repeats until no\nbudget remains. To simplify the presentation, we\nomit the sampling iteration when explaining the\ndetails.\n\n3.3 Structure-aware Uncertainty Sampling\nWe define the influence of an entity on its con-\ntext as the amount of uncertainties it can help its\nneighbours remove. As such, we formulate the\nstructure-aware uncertainty fsu as\n\nf su(e1i ) = \u03b1\n\u2211\n\ne1i\u2192e\n1\nj ,e\n\n1\nj\u2208N\n\nout\ni\n\nwijf\nsu(e1j )\n\n+ (1\u2212 \u03b1)\nfu(e1i )\u2211\n\ne1\u2208E1 f\nu(e1)\n\n,\n\n(1)\n\nwhere N outi is the outbound neighbours of entity\ne1i (i.e. the entities referred to by e\n\n1\ni ) and wij meas-\n\nures the extent to which e1i can help e\n1\nj eliminate\n\nuncertainty. The parameter \u03b1 controls the trade-\noff between the impact of entity e1i on its context\n(first term in the equation) and the normalized un-\ncertainty (second item). Function fu(e1) refers to\nthe margin-based uncertainty of an entity. For each\nentity e1, the EA model can return the matching\nscores F (e1, e2) with all unaligned entities e2 in\nG2. Since these scores in existing works are not\nprobabilities, we exploit the margin-based uncer-\ntainty measure for convenience, outlined in Eq. 2:\n\nfu(e1) = \u2212\n(\nF (e1, e2\u2217)\u2212 F (e\n\n1, e2\u2217\u2217)\n)\n\n(2)\n\nwhere F (e1, e2\u2217) and F (e\n1, e2\u2217\u2217) are the highest and\n\nsecond highest matching scores respectively. A\nlarge margin represents a small uncertainty.\n\nFor each entity e1j , we assume its inbound neigh-\nbours can help it clear all uncertainty. Then, we\nhave\n\n\u2211\ne1i\u2192e\n\n1\nj ,e\n\n1\ni\u2208N\n\nin\nj\nwij = 1, whereN inj is the in-\n\nbound neighbour set of e1j . In this work, we assume\nall inbound neighbours have the same impact on\ne1j . In this case, wij =\n\n1\ndegree(e1j )\n\n, where degree(\u00b7)\nreturns the in-degree of an entity.\n\nUsing matrix notion, Eq. 1 can be rewritten as\n\nf su = \u03b1Wf su + (1\u2212 \u03b1)\nfu\n\n|fu|\n\n\n\nwhere f su is the vector of structure-aware uncer-\ntainties, fu is the vector of uncertainties, and W is\na matrix encoding influence between entities, i.e.,\nwij > 0 if e1i is linked to e\n\n1\nj , otherwise 0.\n\nAs W is a stochastic matrix (Gagniuc, 2017), we\nsolve Eq. 1 iteratively, which can be viewed as the\npower iteration method (Franceschet, 2011), sim-\nilar to Pagerank (Brin and Page, 1998). Specific-\nally, we initialize the structure-aware uncertainty\nvector as f su0 = f\n\nu. Then we update f sut iteratively:\n\nf sut = \u03b1Wf\nsu\nt\u22121 + (1\u2212 \u03b1)\n\nfu\n\n|fu|\n, t = 1, 2, 3, ...\n\nThe computation ends when |f sut \u2212 f sut\u22121| < \ufffd.\n\n3.4 Bachelor Recognizer\n\nThe bachelor recognizer is formulated as a binary\nclassifier, which is trained with the labelled data\nand used to predict the unlabelled data. One chal-\nlenge faced here is the bias between the labelled\ndata and the unlabelled data caused by the sampling\nstrategy (since it is not random sampling). We alle-\nviate this issue with a model ensemble.\n\n3.4.1 Model Structure\nWe apply two GCNs (Kipf and Welling, 2017;\nHamilton et al., 2017) as the encoders to get the\nentity embeddings H1 = GCN1(G1),H2 =\nGCN2(G2), where each row in H1 or H2 cor-\nresponds to a vector representation of a particular\nentity. The two GCN encoders share the same struc-\nture but have separate parameters. With each GCN\nencoder, each entity ei is first assigned a vector\nrepresentation h(0)i . Then contextual features of\neach entity are extracted:\n\nh\n(l)\ni = norm(\u03c3(\n\n\u2211\nj\u2208Ni\u222a{i}\n\nV(l)h\n(l\u22121)\nj + b\n\n(l))),\n\nwhere l is the layer index, Ni is the neighbouring\nentities of entity ei, and \u03c3 is the activation function,\nnorm(\u00b7) is a normalization function, and V(l),b(l)\nare the parameters in the l-th layer. The repres-\nentations of each entity ei obtained in all GCN\nlayers are concatenated into a single representation:\nhi = concat(h\n\n(0)\ni ,h\n\n(1)\ni , ...,h\n\n(L)\ni ), where L is the\n\nnumber of GCN layers.\nAfter getting the representations of entities, we\n\ncompute the similarities of each entity in E1 with\nall entities in E2 (S = H1 \u00b7 H2T ) and obtain\nits corresponding maximum matching score as in\n\nfs(e1i ) = max(Si,:). The entity e\n1\ni whose max-\n\nimum matching score is greater than a threshold\n\u03b3 is considered to be a matchable entity as in\nf b(e1i ) = 1fs(e1i )>\u03b3\n\n, otherwise a bachelor.\n\n3.4.2 Learning\nIn each sampling iteration, we train the bachelor re-\ncognizer with existing annotated data L containing\nmatchable entities L+ and bachelors L\u2212. Further-\nmore, L is divided into a training set Lt and a\nvalidation set Lv.\n\nWe optimize the parameters, including\n{V(l),b(l)}1\u2264l\u2264L of each GCN encoder and the\nthreshold \u03b3, in two phases, sharing similar idea\nwith supervised contrastive learning (Khosla et al.,\n2020). In the first phase, we optimize the scoring\nfunction fs by minimizing the constrastive loss\nshown in Eq. 3.\n\nloss =\n\u2211\n\n(e1i ,e\n2\nj )\u2208L\n\nt,+\n\n\u2016 h1i \u2212 h\n2\nj \u2016\n\n+ \u03b2\n\u2211\n\n(e1\ni\u2032\n,e2\n\nj\u2032\n)\u2208Lt,neg\n\n[\u03bb\u2212 \u2016 h1i\u2032 \u2212 h\n2\nj\u2032 \u2016]+\n\n(3)\n\nHere, \u03b2 is a balance factor, and [\u00b7]+ is max(0, \u00b7),\nand Lt,neg is the set of negative samples gener-\nated by negative sampling (Sun et al., 2018). For\na given pre-aligned entity pair in L+, each entity\nof it is substituted for Nneg times. The distance\nof negative samples is expected to be larger than\nthe margin \u03bb. In the second phase, we freeze the\ntrained fs and optimize \u03b3 for f b. It is easy to op-\ntimize \u03b3, e.g. by simple grid search, so that f b can\nachieve the highest performance on Lv (denoted as\nq(fs, \u03b3,Lv)) using:\n\n\u03b3\u2217 = argmax\u03b3q(f\ns, \u03b3,Lv).\n\n3.4.3 Model Ensemble for Sampling Bias\nThe sampled data may be biased, since they have\nbeen preferred by the sampling strategy rather than\nselected randomly. As a result, even if the bach-\nelor recognizer is well trained with the sampled\ndata it may perform poorly on data yet to sample.\nWe apply a model ensemble to alleviate this prob-\nlem. Specifically, we divide the L into K subsets\nevenly. Then we apply K-fold cross-validation to\ntrain K scoring functions {f s1 , ..., f\n\ns\nK}, each time\n\nusing K \u2212 1 subsets as the training set and the left\nout portion as validation set. Afterwards, we search\nfor an effective \u03b3 threshold:\n\n\u03b3\u2217 = argmax\u03b3\n1\n\nK\n\n\u2211\n1\u2264k\u2264K\n\nq(fsk , \u03b3,L\nv\nk)\n\n\n\nAt inference, we ensemble by averaging the K\nscoring functions fsk to form the final scoring func-\ntion fs as in Eq. 4 and base f b on it.\n\nfs(e1i ) =\n1\n\nK\n\n\u2211\n1\u2264k\u2264K\n\nfsk(e\n1\ni ) (4)\n\n3.5 Final Acquisition Function\n\nWe combine our structure-aware uncertainty\nsampling with the bachelor recognizer to form the\nfinal acquisition function:\n\nf\u03c0(e1i ) = f\nsu(e1i )f\n\nb(e1i )\n\n4 Experimental Setup\n\n4.1 Sampling Strategies\n\nWe construct several baselines for comparison:\nrand random sampling used by existing EA works.\ndegree selects entities with high degrees.\npagerank (Brin and Page, 1998) measures the\ncentrality of entities by considering their degrees\nas well as the importance of its neighbours.\nbetweenness (Freeman, 1977) refers to the num-\nber of shortest paths passing through an entity.\nuncertainty sampling selects entities that the cur-\nrent EA model cannot predict with confidence.\nNote that in this work we measure uncertainty us-\ning Eq. 2 for fair comparison.\n\ndegree, pagerank and betweenness are purely\ntopology-based and do not consider the current EA\nmodel. On the contrary, uncertainty is fully based\non the current EA model without being able to\ncapture the structure information of KG. We com-\npare both our structure-aware uncertainty sampling\n(struct_uncert) and the full framework ActiveEA\nwith the baselines listed above. We also exam-\nine the effect of Bayesian Transformation, which\naims to make deep neural models represent uncer-\ntainty more accurately (Gal et al., 2017).\n\n4.2 EA Models\n\nWe apply our ActiveEA framework to three differ-\nent EA models, which are a representative spread\nof neural EA models and varied in KG encoding,\nconsidered information and training method (Liu\net al., 2020; Sun et al., 2018):\nBootEA (Sun et al., 2018) encodes the KGs with\nthe translation model (Bordes et al., 2013), exploits\nthe structure of KGs, and uses self-training.\nAlinet (Sun et al., 2020a) also exploits the struc-\nture of KGs but with a GCN-based KG encoder,\nand is trained in a supervised manner.\n\nRDGCN (Wu et al., 2019) trains a GCN in a su-\npervised manner, as Alinet, but it can incorporate\nentities\u2019 attributes.\nOur implementations and parameter settings of the\nmodels rely on OpenEA1 (Sun et al., 2020b).\n\n4.3 Datasets\n\nWe use three different datasets: D-W-15K V1\n(DW), EN-DE-15K V1 (ENDE), and EN-FR-100K\nV1 (ENFR), obtained from OpenEA (Sun et al.,\n2020b). Each dataset contains two KGs and equi-\nvalent entity pairs. The KGs used in these data-\nsets were sampled from real KGs, i.e. DBpe-\ndia (Lehmann et al., 2015), Wikidata (Vrandecic\nand Kr\u00f6tzsch, 2014), and YAGO (Rebele et al.,\n2016), which are widely used in EA community.\nThese datasets differ in terms of KG sources, lan-\nguages, sizes, etc. We refer the reader to Sun et al.\n(2020b) for more details.\n\nExisting work on EA assumes all entities in\nthe KGs are matchable, thus only sampling entit-\nies with counterparts when producing the datasets.\nFor investigating the influence of bachelors on AL\nstrategies, we synthetically modify the datasets by\nexcluding a portion of entities from the second KG.\n\n4.4 Evaluation Metrics\n\nWe use Hit@1 as the primary evaluation measure\nof the EA models. To get an overall evaluation of\none AL strategy across different sized budgets, we\nplot the curve of a EA model\u2019s effectiveness with\nrespect to the proportion of annotated entities, and\ncalculate the Area Under the Curve (AUC).\n\n4.5 Parameter Settings\n\nWe set \u03b1 = 0.1, \ufffd = 1e\u22126 for the structure-aware\nuncertainty. We use L = 1 GCN layer for our\nbachelor recognizer with 500 input and 400 output\ndimensions. We set K = 5 for its model ensemble\nand \u03bb = 1.5, \u03b2 = 0.1, Nneg = 10 for its training.\nThe sampling batch size is set to N = 100 for 15K\ndata and N = 1000 for 100K data.\n\n4.6 Reproducibility Details\n\nOur experiments are run on a GPU cluster. We\nallocate 50G memory and one 32GB nVidia Tesla\nV100 GPU for each job on 15K data, and 100G\nmemory for each job on 100K data. The training\nand evaluation of ActiveEA take approximately 3h\nwith Alinet on 15K data, 10h with BootEA on 15K\n\n1https:\/\/github.com\/nju-websoft\/OpenEA\n\nhttps:\/\/github.com\/nju-websoft\/OpenEA\n\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nBootEA, DW (0%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nBootEA, DW (30%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nBootEA, ENDE (0%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nBootEA, ENDE (30%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nHi\nt@\n\n1 \n(%\n\n)\n\nAlinet, DW (0%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nAlinet, DW (30%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nAlinet, ENDE (0%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n\nAlinet, ENDE (30%)\n\n0% 10% 20% 30% 40% 50%\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n\nRDGCN, DW (0%)\n\n0% 10% 20% 30% 40% 50%\n\nPercentage of Annotated Entities\n\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n\nRDGCN, DW (30%)\n\n0% 10% 20% 30% 40% 50%\n65\n\n70\n\n75\n\n80\n\n85\n\n90\n\nRDGCN, ENDE (0%)\n\n0% 10% 20% 30% 40% 50%\n65\n\n70\n\n75\n\n80\n\n85\n\n90\n\nRDGCN, ENDE (30%)\n\nrand degree pagerank betweenness uncertainty struct_uncert ActiveEA\n\nFigure 3: HIT@1 of sampling strategies for all EA models on DW and ENDE, as annotation portion increases. Top\nrow shows experiments that do not include bachelors; bottom row shows experiments that include 30% bachelors.\nActiveEA is equivalent to struct_uncert in absence of bachelors, and is thus shown only for the second row.\n\n0% 10% 20% 30% 40% 50%\n\nPercentage of Annotated Entities\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\nHi\nt@\n\n1 \n(%\n\n)\n\nAlinet, ENFR (0%)\n\n0% 10% 20% 30% 40% 50%\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\nHi\nt@\n\n1 \n(%\n\n)\n\nAlinet, ENFR (30%)\n\nrand\ndegree\n\npagerank\nbetweenness\n\nuncertainty\nstruct_uncert\n\nActiveEA\n\nFigure 4: Hit@1 for all sampling strategies on the\nAlinet EA model on ENFR. Left shows experiments\nwithout bachelors, right shows with 30% bachelors.\n\ndata, 10h with RDGCN on 15K data, and 48h with\nAlinet on 100K data. Most baseline strategies take\nless time than ActiveEA on the same dataset except\nbetweenness on 100K data, which takes more than\n48h. We apply grid search for setting \u03b1 and N\n(shown in Sec. 5.4). Hyper-parameters of the bach-\nelor recognizer are chosen by referring the settings\nof OpenEA and our manual trials. Code and data-\nsets are available at https:\/\/github.com\/\nUQ-Neusoft-Health-Data-Science\/\nActiveEA.\n\n5 Experimental Results\n\n5.1 Comparison with Baselines\n\nFig. 3 presents the overall performance of each\nstrategy with three EA models on two datasets,\neach of which we also synthetically modify to in-\nclude 30% bachelors. We also report the AUC@0.5\nvalues of these curves in Tab. 1. ActiveEA degener-\nates into struct_uncert when there is no bachelor.\n\nRandom Sampling. Random sampling usually\nperforms poorly when the annotation proportion\nis small, while it becomes more competitive when\nthe amount of annotations increases. But for most\nannotation proportions, random sampling exhibits\na large gap in performance compared to the best\nmethod. This observation highlights the need to\ninvestigate data selection for EA.\n\nTopology-based Strategies. The topology-based\nstrategies are effective when few annotations are\nprovided, e.g., < 20%. However, once annota-\ntions increase, the effectiveness of topology-based\nstrategies is often worse than random sampling.\nThis may be because these strategies suffer more\nfrom the bias between the training set and test set.\nTherefore, only considering the structural informa-\ntion of KGs has considerable drawbacks for EA.\n\nUncertainty Sampling. On the contrary, the un-\n\nhttps:\/\/github.com\/UQ-Neusoft-Health-Data-Science\/ActiveEA\nhttps:\/\/github.com\/UQ-Neusoft-Health-Data-Science\/ActiveEA\nhttps:\/\/github.com\/UQ-Neusoft-Health-Data-Science\/ActiveEA\n\n\nStrategy\nBootEA AliNet RDGCN\n\nDW\n(0%)\n\nDW\n(30%)\n\nENDE\n(0%)\n\nENDE\n(30%)\n\nDW\n(0%)\n\nDW\n(30%)\n\nENDE\n(0%)\n\nENDE\n(30%)\n\nDW\n(0%)\n\nDW\n(30%)\n\nENDE\n(0%)\n\nENDE\n(30%)\n\nrand 23.5n 17.0 28.1 21.3 19.4 16.7 26.0 23.7 25.8 25.0 41.3n 41.0\ndegree 19.5 16.0 24.0 20.0 17.1 15.2 22.2 20.5 23.3 22.9 39.1 39.4\npagerank 22.3 18.3 27.6 23.0 19.9 17.3 25.8 24.1 24.5 23.9 40.5 40.6\nbetweenness 20.5 16.3 26.1 21.1 17.8 15.6 23.7 22.3 23.2 22.7 40.2 40.3\nuncertainty 23.9 16.1 29.8 21.2 21.6 15.4 28.2 22.2 24.7 23.9 40.9n 40.5\nstruct_uncert\n\n26.3\n20.8\n\n33.6\n27.4\n\n23.1\n19.1\n\n30.6\n26.8\n\n26.5\n25.6\n\n41.9\n41.0\n\nActiveEA 26.7 31.5 25.7 32.8 28.1 42.3\n\nTable 1: Overall performance (AUC@0.5 (%)) for each sampling strategy. The highest performing strategy in each\ncolumn is indicated in bold. We run each strategy 5 times; most results for ActiveEA show statistically significant\ndifferences over other methods (paired t-test with Bonferroni correction, p < 0.05), except the few cells indicated\nby n.\n\ncertainty sampling strategy performs poorly when\nthe proportion of annotations is small but improves\nafter several annotations have been accumulated.\nOne reason for this is that neural EA models can-\nnot learn useful patterns with a small number of\nannotations. On datasets with bachelors, uncer-\ntainty sampling always performs worse than ran-\ndom sampling. Thus, it is clear that uncertainty\nsampling cannot be applied directly to EA.\n\nStructure-aware Uncertainty Sampling. Struc-\nture-aware uncertainty is effective across all an-\nnotation proportions. One reason for this is that it\ncombines the advantages of both topology-based\nstrategies and uncertainty sampling. This is essen-\ntial for AL as it is impossible to predict the amount\nof annotations required for new datasets.\n\nActiveEA. ActiveEA, which enhances structure-\naware sampling with a bachelor recognizer, greatly\nimproves EA when KGs contain bachelors.\n\n5.1.1 Generality\nThe structure-aware uncertainty sampling mostly\noutperforms the baselines, while ActiveEA per-\nforms even better in almost all cases. ActiveEA\nalso demonstrates generality across datasets, EA\nmodels, and bachelor proportions.\n\nWhen the dataset has no bachelors, our\nuncertainty-aware sampling is exceeded by uncer-\ntainty sampling in few large-budget cases. How-\never, the real-world datasets always have bachelors.\nIn this case, our structure-aware uncertainty shows\nmore obvious advantages.\n\nIn addition, the strategies are less distinguish-\nable when applied to RDGCN. The reason is that\nRDGCN exploits the name of entities for pre-\nalignment and thus all strategies achieve good per-\nformance from the start.\n\n0% 10% 20% 30% 40%\nProportion of Bachelors\n\n10\n\n15\n\n20\n\n25\n\n30\n\nAU\nC@\n\n0.\n5 \n\n(%\n)\n\nBootEA on DW\n\n0% 10% 20% 30% 40%\nProportion of Bachelors\n\n10\n\n15\n\n20\n\n25\n\n30\n\nAU\nC@\n\n0.\n5 \n\n(%\n)\n\nAlinet on DW\n\nrand\ndegree\n\npagerank\nbetweenness\n\nuncertainty\nstruct_uncert\n\nActiveEA\n\nFigure 5: Comparison demonstrating the effect of bach-\nelors (0% \u2013 40%) on the BootEA and Alinet models.\n\n1 11 21 31 41 51 61 71\nSampling iteration\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nM\nic\n\nro\n-F\n\n1\n\nBootEA on DW (30%)\n\nwith ME\nw\/o ME\n\n1 11 21 31 41 51 61 71\nSampling iteration\n\n0.4\n\n0.6\n\n0.8\n\n1.0\nM\n\nic\nro\n\n-F\n1\n\nAlinet on DW (30%)\n\nFigure 6: Comparison demonstrating the effectiveness\nof the bachelor recognizer and the effect of the model\nensemble (ME) on BootEA and Alinet.\n\nTo assess the generality across datasets of differ-\nent sizes, we evaluate the sampling strategies with\nAlinet using ENFR (100K entities), which is larger\nthan DW and ENDE (15K entities). We choose\nAlinet because it is more scalable than BootEA\nand RDGCN (Zhao et al., 2020). Fig. 4 presents\ncomparable results to the 15K datasets.\n\n5.2 Effect of Bachelors\n\nTo investigate the effect of bachelors, we removed\ndifferent amounts of entities randomly (each larger\nsample contains the subset from earlier samples)\nfrom G2 so that G1 had different percentages of\nbachelors. Fig. 5 shows the results of applying all\nstrategies to these datasets. We further make the\n\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n10\n\n15\n\n20\n\n25\n\n30\nAU\n\nC@\n0.\n\n5 \n(%\n\n)\nstruct_uncert on DW(0%)\n\n0.0 0.2 0.4 0.6 0.8 1.0\n10\n\n15\n\n20\n\n25\n\n30\nActiveEA on DW(30%)\n\nAlinet BootEA\n\n200 400 600 800 1000\nN\n\n20\n\n22\n\n24\n\n26\n\n28\n\n30\n\nAU\nC@\n\n0.\n5 \n\n(%\n)\n\n200 400 600 800 1000\nN\n\n20\n\n22\n\n24\n\n26\n\n28\n\n30\n\nFigure 7: Comparison demonstrating the effects differ-\nent parameters have on our sampling strategies.\n\nfollowing four observations:\n1. The performance of all strategies except Act-\niveEA decrease as bachelors increase. How to avoid\nselecting bachelors is an important issue in design-\ning AL strategies for EA.\n2. Among all strategies, uncertainty sampling is\naffected the most, while topology-based methods\nare only marginally affected.\n3. Our structure-aware uncertainty outperforms the\nbaselines in all tested bachelor proportions.\n4. ActiveEA increases performance as the propor-\ntion of bachelors increases. The reason is: if G1 is\nfixed and the bachelors can be recognized success-\nfully, a certain budget can lead to larger ratio of\nannotated matchable entities in datasets with more\nbachelors than in those with less bachelors.\n\n5.3 Effectiveness of Bachelor Recognizer\n\nFig. 6 shows the effectiveness of our bachelor re-\ncognizer in the sampling process and the effect of\nmodel ensemble. The green curve shows the Micro-\nF1 score of our bachelor recognizer using the\nmodel ensemble. Our bachelor recognizer achieves\nhigh effectiveness from the start of sampling, where\nthere are few annotations. Each red dot repres-\nents the performance of the bachelor recognizer\ntrained with a certain data partition without us-\ning the model ensemble. Performance varied be-\ncause of the bias problem. Therefore, our model\nensemble makes the trained model obtain high and\nstable performance.\n\n5.4 Sensitivity of Parameters\n\nTo investigate the sensitivity of parameters, we ran\nour strategy with AliNet and BootEA on two DW\nvariants with bachelor proportions of 0% and 30%.\n\nThe sensitivity w.r.t. \u03b1 is shown in the top row of\n\n3 2 1 0 1 2\n\nRu\nns\n\ndropout=0.05\n\n3 2 1 0 1 2\nDifference in AUC@0.5 (%)\n\ndropout=0.1\nuncertainty\nActiveEA\n\n3 2 1 0 1 2\n\ndropout=0.2\n\nFigure 8: Effect of Bayesian Transformation on uncer-\ntainty and ActiveEA across the DW and ENDE datasets\nand different bachelor percentages.\n\nFig. 7. We observe that our method is not sensitive\nto \u03b1. The effectiveness fluctuates when \u03b1 < 0.5,\nand decreases when \u03b1 > 0.5. This indicates un-\ncertainty is more informative than structural in-\nformation. When \u03b1 = 0, our struct_uncert de-\ngenerates to uncertainty sampling (Eq. 2). In the\nupper left plot, we show the corresponding per-\nformance with dotted lines. Under most settings of\n\u03b1, the struct_uncert is much better than uncertainty\nsampling. This means that introducing structure\ninformation is beneficial.\n\nThe bottom row of Fig. 7 shows the effect of\nsampling batch size N . The overall trend is that\nlarger batch sizes decrease performance. This ob-\nservation confirms the intuition that more frequent\nupdates to the EA model lead to more precise uncer-\ntainty. Therefore, the choice of value of sampling\nbatch size is a matter of trade-off between compu-\ntation cost and sampling quality.\n\n5.5 Examination of Bayesian Transformation\n\nWe enhanced the uncertainty sampling and Act-\niveEA with Bayesian Transformation, implemented\nwith Monte Carlo (MC) dropout, and applied them\nto Alinet and RDGCN on DW and ENDE as in\nSec. 5.1. Fig. 8 shows improvements with different\nsettings of MC dropout rate. We find (1) the vari-\nation of effects on uncertainty sampling is greater\nthan that on ActiveEA; (2) Bayesian Transforma-\ntion with small dropout (e.g., 0.05) results in slight\nimprovements to ActiveEA in most cases.\n\n6 Related Works\n\nEntity Alignment. Entity Alignment refers to\nthe matching of entities across different KGs that\nrefer to the same real-world object. Compared with\nEntity Resolution (Mudgal et al., 2018), which\nmatches duplicate entities in relational data, EA\ndeals with graph data and emphasizes on exploit-\ning the structure of KGs. Neural models (Chen\n\n\n\net al., 2017, 2018; Wang et al., 2018; Cao et al.,\n2019) replaced conventional approaches (Jim\u00e9nez-\nRuiz and Grau, 2011; Suchanek et al., 2011) as\nthe core methods used in recent years. Typically\nthey rely on seed alignment as training data \u2013 this\nis expensive to annotate. Iterative training (i.e.,\nself-training) has been applied to improve EA mod-\nels by generating more training data automatic-\nally (Sun et al., 2018; Mao et al., 2020). These\nworks concern better training methods with given\nannotated data. However, the problem of reducing\nthe cost of annotation has been neglected. Ber-\nrendorf et al. (2021) have been the first to explore\nAL strategies for EA task. They compared sev-\neral types of AL heuristics including node cent-\nrality, uncertainty, graph coverage, unmatchable\nentities, etc. and they empirically showed the im-\npact of sampling strategies on the creation of seed\nalignment. In our work, we highlight the limita-\ntions of single heuristics and propose an AL frame-\nwork that can consider information structure, un-\ncertainty sampling and unmatchable entities at the\nsame time. In addition, existing neural models as-\nsume all KGs entities have counterparts: this is\na very strong assumption in reality (Zhao et al.,\n2020). We provide a solution to recognizing the\nbachelor entities, which is complementary to the\nexisting models.\n\nActive Learning. Active Learning is a gen-\neral framework for selecting the most informative\ndata to annotate when training Machine Learning\nmodels (Aggarwal et al., 2014). The pool-based\nsampling scenario is a popular AL setting where\na base pool of unlabelled instances is available to\nquery from (Settles, 2012; Aggarwal et al., 2014).\nOur proposed AL framework follows this scenario.\nNumerous AL strategies have been proposed in the\ngeneral domain (Aggarwal et al., 2014). Uncer-\ntainty sampling is the most widely used because\nof its ease to implement and its robust effective-\nness (Lewis, 1995; Cohn et al., 1996). However,\nthere are key challenges that general AL strategies\ncannot solve when applying AL to EA. Most AL\nstrategies are designed under the assumption that\nthe data is independent and identically distributed.\nHowever, KGs entities in the AL task are correlated,\nas in other graph-based tasks, e.g., node classific-\nation (Bilgic et al., 2010) and link prediction (Os-\ntapuk et al., 2019). In addition, bachelor entit-\nies cause a very special issue in EA. They may\nhave low informativeness but high uncertainty. We\n\ndesign an AL strategy to solve these special chal-\nlenges. Few existing works (Qian et al., 2017;\nMalmi et al., 2017) have applied AL to conven-\ntional EA but do not consider neural EA models,\nwhich have now become of widespread use. Only\nBerrendorf et al. (2021) empirically explored gen-\neral AL strategies for neural EA but did not solve\nthe aforementioned challenges.\n\n7 Conclusion\n\nEntity Alignment is an essential step for KG fu-\nsion. Current mainstream methods for EA are\nneural models, which rely on seed alignment. The\ncost of labelling seed alignment is often high, but\nhow to reduce this cost has been neglected. In this\nwork, we proposed an Active Learning framework\n(named ActiveEA), aiming to produce the best EA\nmodel with the least annotation cost. Specifically,\nwe attempted to solve two key challenges affect-\ning EA that general AL strategies cannot deal with.\nFirstly, we proposed a structure-aware uncertainty\nsampling, which can combine uncertainty sampling\nwith the structure information of KGs. Secondly,\nwe designed a bachelor recognizer, which reduces\nannotation budget by avoiding the selection of bach-\nelors. Specially, it can tolerate sampling biases. Ex-\ntensive experimental showed ActiveEA is more ef-\nfective than the considered baselines and has great\ngenerality across different datasets, EA models and\nbachelor percentages.\n\nIn future, we plan to explore combining active\nlearning and self-training which we believe are\ncomplementary approaches. Self-training can gen-\nerate extra training data automatically but suffers\nfrom incorrectly labelled data. This can be ad-\ndressed by amending incorrectly labelled data us-\ning AL strategies.\n\nAcknowledgements\n\nThis research is supported by the Shenyang Sci-\nence and Technology Plan Fund (No. 20-201-4-\n10), the Member Program of Neusoft Research of\nIntelligent Healthcare Technology, Co. Ltd.(No.\nNRMP001901)). Dr Wen Hua is the recipient of\nan Australian Research Council DECRA Research\nFellowship (DE210100160). Dr Guido Zuccon is\nthe recipient of an Australian Research Council\nDECRA Research Fellowship (DE180101579).\n\n\n\nReferences\nCharu C. Aggarwal, Xiangnan Kong, Quanquan Gu, Ji-\n\nawei Han, and Philip S. Yu. 2014. Active learning:\nA survey. In Charu C. Aggarwal, editor, Data Clas-\nsification: Algorithms and Applications, pages 571\u2013\n606. CRC Press.\n\nMax Berrendorf, Evgeniy Faerman, and Volker Tresp.\n2021. Active learning for entity alignment. In Ad-\nvances in Information Retrieval - 43rd European\nConference on IR Research, ECIR 2021, Virtual\nEvent, March 28 - April 1, 2021, Proceedings, Part\nI, volume 12656 of Lecture Notes in Computer Sci-\nence, pages 48\u201362. Springer.\n\nMustafa Bilgic, Lilyana Mihalkova, and Lise Getoor.\n2010. Active learning for networked data. In Pro-\nceedings of the 27th International Conference on\nMachine Learning (ICML-10), June 21-24, 2010,\nHaifa, Israel, pages 79\u201386. Omnipress.\n\nAntoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-\nDur\u00e1n, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States, pages 2787\u2013\n2795.\n\nSergey Brin and Lawrence Page. 1998. The anatomy of\na large-scale hypertextual web search engine. Com-\nput. Networks, 30(1-7):107\u2013117.\n\nYixin Cao, Zhiyuan Liu, Chengjiang Li, Zhiyuan Liu,\nJuanzi Li, and Tat-Seng Chua. 2019. Multi-channel\ngraph neural network for entity alignment. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 1452\u20131461. Association for Computa-\ntional Linguistics.\n\nMuhao Chen, Yingtao Tian, Kai-Wei Chang, Steven\nSkiena, and Carlo Zaniolo. 2018. Co-training em-\nbeddings of knowledge graphs and entity descrip-\ntions for cross-lingual entity alignment. In Proceed-\nings of the Twenty-Seventh International Joint Con-\nference on Artificial Intelligence, IJCAI 2018, July\n13-19, 2018, Stockholm, Sweden, pages 3998\u20134004.\nijcai.org.\n\nMuhao Chen, Yingtao Tian, Mohan Yang, and Carlo\nZaniolo. 2017. Multilingual knowledge graph em-\nbeddings for cross-lingual knowledge alignment. In\nProceedings of the Twenty-Sixth International Joint\nConference on Artificial Intelligence, IJCAI 2017,\nMelbourne, Australia, August 19-25, 2017, pages\n1511\u20131517. ijcai.org.\n\nDavid A. Cohn, Zoubin Ghahramani, and Michael I.\nJordan. 1996. Active learning with statistical mod-\nels. J. Artif. Intell. Res., 4:129\u2013145.\n\nMassimo Franceschet. 2011. Pagerank: standing on\nthe shoulders of giants. Commun. ACM, 54(6):92\u2013\n101.\n\nLinton C Freeman. 1977. A set of measures of central-\nity based on betweenness. Sociometry, pages 35\u201341.\n\nPaul A Gagniuc. 2017. Markov chains: from theory to\nimplementation and experimentation. John Wiley &\nSons.\n\nYarin Gal, Riashat Islam, and Zoubin Ghahramani.\n2017. Deep bayesian active learning with image\ndata. In Proceedings of the 34th International Con-\nference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017, volume 70 of\nProceedings of Machine Learning Research, pages\n1183\u20131192. PMLR.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec.\n2017. Inductive representation learning on large\ngraphs. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 1024\u20131034.\n\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Mart-\ntinen, and Philip S. Yu. 2020. A survey on know-\nledge graphs: Representation, acquisition and ap-\nplications. CoRR, abs\/2002.00388.\n\nErnesto Jim\u00e9nez-Ruiz and Bernardo Cuenca Grau.\n2011. Logmap: Logic-based and scalable ontology\nmatching. In The Semantic Web - ISWC 2011 - 10th\nInternational Semantic Web Conference, Bonn, Ger-\nmany, October 23-27, 2011, Proceedings, Part I,\nvolume 7031 of Lecture Notes in Computer Science,\npages 273\u2013288. Springer.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. In Advances in Neural\nInformation Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\n\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentz-\nsch, Dimitris Kontokostas, Pablo N. Mendes, Se-\nbastian Hellmann, Mohamed Morsey, Patrick van\nKleef, S\u00f6ren Auer, and Christian Bizer. 2015. Db-\npedia - A large-scale, multilingual knowledge base\nextracted from wikipedia. Semantic Web, 6(2):167\u2013\n195.\n\nDavid D. Lewis. 1995. A sequential algorithm for train-\ning text classifiers: Corrigendum and additional data.\nSIGIR Forum, 29(2):13\u201319.\n\nhttp:\/\/www.crcnetbase.com\/doi\/abs\/10.1201\/b17320-23\nhttp:\/\/www.crcnetbase.com\/doi\/abs\/10.1201\/b17320-23\nhttps:\/\/doi.org\/10.1007\/978-3-030-72113-8_4\nhttps:\/\/icml.cc\/Conferences\/2010\/papers\/544.pdf\nhttps:\/\/proceedings.neurips.cc\/paper\/2013\/hash\/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html\nhttps:\/\/proceedings.neurips.cc\/paper\/2013\/hash\/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html\nhttps:\/\/doi.org\/10.1016\/S0169-7552(98)00110-X\nhttps:\/\/doi.org\/10.1016\/S0169-7552(98)00110-X\nhttps:\/\/doi.org\/10.18653\/v1\/p19-1140\nhttps:\/\/doi.org\/10.18653\/v1\/p19-1140\nhttps:\/\/doi.org\/10.24963\/ijcai.2018\/556\nhttps:\/\/doi.org\/10.24963\/ijcai.2018\/556\nhttps:\/\/doi.org\/10.24963\/ijcai.2018\/556\nhttps:\/\/doi.org\/10.24963\/ijcai.2017\/209\nhttps:\/\/doi.org\/10.24963\/ijcai.2017\/209\nhttps:\/\/doi.org\/10.1613\/jair.295\nhttps:\/\/doi.org\/10.1613\/jair.295\nhttps:\/\/doi.org\/10.1145\/1953122.1953146\nhttps:\/\/doi.org\/10.1145\/1953122.1953146\nhttp:\/\/proceedings.mlr.press\/v70\/gal17a.html\nhttp:\/\/proceedings.mlr.press\/v70\/gal17a.html\nhttps:\/\/proceedings.neurips.cc\/paper\/2017\/hash\/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html\nhttps:\/\/proceedings.neurips.cc\/paper\/2017\/hash\/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html\nhttp:\/\/arxiv.org\/abs\/2002.00388\nhttp:\/\/arxiv.org\/abs\/2002.00388\nhttp:\/\/arxiv.org\/abs\/2002.00388\nhttps:\/\/doi.org\/10.1007\/978-3-642-25073-6_18\nhttps:\/\/doi.org\/10.1007\/978-3-642-25073-6_18\nhttps:\/\/proceedings.neurips.cc\/paper\/2020\/hash\/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html\nhttps:\/\/proceedings.neurips.cc\/paper\/2020\/hash\/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html\nhttps:\/\/openreview.net\/forum?id=SJU4ayYgl\nhttps:\/\/openreview.net\/forum?id=SJU4ayYgl\nhttps:\/\/openreview.net\/forum?id=SJU4ayYgl\nhttps:\/\/doi.org\/10.3233\/SW-140134\nhttps:\/\/doi.org\/10.3233\/SW-140134\nhttps:\/\/doi.org\/10.3233\/SW-140134\nhttps:\/\/doi.org\/10.1145\/219587.219592\nhttps:\/\/doi.org\/10.1145\/219587.219592\n\n\nZhiyuan Liu, Yixin Cao, Liangming Pan, Juanzi Li, and\nTat-Seng Chua. 2020. Exploring and evaluating at-\ntributes, values, and structures for entity alignment.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 6355\u2013\n6364. Association for Computational Linguistics.\n\nEric Malmi, Aristides Gionis, and Evimaria Terzi.\n2017. Active network alignment: A matching-based\napproach. In Proceedings of the 2017 ACM on\nConference on Information and Knowledge Manage-\nment, CIKM 2017, Singapore, November 06 - 10,\n2017, pages 1687\u20131696. ACM.\n\nXin Mao, Wenting Wang, Huimin Xu, Man Lan, and\nYuanbin Wu. 2020. MRAEA: an efficient and robust\nentity alignment approach for cross-lingual know-\nledge graph. In WSDM \u201920: The Thirteenth ACM\nInternational Conference on Web Search and Data\nMining, Houston, TX, USA, February 3-7, 2020,\npages 420\u2013428. ACM.\n\nSidharth Mudgal, Han Li, Theodoros Rekatsinas, An-\nHai Doan, Youngchoon Park, Ganesh Krishnan, Ro-\nhit Deep, Esteban Arcaute, and Vijay Raghavendra.\n2018. Deep learning for entity matching: A design\nspace exploration. In Proceedings of the 2018 Inter-\nnational Conference on Management of Data, SIG-\nMOD Conference 2018, Houston, TX, USA, June 10-\n15, 2018, pages 19\u201334. ACM.\n\nNatalia Ostapuk, Jie Yang, and Philippe Cudr\u00e9-\nMauroux. 2019. Activelink: Deep active learning\nfor link prediction in knowledge graphs. In The\nWorld Wide Web Conference, WWW 2019, San Fran-\ncisco, CA, USA, May 13-17, 2019, pages 1398\u20131408.\nACM.\n\nKun Qian, Lucian Popa, and Prithviraj Sen. 2017. Act-\nive learning for large-scale entity resolution. In Pro-\nceedings of the 2017 ACM on Conference on Inform-\nation and Knowledge Management, CIKM 2017,\nSingapore, November 06 - 10, 2017, pages 1379\u2013\n1388. ACM.\n\nThomas Rebele, Fabian M. Suchanek, Johannes Hof-\nfart, Joanna Biega, Erdal Kuzey, and Gerhard\nWeikum. 2016. YAGO: A multilingual knowledge\nbase from wikipedia, wordnet, and geonames. In\nThe Semantic Web - ISWC 2016 - 15th International\nSemantic Web Conference, Kobe, Japan, October 17-\n21, 2016, Proceedings, Part II, volume 9982 of Lec-\nture Notes in Computer Science, pages 177\u2013185.\n\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao\nHuang, Zhihui Li, Xiaojiang Chen, and Xin Wang.\n2020. A survey of deep active learning. CoRR,\nabs\/2009.00236.\n\nBurr Settles. 2012. Active Learning. Synthesis Lec-\ntures on Artificial Intelligence and Machine Learn-\ning. Morgan & Claypool Publishers.\n\nFabian M. Suchanek, Serge Abiteboul, and Pierre\nSenellart. 2011. PARIS: probabilistic alignment of\nrelations, instances, and schema. Proc. VLDB En-\ndow., 5(3):157\u2013168.\n\nZequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong\nQu. 2018. Bootstrapping entity alignment with\nknowledge graph embedding. In Proceedings of the\nTwenty-Seventh International Joint Conference on\nArtificial Intelligence, IJCAI 2018, July 13-19, 2018,\nStockholm, Sweden, pages 4396\u20134402. ijcai.org.\n\nZequn Sun, Chengming Wang, Wei Hu, Muhao Chen,\nJian Dai, Wei Zhang, and Yuzhong Qu. 2020a.\nKnowledge graph alignment network with gated\nmulti-hop neighborhood aggregation. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applic-\nations of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 222\u2013229.\nAAAI Press.\n\nZequn Sun, Qingheng Zhang, Wei Hu, Cheng-\nming Wang, Muhao Chen, Farahnaz Akrami, and\nChengkai Li. 2020b. A benchmarking study of\nembedding-based entity alignment for knowledge\ngraphs. Proc. VLDB Endow., 13(11):2326\u20132340.\n\nDenny Vrandecic and Markus Kr\u00f6tzsch. 2014.\nWikidata: a free collaborative knowledgebase.\nCommun. ACM, 57(10):78\u201385.\n\nZhichun Wang, Qingsong Lv, Xiaohan Lan, and\nYu Zhang. 2018. Cross-lingual knowledge graph\nalignment via graph convolutional networks. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, Brussels, Bel-\ngium, October 31 - November 4, 2018, pages 349\u2013\n357. Association for Computational Linguistics.\n\nYuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui\nYan, and Dongyan Zhao. 2019. Relation-aware en-\ntity alignment for heterogeneous knowledge graphs.\nIn Proceedings of the Twenty-Eighth International\nJoint Conference on Artificial Intelligence, IJCAI\n2019, Macao, China, August 10-16, 2019, pages\n5278\u20135284. ijcai.org.\n\nXiang Zhao, Weixin Zeng, Jiuyang Tang, Wei Wang,\nand Fabian Suchanek. 2020. An experimental\nstudy of state-of-the-art entity alignment approaches.\nIEEE Annals of the History of Computing, (01):1\u20131.\n\nhttps:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.515\nhttps:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.515\nhttps:\/\/doi.org\/10.1145\/3132847.3132983\nhttps:\/\/doi.org\/10.1145\/3132847.3132983\nhttps:\/\/doi.org\/10.1145\/3336191.3371804\nhttps:\/\/doi.org\/10.1145\/3336191.3371804\nhttps:\/\/doi.org\/10.1145\/3336191.3371804\nhttps:\/\/doi.org\/10.1145\/3183713.3196926\nhttps:\/\/doi.org\/10.1145\/3183713.3196926\nhttps:\/\/doi.org\/10.1145\/3308558.3313620\nhttps:\/\/doi.org\/10.1145\/3308558.3313620\nhttps:\/\/doi.org\/10.1145\/3132847.3132949\nhttps:\/\/doi.org\/10.1145\/3132847.3132949\nhttps:\/\/doi.org\/10.1007\/978-3-319-46547-0_19\nhttps:\/\/doi.org\/10.1007\/978-3-319-46547-0_19\nhttp:\/\/arxiv.org\/abs\/2009.00236\nhttps:\/\/doi.org\/10.2200\/S00429ED1V01Y201207AIM018\nhttps:\/\/doi.org\/10.14778\/2078331.2078332\nhttps:\/\/doi.org\/10.14778\/2078331.2078332\nhttps:\/\/doi.org\/10.24963\/ijcai.2018\/611\nhttps:\/\/doi.org\/10.24963\/ijcai.2018\/611\nhttps:\/\/aaai.org\/ojs\/index.php\/AAAI\/article\/view\/5354\nhttps:\/\/aaai.org\/ojs\/index.php\/AAAI\/article\/view\/5354\nhttp:\/\/www.vldb.org\/pvldb\/vol13\/p2326-sun.pdf\nhttp:\/\/www.vldb.org\/pvldb\/vol13\/p2326-sun.pdf\nhttp:\/\/www.vldb.org\/pvldb\/vol13\/p2326-sun.pdf\nhttps:\/\/doi.org\/10.1145\/2629489\nhttps:\/\/doi.org\/10.18653\/v1\/d18-1032\nhttps:\/\/doi.org\/10.18653\/v1\/d18-1032\nhttps:\/\/doi.org\/10.24963\/ijcai.2019\/733\nhttps:\/\/doi.org\/10.24963\/ijcai.2019\/733\n\n"}}